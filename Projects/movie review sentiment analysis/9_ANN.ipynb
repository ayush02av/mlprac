{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('df_stem.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    0\n",
       "4    1\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('df_vector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.251147</td>\n",
       "      <td>0.561305</td>\n",
       "      <td>0.126222</td>\n",
       "      <td>-0.094850</td>\n",
       "      <td>0.114038</td>\n",
       "      <td>-0.859551</td>\n",
       "      <td>-0.240192</td>\n",
       "      <td>0.361602</td>\n",
       "      <td>-0.432211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321737</td>\n",
       "      <td>0.545179</td>\n",
       "      <td>0.062861</td>\n",
       "      <td>0.241012</td>\n",
       "      <td>-0.001750</td>\n",
       "      <td>0.349589</td>\n",
       "      <td>0.236443</td>\n",
       "      <td>-0.480595</td>\n",
       "      <td>0.375254</td>\n",
       "      <td>-0.430951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.351663</td>\n",
       "      <td>0.121878</td>\n",
       "      <td>-0.120232</td>\n",
       "      <td>-0.553368</td>\n",
       "      <td>0.182102</td>\n",
       "      <td>-1.230392</td>\n",
       "      <td>0.201216</td>\n",
       "      <td>0.558062</td>\n",
       "      <td>-0.544723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239608</td>\n",
       "      <td>0.648976</td>\n",
       "      <td>0.118813</td>\n",
       "      <td>0.202644</td>\n",
       "      <td>-0.037746</td>\n",
       "      <td>0.672800</td>\n",
       "      <td>0.460015</td>\n",
       "      <td>-0.634437</td>\n",
       "      <td>0.277175</td>\n",
       "      <td>-0.146046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.552294</td>\n",
       "      <td>0.300187</td>\n",
       "      <td>-0.094526</td>\n",
       "      <td>-0.310396</td>\n",
       "      <td>-0.018994</td>\n",
       "      <td>-0.820702</td>\n",
       "      <td>0.353900</td>\n",
       "      <td>0.716587</td>\n",
       "      <td>-0.478776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316577</td>\n",
       "      <td>0.510552</td>\n",
       "      <td>0.079388</td>\n",
       "      <td>0.559151</td>\n",
       "      <td>-0.047210</td>\n",
       "      <td>0.351930</td>\n",
       "      <td>0.533410</td>\n",
       "      <td>-0.288712</td>\n",
       "      <td>0.183992</td>\n",
       "      <td>-0.352972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.233144</td>\n",
       "      <td>0.494998</td>\n",
       "      <td>0.041341</td>\n",
       "      <td>-0.211720</td>\n",
       "      <td>-0.121327</td>\n",
       "      <td>-0.502161</td>\n",
       "      <td>0.055911</td>\n",
       "      <td>0.394001</td>\n",
       "      <td>-0.394661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094764</td>\n",
       "      <td>0.917627</td>\n",
       "      <td>0.364555</td>\n",
       "      <td>0.159884</td>\n",
       "      <td>-0.202905</td>\n",
       "      <td>0.805818</td>\n",
       "      <td>0.485097</td>\n",
       "      <td>-0.742107</td>\n",
       "      <td>0.061896</td>\n",
       "      <td>-0.303398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.376380</td>\n",
       "      <td>0.266668</td>\n",
       "      <td>-0.131451</td>\n",
       "      <td>-0.053470</td>\n",
       "      <td>0.472519</td>\n",
       "      <td>-1.122471</td>\n",
       "      <td>0.230775</td>\n",
       "      <td>0.287217</td>\n",
       "      <td>-0.643272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239622</td>\n",
       "      <td>0.739376</td>\n",
       "      <td>0.068496</td>\n",
       "      <td>0.188084</td>\n",
       "      <td>0.096164</td>\n",
       "      <td>0.574591</td>\n",
       "      <td>0.168376</td>\n",
       "      <td>-0.363870</td>\n",
       "      <td>0.606393</td>\n",
       "      <td>-0.529876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0         1         2         3         4         5   \n",
       "0           0 -0.251147  0.561305  0.126222 -0.094850  0.114038 -0.859551  \\\n",
       "1           1 -0.351663  0.121878 -0.120232 -0.553368  0.182102 -1.230392   \n",
       "2           2 -0.552294  0.300187 -0.094526 -0.310396 -0.018994 -0.820702   \n",
       "3           3 -0.233144  0.494998  0.041341 -0.211720 -0.121327 -0.502161   \n",
       "4           4 -0.376380  0.266668 -0.131451 -0.053470  0.472519 -1.122471   \n",
       "\n",
       "          6         7         8  ...        90        91        92        93   \n",
       "0 -0.240192  0.361602 -0.432211  ...  0.321737  0.545179  0.062861  0.241012  \\\n",
       "1  0.201216  0.558062 -0.544723  ...  0.239608  0.648976  0.118813  0.202644   \n",
       "2  0.353900  0.716587 -0.478776  ...  0.316577  0.510552  0.079388  0.559151   \n",
       "3  0.055911  0.394001 -0.394661  ...  0.094764  0.917627  0.364555  0.159884   \n",
       "4  0.230775  0.287217 -0.643272  ...  0.239622  0.739376  0.068496  0.188084   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0 -0.001750  0.349589  0.236443 -0.480595  0.375254 -0.430951  \n",
       "1 -0.037746  0.672800  0.460015 -0.634437  0.277175 -0.146046  \n",
       "2 -0.047210  0.351930  0.533410 -0.288712  0.183992 -0.352972  \n",
       "3 -0.202905  0.805818  0.485097 -0.742107  0.061896 -0.303398  \n",
       "4  0.096164  0.574591  0.168376 -0.363870  0.606393 -0.529876  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.iloc[:, 1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(layers, epochs, x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y,\n",
    "        test_size = 0.2\n",
    "    )\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    for units in layers:\n",
    "        model.add(\n",
    "            keras.layers.Dense(\n",
    "                units = units,\n",
    "                kernel_initializer = 'he_uniform',\n",
    "                activation = 'relu'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    model.add(\n",
    "        keras.layers.Dense(\n",
    "            units = 1,\n",
    "            kernel_initializer = 'glorot_uniform',\n",
    "            activation = 'sigmoid',\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = 'adam',\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_split = 0.33,\n",
    "        batch_size = 100,\n",
    "        epochs = epochs\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN with 1 layer of 50 neurons (without scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.4679 - accuracy: 0.7839 - val_loss: 0.3909 - val_accuracy: 0.8281\n",
      "Epoch 2/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3603 - accuracy: 0.8456 - val_loss: 0.3608 - val_accuracy: 0.8436\n",
      "Epoch 3/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3476 - accuracy: 0.8507 - val_loss: 0.3494 - val_accuracy: 0.8496\n",
      "Epoch 4/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3409 - accuracy: 0.8544 - val_loss: 0.3449 - val_accuracy: 0.8549\n",
      "Epoch 5/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3374 - accuracy: 0.8559 - val_loss: 0.3451 - val_accuracy: 0.8491\n",
      "Epoch 6/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3337 - accuracy: 0.8568 - val_loss: 0.3391 - val_accuracy: 0.8551\n",
      "Epoch 7/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8581 - val_loss: 0.3384 - val_accuracy: 0.8549\n",
      "Epoch 8/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3296 - accuracy: 0.8593 - val_loss: 0.3366 - val_accuracy: 0.8568\n",
      "Epoch 9/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3282 - accuracy: 0.8596 - val_loss: 0.3386 - val_accuracy: 0.8525\n",
      "Epoch 10/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3268 - accuracy: 0.8613 - val_loss: 0.3342 - val_accuracy: 0.8564\n",
      "Epoch 11/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3253 - accuracy: 0.8610 - val_loss: 0.3359 - val_accuracy: 0.8552\n",
      "Epoch 12/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3243 - accuracy: 0.8606 - val_loss: 0.3318 - val_accuracy: 0.8577\n",
      "Epoch 13/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3232 - accuracy: 0.8624 - val_loss: 0.3318 - val_accuracy: 0.8583\n",
      "Epoch 14/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3216 - accuracy: 0.8622 - val_loss: 0.3323 - val_accuracy: 0.8570\n",
      "Epoch 15/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3195 - accuracy: 0.8635 - val_loss: 0.3299 - val_accuracy: 0.8575\n",
      "Epoch 16/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3185 - accuracy: 0.8646 - val_loss: 0.3303 - val_accuracy: 0.8575\n",
      "Epoch 17/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3171 - accuracy: 0.8650 - val_loss: 0.3307 - val_accuracy: 0.8580\n",
      "Epoch 18/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3175 - accuracy: 0.8640 - val_loss: 0.3382 - val_accuracy: 0.8540\n",
      "Epoch 19/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3163 - accuracy: 0.8652 - val_loss: 0.3311 - val_accuracy: 0.8571\n",
      "Epoch 20/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3145 - accuracy: 0.8665 - val_loss: 0.3362 - val_accuracy: 0.8538\n",
      "Epoch 21/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3132 - accuracy: 0.8671 - val_loss: 0.3304 - val_accuracy: 0.8581\n",
      "Epoch 22/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3128 - accuracy: 0.8658 - val_loss: 0.3275 - val_accuracy: 0.8579\n",
      "Epoch 23/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3111 - accuracy: 0.8674 - val_loss: 0.3278 - val_accuracy: 0.8580\n",
      "Epoch 24/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3095 - accuracy: 0.8678 - val_loss: 0.3321 - val_accuracy: 0.8536\n",
      "Epoch 25/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3089 - accuracy: 0.8681 - val_loss: 0.3273 - val_accuracy: 0.8593\n",
      "Epoch 26/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3083 - accuracy: 0.8688 - val_loss: 0.3295 - val_accuracy: 0.8578\n",
      "Epoch 27/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3068 - accuracy: 0.8693 - val_loss: 0.3297 - val_accuracy: 0.8571\n",
      "Epoch 28/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3059 - accuracy: 0.8683 - val_loss: 0.3270 - val_accuracy: 0.8595\n",
      "Epoch 29/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3057 - accuracy: 0.8697 - val_loss: 0.3284 - val_accuracy: 0.8588\n",
      "Epoch 30/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3058 - accuracy: 0.8711 - val_loss: 0.3265 - val_accuracy: 0.8590\n",
      "Epoch 31/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3031 - accuracy: 0.8710 - val_loss: 0.3277 - val_accuracy: 0.8590\n",
      "Epoch 32/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3030 - accuracy: 0.8718 - val_loss: 0.3289 - val_accuracy: 0.8601\n",
      "Epoch 33/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3020 - accuracy: 0.8713 - val_loss: 0.3288 - val_accuracy: 0.8596\n",
      "Epoch 34/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3000 - accuracy: 0.8731 - val_loss: 0.3270 - val_accuracy: 0.8600\n",
      "Epoch 35/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3007 - accuracy: 0.8727 - val_loss: 0.3300 - val_accuracy: 0.8574\n",
      "Epoch 36/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2995 - accuracy: 0.8736 - val_loss: 0.3263 - val_accuracy: 0.8615\n",
      "Epoch 37/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3003 - accuracy: 0.8722 - val_loss: 0.3275 - val_accuracy: 0.8604\n",
      "Epoch 38/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2974 - accuracy: 0.8732 - val_loss: 0.3267 - val_accuracy: 0.8605\n",
      "Epoch 39/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2963 - accuracy: 0.8742 - val_loss: 0.3268 - val_accuracy: 0.8602\n",
      "Epoch 40/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2969 - accuracy: 0.8743 - val_loss: 0.3307 - val_accuracy: 0.8561\n",
      "Epoch 41/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2951 - accuracy: 0.8738 - val_loss: 0.3278 - val_accuracy: 0.8594\n",
      "Epoch 42/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2962 - accuracy: 0.8749 - val_loss: 0.3289 - val_accuracy: 0.8600\n",
      "Epoch 43/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2940 - accuracy: 0.8757 - val_loss: 0.3267 - val_accuracy: 0.8594\n",
      "Epoch 44/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2934 - accuracy: 0.8765 - val_loss: 0.3288 - val_accuracy: 0.8602\n",
      "Epoch 45/100\n",
      "268/268 [==============================] - 0s 2ms/step - loss: 0.2927 - accuracy: 0.8756 - val_loss: 0.3263 - val_accuracy: 0.8590\n",
      "Epoch 46/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2923 - accuracy: 0.8766 - val_loss: 0.3273 - val_accuracy: 0.8599\n",
      "Epoch 47/100\n",
      "268/268 [==============================] - 0s 2ms/step - loss: 0.2911 - accuracy: 0.8784 - val_loss: 0.3308 - val_accuracy: 0.8573\n",
      "Epoch 48/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2915 - accuracy: 0.8771 - val_loss: 0.3309 - val_accuracy: 0.8599\n",
      "Epoch 49/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2901 - accuracy: 0.8752 - val_loss: 0.3327 - val_accuracy: 0.8572\n",
      "Epoch 50/100\n",
      "268/268 [==============================] - 0s 2ms/step - loss: 0.2894 - accuracy: 0.8762 - val_loss: 0.3293 - val_accuracy: 0.8589\n",
      "Epoch 51/100\n",
      "268/268 [==============================] - 0s 2ms/step - loss: 0.2901 - accuracy: 0.8775 - val_loss: 0.3272 - val_accuracy: 0.8626\n",
      "Epoch 52/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2883 - accuracy: 0.8785 - val_loss: 0.3286 - val_accuracy: 0.8613\n",
      "Epoch 53/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2888 - accuracy: 0.8781 - val_loss: 0.3283 - val_accuracy: 0.8595\n",
      "Epoch 54/100\n",
      "268/268 [==============================] - 0s 2ms/step - loss: 0.2868 - accuracy: 0.8798 - val_loss: 0.3274 - val_accuracy: 0.8625\n",
      "Epoch 55/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2879 - accuracy: 0.8785 - val_loss: 0.3367 - val_accuracy: 0.8543\n",
      "Epoch 56/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2879 - accuracy: 0.8782 - val_loss: 0.3292 - val_accuracy: 0.8597\n",
      "Epoch 57/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2851 - accuracy: 0.8809 - val_loss: 0.3263 - val_accuracy: 0.8608\n",
      "Epoch 58/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2857 - accuracy: 0.8794 - val_loss: 0.3276 - val_accuracy: 0.8600\n",
      "Epoch 59/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2848 - accuracy: 0.8795 - val_loss: 0.3268 - val_accuracy: 0.8605\n",
      "Epoch 60/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2843 - accuracy: 0.8810 - val_loss: 0.3283 - val_accuracy: 0.8599\n",
      "Epoch 61/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2837 - accuracy: 0.8801 - val_loss: 0.3317 - val_accuracy: 0.8577\n",
      "Epoch 62/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2836 - accuracy: 0.8800 - val_loss: 0.3324 - val_accuracy: 0.8574\n",
      "Epoch 63/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2808 - accuracy: 0.8812 - val_loss: 0.3311 - val_accuracy: 0.8578\n",
      "Epoch 64/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2811 - accuracy: 0.8821 - val_loss: 0.3297 - val_accuracy: 0.8595\n",
      "Epoch 65/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2801 - accuracy: 0.8826 - val_loss: 0.3285 - val_accuracy: 0.8609\n",
      "Epoch 66/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2806 - accuracy: 0.8809 - val_loss: 0.3337 - val_accuracy: 0.8592\n",
      "Epoch 67/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2788 - accuracy: 0.8817 - val_loss: 0.3341 - val_accuracy: 0.8572\n",
      "Epoch 68/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2801 - accuracy: 0.8831 - val_loss: 0.3283 - val_accuracy: 0.8595\n",
      "Epoch 69/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2779 - accuracy: 0.8837 - val_loss: 0.3328 - val_accuracy: 0.8577\n",
      "Epoch 70/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2777 - accuracy: 0.8832 - val_loss: 0.3304 - val_accuracy: 0.8596\n",
      "Epoch 71/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2769 - accuracy: 0.8852 - val_loss: 0.3348 - val_accuracy: 0.8573\n",
      "Epoch 72/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2772 - accuracy: 0.8843 - val_loss: 0.3385 - val_accuracy: 0.8555\n",
      "Epoch 73/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2764 - accuracy: 0.8853 - val_loss: 0.3341 - val_accuracy: 0.8577\n",
      "Epoch 74/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2770 - accuracy: 0.8835 - val_loss: 0.3321 - val_accuracy: 0.8595\n",
      "Epoch 75/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2750 - accuracy: 0.8852 - val_loss: 0.3327 - val_accuracy: 0.8583\n",
      "Epoch 76/100\n",
      "268/268 [==============================] - 0s 2ms/step - loss: 0.2745 - accuracy: 0.8849 - val_loss: 0.3301 - val_accuracy: 0.8605\n",
      "Epoch 77/100\n",
      "268/268 [==============================] - 0s 2ms/step - loss: 0.2735 - accuracy: 0.8847 - val_loss: 0.3347 - val_accuracy: 0.8585\n",
      "Epoch 78/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2750 - accuracy: 0.8854 - val_loss: 0.3345 - val_accuracy: 0.8585\n",
      "Epoch 79/100\n",
      "268/268 [==============================] - 0s 2ms/step - loss: 0.2729 - accuracy: 0.8860 - val_loss: 0.3360 - val_accuracy: 0.8558\n",
      "Epoch 80/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2735 - accuracy: 0.8860 - val_loss: 0.3324 - val_accuracy: 0.8595\n",
      "Epoch 81/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2712 - accuracy: 0.8879 - val_loss: 0.3325 - val_accuracy: 0.8586\n",
      "Epoch 82/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2727 - accuracy: 0.8858 - val_loss: 0.3304 - val_accuracy: 0.8607\n",
      "Epoch 83/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2713 - accuracy: 0.8875 - val_loss: 0.3366 - val_accuracy: 0.8584\n",
      "Epoch 84/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2705 - accuracy: 0.8865 - val_loss: 0.3371 - val_accuracy: 0.8564\n",
      "Epoch 85/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2696 - accuracy: 0.8885 - val_loss: 0.3324 - val_accuracy: 0.8596\n",
      "Epoch 86/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2695 - accuracy: 0.8882 - val_loss: 0.3340 - val_accuracy: 0.8590\n",
      "Epoch 87/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2689 - accuracy: 0.8892 - val_loss: 0.3429 - val_accuracy: 0.8545\n",
      "Epoch 88/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2684 - accuracy: 0.8893 - val_loss: 0.3374 - val_accuracy: 0.8574\n",
      "Epoch 89/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2691 - accuracy: 0.8897 - val_loss: 0.3364 - val_accuracy: 0.8575\n",
      "Epoch 90/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2677 - accuracy: 0.8888 - val_loss: 0.3513 - val_accuracy: 0.8497\n",
      "Epoch 91/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2674 - accuracy: 0.8893 - val_loss: 0.3341 - val_accuracy: 0.8588\n",
      "Epoch 92/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2671 - accuracy: 0.8906 - val_loss: 0.3369 - val_accuracy: 0.8561\n",
      "Epoch 93/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2668 - accuracy: 0.8893 - val_loss: 0.3343 - val_accuracy: 0.8605\n",
      "Epoch 94/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2665 - accuracy: 0.8899 - val_loss: 0.3354 - val_accuracy: 0.8574\n",
      "Epoch 95/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2659 - accuracy: 0.8906 - val_loss: 0.3415 - val_accuracy: 0.8553\n",
      "Epoch 96/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2638 - accuracy: 0.8924 - val_loss: 0.3409 - val_accuracy: 0.8560\n",
      "Epoch 97/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2649 - accuracy: 0.8904 - val_loss: 0.3377 - val_accuracy: 0.8584\n",
      "Epoch 98/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2636 - accuracy: 0.8912 - val_loss: 0.3390 - val_accuracy: 0.8537\n",
      "Epoch 99/100\n",
      "268/268 [==============================] - 0s 2ms/step - loss: 0.2637 - accuracy: 0.8922 - val_loss: 0.3367 - val_accuracy: 0.8573\n",
      "Epoch 100/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2625 - accuracy: 0.8925 - val_loss: 0.3375 - val_accuracy: 0.8568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d987416d90>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test([50], 100, x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "89.25% training accuracy with 85.68% validation accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN with 2 layers of 50 and 10 neurons (without scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.4467 - accuracy: 0.8043 - val_loss: 0.3596 - val_accuracy: 0.8429\n",
      "Epoch 2/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3473 - accuracy: 0.8505 - val_loss: 0.3408 - val_accuracy: 0.8544\n",
      "Epoch 3/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3407 - accuracy: 0.8543 - val_loss: 0.3434 - val_accuracy: 0.8527\n",
      "Epoch 4/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3347 - accuracy: 0.8576 - val_loss: 0.3342 - val_accuracy: 0.8556\n",
      "Epoch 5/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8569 - val_loss: 0.3331 - val_accuracy: 0.8577\n",
      "Epoch 6/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3304 - accuracy: 0.8567 - val_loss: 0.3316 - val_accuracy: 0.8587\n",
      "Epoch 7/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3291 - accuracy: 0.8575 - val_loss: 0.3345 - val_accuracy: 0.8551\n",
      "Epoch 8/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3250 - accuracy: 0.8607 - val_loss: 0.3275 - val_accuracy: 0.8605\n",
      "Epoch 9/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3242 - accuracy: 0.8601 - val_loss: 0.3271 - val_accuracy: 0.8608\n",
      "Epoch 10/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3207 - accuracy: 0.8616 - val_loss: 0.3333 - val_accuracy: 0.8577\n",
      "Epoch 11/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3205 - accuracy: 0.8620 - val_loss: 0.3397 - val_accuracy: 0.8528\n",
      "Epoch 12/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3190 - accuracy: 0.8630 - val_loss: 0.3272 - val_accuracy: 0.8607\n",
      "Epoch 13/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3181 - accuracy: 0.8629 - val_loss: 0.3267 - val_accuracy: 0.8621\n",
      "Epoch 14/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3161 - accuracy: 0.8632 - val_loss: 0.3409 - val_accuracy: 0.8544\n",
      "Epoch 15/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3145 - accuracy: 0.8638 - val_loss: 0.3257 - val_accuracy: 0.8622\n",
      "Epoch 16/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3128 - accuracy: 0.8651 - val_loss: 0.3265 - val_accuracy: 0.8617\n",
      "Epoch 17/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3104 - accuracy: 0.8674 - val_loss: 0.3268 - val_accuracy: 0.8606\n",
      "Epoch 18/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3094 - accuracy: 0.8675 - val_loss: 0.3292 - val_accuracy: 0.8606\n",
      "Epoch 19/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3085 - accuracy: 0.8663 - val_loss: 0.3289 - val_accuracy: 0.8598\n",
      "Epoch 20/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3080 - accuracy: 0.8673 - val_loss: 0.3269 - val_accuracy: 0.8622\n",
      "Epoch 21/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3063 - accuracy: 0.8687 - val_loss: 0.3284 - val_accuracy: 0.8620\n",
      "Epoch 22/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3045 - accuracy: 0.8690 - val_loss: 0.3252 - val_accuracy: 0.8630\n",
      "Epoch 23/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3045 - accuracy: 0.8698 - val_loss: 0.3288 - val_accuracy: 0.8593\n",
      "Epoch 24/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3033 - accuracy: 0.8707 - val_loss: 0.3326 - val_accuracy: 0.8599\n",
      "Epoch 25/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3014 - accuracy: 0.8714 - val_loss: 0.3312 - val_accuracy: 0.8588\n",
      "Epoch 26/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2991 - accuracy: 0.8699 - val_loss: 0.3281 - val_accuracy: 0.8597\n",
      "Epoch 27/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2980 - accuracy: 0.8728 - val_loss: 0.3296 - val_accuracy: 0.8602\n",
      "Epoch 28/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2978 - accuracy: 0.8738 - val_loss: 0.3281 - val_accuracy: 0.8614\n",
      "Epoch 29/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2971 - accuracy: 0.8730 - val_loss: 0.3346 - val_accuracy: 0.8578\n",
      "Epoch 30/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2964 - accuracy: 0.8747 - val_loss: 0.3310 - val_accuracy: 0.8621\n",
      "Epoch 31/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2940 - accuracy: 0.8759 - val_loss: 0.3373 - val_accuracy: 0.8564\n",
      "Epoch 32/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2940 - accuracy: 0.8750 - val_loss: 0.3316 - val_accuracy: 0.8602\n",
      "Epoch 33/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2937 - accuracy: 0.8765 - val_loss: 0.3344 - val_accuracy: 0.8576\n",
      "Epoch 34/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2924 - accuracy: 0.8760 - val_loss: 0.3312 - val_accuracy: 0.8604\n",
      "Epoch 35/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2893 - accuracy: 0.8772 - val_loss: 0.3311 - val_accuracy: 0.8585\n",
      "Epoch 36/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2895 - accuracy: 0.8769 - val_loss: 0.3358 - val_accuracy: 0.8583\n",
      "Epoch 37/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2883 - accuracy: 0.8779 - val_loss: 0.3325 - val_accuracy: 0.8597\n",
      "Epoch 38/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2880 - accuracy: 0.8778 - val_loss: 0.3337 - val_accuracy: 0.8589\n",
      "Epoch 39/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2869 - accuracy: 0.8786 - val_loss: 0.3322 - val_accuracy: 0.8601\n",
      "Epoch 40/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2865 - accuracy: 0.8778 - val_loss: 0.3372 - val_accuracy: 0.8597\n",
      "Epoch 41/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2881 - accuracy: 0.8787 - val_loss: 0.3381 - val_accuracy: 0.8580\n",
      "Epoch 42/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2847 - accuracy: 0.8795 - val_loss: 0.3330 - val_accuracy: 0.8602\n",
      "Epoch 43/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2830 - accuracy: 0.8810 - val_loss: 0.3340 - val_accuracy: 0.8607\n",
      "Epoch 44/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2822 - accuracy: 0.8815 - val_loss: 0.3354 - val_accuracy: 0.8596\n",
      "Epoch 45/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2807 - accuracy: 0.8813 - val_loss: 0.3426 - val_accuracy: 0.8574\n",
      "Epoch 46/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2802 - accuracy: 0.8820 - val_loss: 0.3378 - val_accuracy: 0.8574\n",
      "Epoch 47/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2809 - accuracy: 0.8818 - val_loss: 0.3391 - val_accuracy: 0.8567\n",
      "Epoch 48/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2787 - accuracy: 0.8825 - val_loss: 0.3411 - val_accuracy: 0.8599\n",
      "Epoch 49/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2776 - accuracy: 0.8822 - val_loss: 0.3466 - val_accuracy: 0.8515\n",
      "Epoch 50/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2785 - accuracy: 0.8829 - val_loss: 0.3470 - val_accuracy: 0.8533\n",
      "Epoch 51/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2786 - accuracy: 0.8825 - val_loss: 0.3387 - val_accuracy: 0.8577\n",
      "Epoch 52/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2762 - accuracy: 0.8847 - val_loss: 0.3445 - val_accuracy: 0.8574\n",
      "Epoch 53/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2754 - accuracy: 0.8823 - val_loss: 0.3417 - val_accuracy: 0.8595\n",
      "Epoch 54/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2744 - accuracy: 0.8853 - val_loss: 0.3403 - val_accuracy: 0.8583\n",
      "Epoch 55/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2742 - accuracy: 0.8840 - val_loss: 0.3393 - val_accuracy: 0.8595\n",
      "Epoch 56/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2731 - accuracy: 0.8849 - val_loss: 0.3465 - val_accuracy: 0.8544\n",
      "Epoch 57/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2726 - accuracy: 0.8863 - val_loss: 0.3412 - val_accuracy: 0.8592\n",
      "Epoch 58/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2725 - accuracy: 0.8856 - val_loss: 0.3464 - val_accuracy: 0.8542\n",
      "Epoch 59/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2719 - accuracy: 0.8856 - val_loss: 0.3434 - val_accuracy: 0.8599\n",
      "Epoch 60/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2707 - accuracy: 0.8856 - val_loss: 0.3467 - val_accuracy: 0.8568\n",
      "Epoch 61/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2707 - accuracy: 0.8865 - val_loss: 0.3485 - val_accuracy: 0.8558\n",
      "Epoch 62/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2691 - accuracy: 0.8866 - val_loss: 0.3432 - val_accuracy: 0.8593\n",
      "Epoch 63/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2684 - accuracy: 0.8876 - val_loss: 0.3501 - val_accuracy: 0.8578\n",
      "Epoch 64/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2697 - accuracy: 0.8860 - val_loss: 0.3466 - val_accuracy: 0.8550\n",
      "Epoch 65/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2675 - accuracy: 0.8876 - val_loss: 0.3539 - val_accuracy: 0.8489\n",
      "Epoch 66/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2682 - accuracy: 0.8873 - val_loss: 0.3451 - val_accuracy: 0.8561\n",
      "Epoch 67/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2655 - accuracy: 0.8891 - val_loss: 0.3544 - val_accuracy: 0.8536\n",
      "Epoch 68/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2654 - accuracy: 0.8893 - val_loss: 0.3576 - val_accuracy: 0.8490\n",
      "Epoch 69/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2657 - accuracy: 0.8892 - val_loss: 0.3474 - val_accuracy: 0.8575\n",
      "Epoch 70/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2641 - accuracy: 0.8898 - val_loss: 0.3504 - val_accuracy: 0.8577\n",
      "Epoch 71/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2634 - accuracy: 0.8900 - val_loss: 0.3507 - val_accuracy: 0.8545\n",
      "Epoch 72/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2641 - accuracy: 0.8899 - val_loss: 0.3529 - val_accuracy: 0.8549\n",
      "Epoch 73/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2619 - accuracy: 0.8907 - val_loss: 0.3544 - val_accuracy: 0.8578\n",
      "Epoch 74/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2621 - accuracy: 0.8908 - val_loss: 0.3530 - val_accuracy: 0.8591\n",
      "Epoch 75/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2612 - accuracy: 0.8897 - val_loss: 0.3582 - val_accuracy: 0.8509\n",
      "Epoch 76/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2614 - accuracy: 0.8908 - val_loss: 0.3564 - val_accuracy: 0.8521\n",
      "Epoch 77/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2609 - accuracy: 0.8909 - val_loss: 0.3537 - val_accuracy: 0.8555\n",
      "Epoch 78/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2595 - accuracy: 0.8925 - val_loss: 0.3552 - val_accuracy: 0.8558\n",
      "Epoch 79/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2596 - accuracy: 0.8917 - val_loss: 0.3565 - val_accuracy: 0.8558\n",
      "Epoch 80/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2582 - accuracy: 0.8929 - val_loss: 0.3584 - val_accuracy: 0.8521\n",
      "Epoch 81/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2570 - accuracy: 0.8932 - val_loss: 0.3578 - val_accuracy: 0.8503\n",
      "Epoch 82/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2584 - accuracy: 0.8922 - val_loss: 0.3545 - val_accuracy: 0.8571\n",
      "Epoch 83/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2576 - accuracy: 0.8917 - val_loss: 0.3596 - val_accuracy: 0.8536\n",
      "Epoch 84/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2561 - accuracy: 0.8921 - val_loss: 0.3799 - val_accuracy: 0.8444\n",
      "Epoch 85/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2594 - accuracy: 0.8910 - val_loss: 0.3574 - val_accuracy: 0.8558\n",
      "Epoch 86/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2551 - accuracy: 0.8935 - val_loss: 0.3648 - val_accuracy: 0.8507\n",
      "Epoch 87/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2536 - accuracy: 0.8960 - val_loss: 0.3658 - val_accuracy: 0.8554\n",
      "Epoch 88/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2547 - accuracy: 0.8946 - val_loss: 0.3600 - val_accuracy: 0.8521\n",
      "Epoch 89/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2527 - accuracy: 0.8950 - val_loss: 0.3606 - val_accuracy: 0.8543\n",
      "Epoch 90/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2534 - accuracy: 0.8948 - val_loss: 0.3711 - val_accuracy: 0.8487\n",
      "Epoch 91/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2522 - accuracy: 0.8951 - val_loss: 0.3760 - val_accuracy: 0.8530\n",
      "Epoch 92/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2532 - accuracy: 0.8957 - val_loss: 0.3613 - val_accuracy: 0.8553\n",
      "Epoch 93/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2505 - accuracy: 0.8971 - val_loss: 0.3643 - val_accuracy: 0.8566\n",
      "Epoch 94/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2516 - accuracy: 0.8951 - val_loss: 0.3629 - val_accuracy: 0.8582\n",
      "Epoch 95/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2512 - accuracy: 0.8956 - val_loss: 0.3683 - val_accuracy: 0.8549\n",
      "Epoch 96/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2507 - accuracy: 0.8961 - val_loss: 0.3647 - val_accuracy: 0.8541\n",
      "Epoch 97/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2493 - accuracy: 0.8955 - val_loss: 0.3668 - val_accuracy: 0.8566\n",
      "Epoch 98/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2504 - accuracy: 0.8963 - val_loss: 0.3709 - val_accuracy: 0.8519\n",
      "Epoch 99/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.8965 - val_loss: 0.3696 - val_accuracy: 0.8535\n",
      "Epoch 100/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2493 - accuracy: 0.8973 - val_loss: 0.3691 - val_accuracy: 0.8545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d98974b210>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test([50, 10], 100, x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "89.73% training accuracy with 85.45% validation accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN with 2 layers of 60 and 20 neurons (without scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.4235 - accuracy: 0.8075 - val_loss: 0.3648 - val_accuracy: 0.8399\n",
      "Epoch 2/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3420 - accuracy: 0.8523 - val_loss: 0.3520 - val_accuracy: 0.8501\n",
      "Epoch 3/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3330 - accuracy: 0.8571 - val_loss: 0.3510 - val_accuracy: 0.8485\n",
      "Epoch 4/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3291 - accuracy: 0.8585 - val_loss: 0.3450 - val_accuracy: 0.8510\n",
      "Epoch 5/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3245 - accuracy: 0.8606 - val_loss: 0.3424 - val_accuracy: 0.8530\n",
      "Epoch 6/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3202 - accuracy: 0.8626 - val_loss: 0.3439 - val_accuracy: 0.8517\n",
      "Epoch 7/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3204 - accuracy: 0.8614 - val_loss: 0.3440 - val_accuracy: 0.8508\n",
      "Epoch 8/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3181 - accuracy: 0.8634 - val_loss: 0.3398 - val_accuracy: 0.8514\n",
      "Epoch 9/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3164 - accuracy: 0.8648 - val_loss: 0.3441 - val_accuracy: 0.8506\n",
      "Epoch 10/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3143 - accuracy: 0.8638 - val_loss: 0.3414 - val_accuracy: 0.8528\n",
      "Epoch 11/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3107 - accuracy: 0.8665 - val_loss: 0.3413 - val_accuracy: 0.8544\n",
      "Epoch 12/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3089 - accuracy: 0.8682 - val_loss: 0.3384 - val_accuracy: 0.8555\n",
      "Epoch 13/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3081 - accuracy: 0.8681 - val_loss: 0.3461 - val_accuracy: 0.8510\n",
      "Epoch 14/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3061 - accuracy: 0.8698 - val_loss: 0.3436 - val_accuracy: 0.8515\n",
      "Epoch 15/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3049 - accuracy: 0.8699 - val_loss: 0.3432 - val_accuracy: 0.8537\n",
      "Epoch 16/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3025 - accuracy: 0.8713 - val_loss: 0.3404 - val_accuracy: 0.8534\n",
      "Epoch 17/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3005 - accuracy: 0.8720 - val_loss: 0.3396 - val_accuracy: 0.8524\n",
      "Epoch 18/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2995 - accuracy: 0.8723 - val_loss: 0.3417 - val_accuracy: 0.8531\n",
      "Epoch 19/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2991 - accuracy: 0.8710 - val_loss: 0.3444 - val_accuracy: 0.8542\n",
      "Epoch 20/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2965 - accuracy: 0.8731 - val_loss: 0.3397 - val_accuracy: 0.8529\n",
      "Epoch 21/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2953 - accuracy: 0.8759 - val_loss: 0.3441 - val_accuracy: 0.8515\n",
      "Epoch 22/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2958 - accuracy: 0.8735 - val_loss: 0.3424 - val_accuracy: 0.8497\n",
      "Epoch 23/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2920 - accuracy: 0.8754 - val_loss: 0.3467 - val_accuracy: 0.8500\n",
      "Epoch 24/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2906 - accuracy: 0.8775 - val_loss: 0.3429 - val_accuracy: 0.8536\n",
      "Epoch 25/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2895 - accuracy: 0.8765 - val_loss: 0.3459 - val_accuracy: 0.8527\n",
      "Epoch 26/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2878 - accuracy: 0.8772 - val_loss: 0.3438 - val_accuracy: 0.8529\n",
      "Epoch 27/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2894 - accuracy: 0.8762 - val_loss: 0.3449 - val_accuracy: 0.8506\n",
      "Epoch 28/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2855 - accuracy: 0.8800 - val_loss: 0.3460 - val_accuracy: 0.8515\n",
      "Epoch 29/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2840 - accuracy: 0.8807 - val_loss: 0.3441 - val_accuracy: 0.8524\n",
      "Epoch 30/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2826 - accuracy: 0.8801 - val_loss: 0.3470 - val_accuracy: 0.8538\n",
      "Epoch 31/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2803 - accuracy: 0.8820 - val_loss: 0.3458 - val_accuracy: 0.8543\n",
      "Epoch 32/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2816 - accuracy: 0.8806 - val_loss: 0.3451 - val_accuracy: 0.8513\n",
      "Epoch 33/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2774 - accuracy: 0.8828 - val_loss: 0.3450 - val_accuracy: 0.8540\n",
      "Epoch 34/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2777 - accuracy: 0.8826 - val_loss: 0.3469 - val_accuracy: 0.8536\n",
      "Epoch 35/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2765 - accuracy: 0.8849 - val_loss: 0.3471 - val_accuracy: 0.8528\n",
      "Epoch 36/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2764 - accuracy: 0.8840 - val_loss: 0.3614 - val_accuracy: 0.8468\n",
      "Epoch 37/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2749 - accuracy: 0.8845 - val_loss: 0.3560 - val_accuracy: 0.8493\n",
      "Epoch 38/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2734 - accuracy: 0.8852 - val_loss: 0.3470 - val_accuracy: 0.8515\n",
      "Epoch 39/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2725 - accuracy: 0.8844 - val_loss: 0.3539 - val_accuracy: 0.8482\n",
      "Epoch 40/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2697 - accuracy: 0.8866 - val_loss: 0.3513 - val_accuracy: 0.8500\n",
      "Epoch 41/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2699 - accuracy: 0.8862 - val_loss: 0.3578 - val_accuracy: 0.8517\n",
      "Epoch 42/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2686 - accuracy: 0.8852 - val_loss: 0.3569 - val_accuracy: 0.8503\n",
      "Epoch 43/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2665 - accuracy: 0.8874 - val_loss: 0.3556 - val_accuracy: 0.8513\n",
      "Epoch 44/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2643 - accuracy: 0.8891 - val_loss: 0.3549 - val_accuracy: 0.8493\n",
      "Epoch 45/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2665 - accuracy: 0.8872 - val_loss: 0.3510 - val_accuracy: 0.8509\n",
      "Epoch 46/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2626 - accuracy: 0.8908 - val_loss: 0.3592 - val_accuracy: 0.8506\n",
      "Epoch 47/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2610 - accuracy: 0.8902 - val_loss: 0.3592 - val_accuracy: 0.8502\n",
      "Epoch 48/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2605 - accuracy: 0.8903 - val_loss: 0.3623 - val_accuracy: 0.8514\n",
      "Epoch 49/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2595 - accuracy: 0.8902 - val_loss: 0.3590 - val_accuracy: 0.8488\n",
      "Epoch 50/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2604 - accuracy: 0.8896 - val_loss: 0.3626 - val_accuracy: 0.8481\n",
      "Epoch 51/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2574 - accuracy: 0.8915 - val_loss: 0.3597 - val_accuracy: 0.8520\n",
      "Epoch 52/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2546 - accuracy: 0.8913 - val_loss: 0.3627 - val_accuracy: 0.8488\n",
      "Epoch 53/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2543 - accuracy: 0.8935 - val_loss: 0.3629 - val_accuracy: 0.8515\n",
      "Epoch 54/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2524 - accuracy: 0.8925 - val_loss: 0.3641 - val_accuracy: 0.8504\n",
      "Epoch 55/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2562 - accuracy: 0.8910 - val_loss: 0.3798 - val_accuracy: 0.8402\n",
      "Epoch 56/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2524 - accuracy: 0.8929 - val_loss: 0.3661 - val_accuracy: 0.8486\n",
      "Epoch 57/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2515 - accuracy: 0.8945 - val_loss: 0.3656 - val_accuracy: 0.8511\n",
      "Epoch 58/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2499 - accuracy: 0.8954 - val_loss: 0.3900 - val_accuracy: 0.8415\n",
      "Epoch 59/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2495 - accuracy: 0.8946 - val_loss: 0.3684 - val_accuracy: 0.8486\n",
      "Epoch 60/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2476 - accuracy: 0.8964 - val_loss: 0.3770 - val_accuracy: 0.8450\n",
      "Epoch 61/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2455 - accuracy: 0.8965 - val_loss: 0.3786 - val_accuracy: 0.8435\n",
      "Epoch 62/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2446 - accuracy: 0.8973 - val_loss: 0.3713 - val_accuracy: 0.8480\n",
      "Epoch 63/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2444 - accuracy: 0.8974 - val_loss: 0.3753 - val_accuracy: 0.8508\n",
      "Epoch 64/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2437 - accuracy: 0.8982 - val_loss: 0.3729 - val_accuracy: 0.8479\n",
      "Epoch 65/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2401 - accuracy: 0.8988 - val_loss: 0.3822 - val_accuracy: 0.8415\n",
      "Epoch 66/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2429 - accuracy: 0.8992 - val_loss: 0.3754 - val_accuracy: 0.8463\n",
      "Epoch 67/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2404 - accuracy: 0.8985 - val_loss: 0.3774 - val_accuracy: 0.8452\n",
      "Epoch 68/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2378 - accuracy: 0.8996 - val_loss: 0.3820 - val_accuracy: 0.8493\n",
      "Epoch 69/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2390 - accuracy: 0.8994 - val_loss: 0.3725 - val_accuracy: 0.8466\n",
      "Epoch 70/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2375 - accuracy: 0.9008 - val_loss: 0.3781 - val_accuracy: 0.8451\n",
      "Epoch 71/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2352 - accuracy: 0.9016 - val_loss: 0.3855 - val_accuracy: 0.8486\n",
      "Epoch 72/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2353 - accuracy: 0.9010 - val_loss: 0.3791 - val_accuracy: 0.8454\n",
      "Epoch 73/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2321 - accuracy: 0.9033 - val_loss: 0.3849 - val_accuracy: 0.8440\n",
      "Epoch 74/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2334 - accuracy: 0.9035 - val_loss: 0.3865 - val_accuracy: 0.8461\n",
      "Epoch 75/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2329 - accuracy: 0.9030 - val_loss: 0.3879 - val_accuracy: 0.8456\n",
      "Epoch 76/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2301 - accuracy: 0.9035 - val_loss: 0.4029 - val_accuracy: 0.8433\n",
      "Epoch 77/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2290 - accuracy: 0.9045 - val_loss: 0.3890 - val_accuracy: 0.8468\n",
      "Epoch 78/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2294 - accuracy: 0.9035 - val_loss: 0.3988 - val_accuracy: 0.8427\n",
      "Epoch 79/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2269 - accuracy: 0.9066 - val_loss: 0.3976 - val_accuracy: 0.8444\n",
      "Epoch 80/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2265 - accuracy: 0.9065 - val_loss: 0.4031 - val_accuracy: 0.8451\n",
      "Epoch 81/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2242 - accuracy: 0.9061 - val_loss: 0.3968 - val_accuracy: 0.8433\n",
      "Epoch 82/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2239 - accuracy: 0.9079 - val_loss: 0.3999 - val_accuracy: 0.8433\n",
      "Epoch 83/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2224 - accuracy: 0.9083 - val_loss: 0.4068 - val_accuracy: 0.8422\n",
      "Epoch 84/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2243 - accuracy: 0.9048 - val_loss: 0.4093 - val_accuracy: 0.8393\n",
      "Epoch 85/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2222 - accuracy: 0.9068 - val_loss: 0.4085 - val_accuracy: 0.8407\n",
      "Epoch 86/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2209 - accuracy: 0.9070 - val_loss: 0.4208 - val_accuracy: 0.8355\n",
      "Epoch 87/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2213 - accuracy: 0.9091 - val_loss: 0.4026 - val_accuracy: 0.8426\n",
      "Epoch 88/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2171 - accuracy: 0.9102 - val_loss: 0.4091 - val_accuracy: 0.8410\n",
      "Epoch 89/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2209 - accuracy: 0.9073 - val_loss: 0.4119 - val_accuracy: 0.8428\n",
      "Epoch 90/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2185 - accuracy: 0.9090 - val_loss: 0.4077 - val_accuracy: 0.8436\n",
      "Epoch 91/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2152 - accuracy: 0.9089 - val_loss: 0.4090 - val_accuracy: 0.8405\n",
      "Epoch 92/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2128 - accuracy: 0.9120 - val_loss: 0.4040 - val_accuracy: 0.8414\n",
      "Epoch 93/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2141 - accuracy: 0.9122 - val_loss: 0.4165 - val_accuracy: 0.8440\n",
      "Epoch 94/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2136 - accuracy: 0.9110 - val_loss: 0.4109 - val_accuracy: 0.8436\n",
      "Epoch 95/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2135 - accuracy: 0.9114 - val_loss: 0.4168 - val_accuracy: 0.8436\n",
      "Epoch 96/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2115 - accuracy: 0.9114 - val_loss: 0.4327 - val_accuracy: 0.8423\n",
      "Epoch 97/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2111 - accuracy: 0.9134 - val_loss: 0.4266 - val_accuracy: 0.8417\n",
      "Epoch 98/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2084 - accuracy: 0.9140 - val_loss: 0.4203 - val_accuracy: 0.8422\n",
      "Epoch 99/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2090 - accuracy: 0.9133 - val_loss: 0.4236 - val_accuracy: 0.8349\n",
      "Epoch 100/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2109 - accuracy: 0.9123 - val_loss: 0.4231 - val_accuracy: 0.8436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d989789850>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test([60, 20], 100, x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "91.23% training accuracy with 84.36% validation accuracy\n",
    "\n",
    "- slightly overfitting\n",
    "- should add dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN with 2 layers of 60 and 20 neurons (with Standard scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = pd.DataFrame(StandardScaler().fit_transform(x), columns=x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.173284e-16</td>\n",
       "      <td>3.925038e-16</td>\n",
       "      <td>-9.791279e-17</td>\n",
       "      <td>8.327561e-17</td>\n",
       "      <td>2.842171e-18</td>\n",
       "      <td>-5.124434e-16</td>\n",
       "      <td>5.172751e-17</td>\n",
       "      <td>-3.666401e-17</td>\n",
       "      <td>2.529532e-17</td>\n",
       "      <td>-8.014922e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>2.636824e-16</td>\n",
       "      <td>5.329071e-16</td>\n",
       "      <td>-2.856382e-17</td>\n",
       "      <td>1.239187e-16</td>\n",
       "      <td>-2.842171e-17</td>\n",
       "      <td>-6.414780e-16</td>\n",
       "      <td>3.038281e-16</td>\n",
       "      <td>-2.927436e-16</td>\n",
       "      <td>-4.433787e-17</td>\n",
       "      <td>-3.026202e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "      <td>1.000010e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-6.202139e+00</td>\n",
       "      <td>-1.486309e+01</td>\n",
       "      <td>-6.757726e+00</td>\n",
       "      <td>-7.300654e+00</td>\n",
       "      <td>-7.937158e+00</td>\n",
       "      <td>-4.778139e+00</td>\n",
       "      <td>-6.168499e+00</td>\n",
       "      <td>-6.339533e+00</td>\n",
       "      <td>-7.330723e+00</td>\n",
       "      <td>-7.146061e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.783539e+00</td>\n",
       "      <td>-7.973312e+00</td>\n",
       "      <td>-4.788789e+00</td>\n",
       "      <td>-7.246421e+00</td>\n",
       "      <td>-5.562719e+00</td>\n",
       "      <td>-4.311398e+00</td>\n",
       "      <td>-5.486682e+00</td>\n",
       "      <td>-6.412036e+00</td>\n",
       "      <td>-5.199415e+00</td>\n",
       "      <td>-7.108011e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.225139e-01</td>\n",
       "      <td>-6.173718e-01</td>\n",
       "      <td>-6.262052e-01</td>\n",
       "      <td>-6.319055e-01</td>\n",
       "      <td>-6.305686e-01</td>\n",
       "      <td>-6.539860e-01</td>\n",
       "      <td>-6.427274e-01</td>\n",
       "      <td>-6.326161e-01</td>\n",
       "      <td>-6.198444e-01</td>\n",
       "      <td>-6.096701e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.435937e-01</td>\n",
       "      <td>-6.397675e-01</td>\n",
       "      <td>-6.614029e-01</td>\n",
       "      <td>-6.346644e-01</td>\n",
       "      <td>-6.166865e-01</td>\n",
       "      <td>-6.801480e-01</td>\n",
       "      <td>-6.601366e-01</td>\n",
       "      <td>-6.117251e-01</td>\n",
       "      <td>-6.418793e-01</td>\n",
       "      <td>-6.227626e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.536865e-02</td>\n",
       "      <td>1.606233e-02</td>\n",
       "      <td>4.143598e-02</td>\n",
       "      <td>2.455183e-02</td>\n",
       "      <td>5.129331e-02</td>\n",
       "      <td>1.163906e-02</td>\n",
       "      <td>-2.702323e-02</td>\n",
       "      <td>-5.582847e-03</td>\n",
       "      <td>1.585602e-02</td>\n",
       "      <td>9.346582e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.012027e-02</td>\n",
       "      <td>1.235018e-03</td>\n",
       "      <td>-4.511427e-02</td>\n",
       "      <td>-4.334273e-04</td>\n",
       "      <td>4.115596e-02</td>\n",
       "      <td>-6.526293e-02</td>\n",
       "      <td>-1.300968e-02</td>\n",
       "      <td>2.048285e-02</td>\n",
       "      <td>3.744644e-02</td>\n",
       "      <td>3.542545e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.604330e-01</td>\n",
       "      <td>6.423465e-01</td>\n",
       "      <td>6.776879e-01</td>\n",
       "      <td>6.542170e-01</td>\n",
       "      <td>6.830048e-01</td>\n",
       "      <td>6.624647e-01</td>\n",
       "      <td>6.158727e-01</td>\n",
       "      <td>6.231269e-01</td>\n",
       "      <td>6.372821e-01</td>\n",
       "      <td>7.004243e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>6.120204e-01</td>\n",
       "      <td>6.385443e-01</td>\n",
       "      <td>6.103876e-01</td>\n",
       "      <td>6.315670e-01</td>\n",
       "      <td>6.560017e-01</td>\n",
       "      <td>6.227306e-01</td>\n",
       "      <td>6.463611e-01</td>\n",
       "      <td>6.382716e-01</td>\n",
       "      <td>6.811852e-01</td>\n",
       "      <td>6.553488e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.068933e+00</td>\n",
       "      <td>4.830957e+00</td>\n",
       "      <td>4.701224e+00</td>\n",
       "      <td>4.675281e+00</td>\n",
       "      <td>5.263099e+00</td>\n",
       "      <td>5.020806e+00</td>\n",
       "      <td>6.138458e+00</td>\n",
       "      <td>5.728607e+00</td>\n",
       "      <td>5.486503e+00</td>\n",
       "      <td>4.661029e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.400360e+00</td>\n",
       "      <td>5.735487e+00</td>\n",
       "      <td>6.577711e+00</td>\n",
       "      <td>6.951266e+00</td>\n",
       "      <td>5.399605e+00</td>\n",
       "      <td>5.530861e+00</td>\n",
       "      <td>6.506093e+00</td>\n",
       "      <td>6.276730e+00</td>\n",
       "      <td>4.186152e+00</td>\n",
       "      <td>6.961047e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4   \n",
       "count  5.000000e+04  5.000000e+04  5.000000e+04  5.000000e+04  5.000000e+04  \\\n",
       "mean  -3.173284e-16  3.925038e-16 -9.791279e-17  8.327561e-17  2.842171e-18   \n",
       "std    1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00   \n",
       "min   -6.202139e+00 -1.486309e+01 -6.757726e+00 -7.300654e+00 -7.937158e+00   \n",
       "25%   -6.225139e-01 -6.173718e-01 -6.262052e-01 -6.319055e-01 -6.305686e-01   \n",
       "50%    3.536865e-02  1.606233e-02  4.143598e-02  2.455183e-02  5.129331e-02   \n",
       "75%    6.604330e-01  6.423465e-01  6.776879e-01  6.542170e-01  6.830048e-01   \n",
       "max    5.068933e+00  4.830957e+00  4.701224e+00  4.675281e+00  5.263099e+00   \n",
       "\n",
       "                  5             6             7             8             9   \n",
       "count  5.000000e+04  5.000000e+04  5.000000e+04  5.000000e+04  5.000000e+04  \\\n",
       "mean  -5.124434e-16  5.172751e-17 -3.666401e-17  2.529532e-17 -8.014922e-17   \n",
       "std    1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00   \n",
       "min   -4.778139e+00 -6.168499e+00 -6.339533e+00 -7.330723e+00 -7.146061e+00   \n",
       "25%   -6.539860e-01 -6.427274e-01 -6.326161e-01 -6.198444e-01 -6.096701e-01   \n",
       "50%    1.163906e-02 -2.702323e-02 -5.582847e-03  1.585602e-02  9.346582e-02   \n",
       "75%    6.624647e-01  6.158727e-01  6.231269e-01  6.372821e-01  7.004243e-01   \n",
       "max    5.020806e+00  6.138458e+00  5.728607e+00  5.486503e+00  4.661029e+00   \n",
       "\n",
       "       ...            90            91            92            93   \n",
       "count  ...  5.000000e+04  5.000000e+04  5.000000e+04  5.000000e+04  \\\n",
       "mean   ...  2.636824e-16  5.329071e-16 -2.856382e-17  1.239187e-16   \n",
       "std    ...  1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00   \n",
       "min    ... -4.783539e+00 -7.973312e+00 -4.788789e+00 -7.246421e+00   \n",
       "25%    ... -6.435937e-01 -6.397675e-01 -6.614029e-01 -6.346644e-01   \n",
       "50%    ... -4.012027e-02  1.235018e-03 -4.511427e-02 -4.334273e-04   \n",
       "75%    ...  6.120204e-01  6.385443e-01  6.103876e-01  6.315670e-01   \n",
       "max    ...  6.400360e+00  5.735487e+00  6.577711e+00  6.951266e+00   \n",
       "\n",
       "                 94            95            96            97            98   \n",
       "count  5.000000e+04  5.000000e+04  5.000000e+04  5.000000e+04  5.000000e+04  \\\n",
       "mean  -2.842171e-17 -6.414780e-16  3.038281e-16 -2.927436e-16 -4.433787e-17   \n",
       "std    1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00  1.000010e+00   \n",
       "min   -5.562719e+00 -4.311398e+00 -5.486682e+00 -6.412036e+00 -5.199415e+00   \n",
       "25%   -6.166865e-01 -6.801480e-01 -6.601366e-01 -6.117251e-01 -6.418793e-01   \n",
       "50%    4.115596e-02 -6.526293e-02 -1.300968e-02  2.048285e-02  3.744644e-02   \n",
       "75%    6.560017e-01  6.227306e-01  6.463611e-01  6.382716e-01  6.811852e-01   \n",
       "max    5.399605e+00  5.530861e+00  6.506093e+00  6.276730e+00  4.186152e+00   \n",
       "\n",
       "                 99  \n",
       "count  5.000000e+04  \n",
       "mean  -3.026202e-16  \n",
       "std    1.000010e+00  \n",
       "min   -7.108011e+00  \n",
       "25%   -6.227626e-01  \n",
       "50%    3.542545e-02  \n",
       "75%    6.553488e-01  \n",
       "max    6.961047e+00  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.4154 - accuracy: 0.8045 - val_loss: 0.3502 - val_accuracy: 0.8487\n",
      "Epoch 2/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3340 - accuracy: 0.8575 - val_loss: 0.3386 - val_accuracy: 0.8536\n",
      "Epoch 3/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3202 - accuracy: 0.8654 - val_loss: 0.3377 - val_accuracy: 0.8545\n",
      "Epoch 4/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3118 - accuracy: 0.8677 - val_loss: 0.3316 - val_accuracy: 0.8577\n",
      "Epoch 5/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3057 - accuracy: 0.8704 - val_loss: 0.3286 - val_accuracy: 0.8585\n",
      "Epoch 6/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3000 - accuracy: 0.8725 - val_loss: 0.3351 - val_accuracy: 0.8555\n",
      "Epoch 7/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2953 - accuracy: 0.8750 - val_loss: 0.3303 - val_accuracy: 0.8577\n",
      "Epoch 8/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2892 - accuracy: 0.8775 - val_loss: 0.3323 - val_accuracy: 0.8580\n",
      "Epoch 9/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2858 - accuracy: 0.8803 - val_loss: 0.3310 - val_accuracy: 0.8575\n",
      "Epoch 10/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2809 - accuracy: 0.8819 - val_loss: 0.3369 - val_accuracy: 0.8577\n",
      "Epoch 11/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2762 - accuracy: 0.8847 - val_loss: 0.3375 - val_accuracy: 0.8546\n",
      "Epoch 12/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2718 - accuracy: 0.8857 - val_loss: 0.3358 - val_accuracy: 0.8558\n",
      "Epoch 13/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2667 - accuracy: 0.8874 - val_loss: 0.3386 - val_accuracy: 0.8582\n",
      "Epoch 14/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2630 - accuracy: 0.8913 - val_loss: 0.3455 - val_accuracy: 0.8543\n",
      "Epoch 15/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2579 - accuracy: 0.8934 - val_loss: 0.3437 - val_accuracy: 0.8534\n",
      "Epoch 16/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2536 - accuracy: 0.8952 - val_loss: 0.3470 - val_accuracy: 0.8533\n",
      "Epoch 17/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2504 - accuracy: 0.8971 - val_loss: 0.3512 - val_accuracy: 0.8516\n",
      "Epoch 18/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2468 - accuracy: 0.9004 - val_loss: 0.3553 - val_accuracy: 0.8509\n",
      "Epoch 19/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2426 - accuracy: 0.9002 - val_loss: 0.3545 - val_accuracy: 0.8540\n",
      "Epoch 20/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2382 - accuracy: 0.9015 - val_loss: 0.3571 - val_accuracy: 0.8534\n",
      "Epoch 21/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2352 - accuracy: 0.9036 - val_loss: 0.3661 - val_accuracy: 0.8515\n",
      "Epoch 22/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2313 - accuracy: 0.9069 - val_loss: 0.3703 - val_accuracy: 0.8498\n",
      "Epoch 23/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2282 - accuracy: 0.9076 - val_loss: 0.3691 - val_accuracy: 0.8494\n",
      "Epoch 24/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2244 - accuracy: 0.9083 - val_loss: 0.3735 - val_accuracy: 0.8474\n",
      "Epoch 25/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2212 - accuracy: 0.9107 - val_loss: 0.3818 - val_accuracy: 0.8457\n",
      "Epoch 26/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2175 - accuracy: 0.9120 - val_loss: 0.3800 - val_accuracy: 0.8489\n",
      "Epoch 27/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2151 - accuracy: 0.9143 - val_loss: 0.3888 - val_accuracy: 0.8446\n",
      "Epoch 28/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2110 - accuracy: 0.9145 - val_loss: 0.3911 - val_accuracy: 0.8454\n",
      "Epoch 29/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2081 - accuracy: 0.9166 - val_loss: 0.4084 - val_accuracy: 0.8427\n",
      "Epoch 30/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2059 - accuracy: 0.9170 - val_loss: 0.4014 - val_accuracy: 0.8459\n",
      "Epoch 31/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2029 - accuracy: 0.9198 - val_loss: 0.4103 - val_accuracy: 0.8415\n",
      "Epoch 32/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2000 - accuracy: 0.9199 - val_loss: 0.4144 - val_accuracy: 0.8436\n",
      "Epoch 33/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1976 - accuracy: 0.9211 - val_loss: 0.4136 - val_accuracy: 0.8431\n",
      "Epoch 34/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1949 - accuracy: 0.9224 - val_loss: 0.4228 - val_accuracy: 0.8428\n",
      "Epoch 35/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1916 - accuracy: 0.9233 - val_loss: 0.4263 - val_accuracy: 0.8419\n",
      "Epoch 36/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1899 - accuracy: 0.9254 - val_loss: 0.4299 - val_accuracy: 0.8398\n",
      "Epoch 37/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9254 - val_loss: 0.4466 - val_accuracy: 0.8389\n",
      "Epoch 38/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1860 - accuracy: 0.9275 - val_loss: 0.4411 - val_accuracy: 0.8399\n",
      "Epoch 39/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1827 - accuracy: 0.9290 - val_loss: 0.4480 - val_accuracy: 0.8381\n",
      "Epoch 40/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1804 - accuracy: 0.9297 - val_loss: 0.4523 - val_accuracy: 0.8384\n",
      "Epoch 41/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1789 - accuracy: 0.9312 - val_loss: 0.4578 - val_accuracy: 0.8383\n",
      "Epoch 42/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.9308 - val_loss: 0.4651 - val_accuracy: 0.8394\n",
      "Epoch 43/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1732 - accuracy: 0.9328 - val_loss: 0.4634 - val_accuracy: 0.8387\n",
      "Epoch 44/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1710 - accuracy: 0.9331 - val_loss: 0.4774 - val_accuracy: 0.8393\n",
      "Epoch 45/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1700 - accuracy: 0.9338 - val_loss: 0.4783 - val_accuracy: 0.8385\n",
      "Epoch 46/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1669 - accuracy: 0.9369 - val_loss: 0.4870 - val_accuracy: 0.8390\n",
      "Epoch 47/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1654 - accuracy: 0.9368 - val_loss: 0.4866 - val_accuracy: 0.8359\n",
      "Epoch 48/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1646 - accuracy: 0.9366 - val_loss: 0.4941 - val_accuracy: 0.8340\n",
      "Epoch 49/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1640 - accuracy: 0.9369 - val_loss: 0.4924 - val_accuracy: 0.8365\n",
      "Epoch 50/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1585 - accuracy: 0.9397 - val_loss: 0.5076 - val_accuracy: 0.8296\n",
      "Epoch 51/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1593 - accuracy: 0.9378 - val_loss: 0.5170 - val_accuracy: 0.8354\n",
      "Epoch 52/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1586 - accuracy: 0.9381 - val_loss: 0.5092 - val_accuracy: 0.8353\n",
      "Epoch 53/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1538 - accuracy: 0.9421 - val_loss: 0.5165 - val_accuracy: 0.8360\n",
      "Epoch 54/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1540 - accuracy: 0.9404 - val_loss: 0.5334 - val_accuracy: 0.8330\n",
      "Epoch 55/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1526 - accuracy: 0.9418 - val_loss: 0.5276 - val_accuracy: 0.8312\n",
      "Epoch 56/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1500 - accuracy: 0.9428 - val_loss: 0.5433 - val_accuracy: 0.8291\n",
      "Epoch 57/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1489 - accuracy: 0.9443 - val_loss: 0.5395 - val_accuracy: 0.8355\n",
      "Epoch 58/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1480 - accuracy: 0.9441 - val_loss: 0.5493 - val_accuracy: 0.8330\n",
      "Epoch 59/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1454 - accuracy: 0.9449 - val_loss: 0.5519 - val_accuracy: 0.8330\n",
      "Epoch 60/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1441 - accuracy: 0.9455 - val_loss: 0.5569 - val_accuracy: 0.8295\n",
      "Epoch 61/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1417 - accuracy: 0.9462 - val_loss: 0.5632 - val_accuracy: 0.8305\n",
      "Epoch 62/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1408 - accuracy: 0.9473 - val_loss: 0.5716 - val_accuracy: 0.8311\n",
      "Epoch 63/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1410 - accuracy: 0.9465 - val_loss: 0.5861 - val_accuracy: 0.8272\n",
      "Epoch 64/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1388 - accuracy: 0.9475 - val_loss: 0.5881 - val_accuracy: 0.8290\n",
      "Epoch 65/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1373 - accuracy: 0.9482 - val_loss: 0.5809 - val_accuracy: 0.8352\n",
      "Epoch 66/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1367 - accuracy: 0.9466 - val_loss: 0.5911 - val_accuracy: 0.8315\n",
      "Epoch 67/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1350 - accuracy: 0.9493 - val_loss: 0.6042 - val_accuracy: 0.8280\n",
      "Epoch 68/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1345 - accuracy: 0.9494 - val_loss: 0.6005 - val_accuracy: 0.8345\n",
      "Epoch 69/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1316 - accuracy: 0.9509 - val_loss: 0.6070 - val_accuracy: 0.8315\n",
      "Epoch 70/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1292 - accuracy: 0.9528 - val_loss: 0.6125 - val_accuracy: 0.8308\n",
      "Epoch 71/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1281 - accuracy: 0.9509 - val_loss: 0.6243 - val_accuracy: 0.8301\n",
      "Epoch 72/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1274 - accuracy: 0.9527 - val_loss: 0.6299 - val_accuracy: 0.8314\n",
      "Epoch 73/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1285 - accuracy: 0.9514 - val_loss: 0.6505 - val_accuracy: 0.8321\n",
      "Epoch 74/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1252 - accuracy: 0.9522 - val_loss: 0.6516 - val_accuracy: 0.8274\n",
      "Epoch 75/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1263 - accuracy: 0.9529 - val_loss: 0.6396 - val_accuracy: 0.8314\n",
      "Epoch 76/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1245 - accuracy: 0.9525 - val_loss: 0.6505 - val_accuracy: 0.8302\n",
      "Epoch 77/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1208 - accuracy: 0.9550 - val_loss: 0.6737 - val_accuracy: 0.8265\n",
      "Epoch 78/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1187 - accuracy: 0.9571 - val_loss: 0.6667 - val_accuracy: 0.8293\n",
      "Epoch 79/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1180 - accuracy: 0.9571 - val_loss: 0.6771 - val_accuracy: 0.8281\n",
      "Epoch 80/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1202 - accuracy: 0.9533 - val_loss: 0.6809 - val_accuracy: 0.8299\n",
      "Epoch 81/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1167 - accuracy: 0.9561 - val_loss: 0.6886 - val_accuracy: 0.8299\n",
      "Epoch 82/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1162 - accuracy: 0.9573 - val_loss: 0.7056 - val_accuracy: 0.8230\n",
      "Epoch 83/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1136 - accuracy: 0.9578 - val_loss: 0.7082 - val_accuracy: 0.8269\n",
      "Epoch 84/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1138 - accuracy: 0.9575 - val_loss: 0.7081 - val_accuracy: 0.8238\n",
      "Epoch 85/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1112 - accuracy: 0.9588 - val_loss: 0.7192 - val_accuracy: 0.8285\n",
      "Epoch 86/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1110 - accuracy: 0.9582 - val_loss: 0.7269 - val_accuracy: 0.8306\n",
      "Epoch 87/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1126 - accuracy: 0.9569 - val_loss: 0.7290 - val_accuracy: 0.8238\n",
      "Epoch 88/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1101 - accuracy: 0.9580 - val_loss: 0.7400 - val_accuracy: 0.8254\n",
      "Epoch 89/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1059 - accuracy: 0.9610 - val_loss: 0.7421 - val_accuracy: 0.8271\n",
      "Epoch 90/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1090 - accuracy: 0.9584 - val_loss: 0.7442 - val_accuracy: 0.8241\n",
      "Epoch 91/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1055 - accuracy: 0.9617 - val_loss: 0.7633 - val_accuracy: 0.8246\n",
      "Epoch 92/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1044 - accuracy: 0.9615 - val_loss: 0.7563 - val_accuracy: 0.8210\n",
      "Epoch 93/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1051 - accuracy: 0.9610 - val_loss: 0.7690 - val_accuracy: 0.8236\n",
      "Epoch 94/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1029 - accuracy: 0.9618 - val_loss: 0.7699 - val_accuracy: 0.8271\n",
      "Epoch 95/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1018 - accuracy: 0.9630 - val_loss: 0.7881 - val_accuracy: 0.8235\n",
      "Epoch 96/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.1018 - accuracy: 0.9615 - val_loss: 0.8012 - val_accuracy: 0.8265\n",
      "Epoch 97/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.0993 - accuracy: 0.9642 - val_loss: 0.7919 - val_accuracy: 0.8248\n",
      "Epoch 98/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.0989 - accuracy: 0.9644 - val_loss: 0.8068 - val_accuracy: 0.8219\n",
      "Epoch 99/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.0972 - accuracy: 0.9648 - val_loss: 0.8089 - val_accuracy: 0.8254\n",
      "Epoch 100/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.0978 - accuracy: 0.9639 - val_loss: 0.8183 - val_accuracy: 0.8211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d994ac56d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test([60, 20], 100, x_scaled, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "96.39% training accuracy with 82.11% validation accuracy\n",
    "\n",
    "- highly overfitting\n",
    "- ADD DROPOUT\n",
    "- try other scalers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN with 2 layers of 60 and 20 neurons (with MinMax scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = pd.DataFrame(MinMaxScaler().fit_transform(x), columns=x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.550271</td>\n",
       "      <td>0.754700</td>\n",
       "      <td>0.589733</td>\n",
       "      <td>0.609610</td>\n",
       "      <td>0.601288</td>\n",
       "      <td>0.487618</td>\n",
       "      <td>0.501220</td>\n",
       "      <td>0.525312</td>\n",
       "      <td>0.571943</td>\n",
       "      <td>0.605235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427717</td>\n",
       "      <td>0.581620</td>\n",
       "      <td>0.421307</td>\n",
       "      <td>0.510394</td>\n",
       "      <td>0.507440</td>\n",
       "      <td>0.438050</td>\n",
       "      <td>0.457499</td>\n",
       "      <td>0.505332</td>\n",
       "      <td>0.553980</td>\n",
       "      <td>0.505223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.088724</td>\n",
       "      <td>0.050777</td>\n",
       "      <td>0.087269</td>\n",
       "      <td>0.083502</td>\n",
       "      <td>0.075757</td>\n",
       "      <td>0.102053</td>\n",
       "      <td>0.081256</td>\n",
       "      <td>0.082864</td>\n",
       "      <td>0.078021</td>\n",
       "      <td>0.084696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089415</td>\n",
       "      <td>0.072947</td>\n",
       "      <td>0.087979</td>\n",
       "      <td>0.070435</td>\n",
       "      <td>0.091222</td>\n",
       "      <td>0.101604</td>\n",
       "      <td>0.083384</td>\n",
       "      <td>0.078811</td>\n",
       "      <td>0.106548</td>\n",
       "      <td>0.071079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.495039</td>\n",
       "      <td>0.723352</td>\n",
       "      <td>0.535086</td>\n",
       "      <td>0.556846</td>\n",
       "      <td>0.553519</td>\n",
       "      <td>0.420877</td>\n",
       "      <td>0.448996</td>\n",
       "      <td>0.472891</td>\n",
       "      <td>0.523583</td>\n",
       "      <td>0.553599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370170</td>\n",
       "      <td>0.534952</td>\n",
       "      <td>0.363118</td>\n",
       "      <td>0.465693</td>\n",
       "      <td>0.451185</td>\n",
       "      <td>0.368945</td>\n",
       "      <td>0.402454</td>\n",
       "      <td>0.457122</td>\n",
       "      <td>0.485590</td>\n",
       "      <td>0.460958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.553409</td>\n",
       "      <td>0.755515</td>\n",
       "      <td>0.593349</td>\n",
       "      <td>0.611660</td>\n",
       "      <td>0.605174</td>\n",
       "      <td>0.488805</td>\n",
       "      <td>0.499025</td>\n",
       "      <td>0.524849</td>\n",
       "      <td>0.573180</td>\n",
       "      <td>0.613151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424129</td>\n",
       "      <td>0.581710</td>\n",
       "      <td>0.417338</td>\n",
       "      <td>0.510364</td>\n",
       "      <td>0.511194</td>\n",
       "      <td>0.431419</td>\n",
       "      <td>0.456414</td>\n",
       "      <td>0.506946</td>\n",
       "      <td>0.557970</td>\n",
       "      <td>0.507741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.608866</td>\n",
       "      <td>0.787316</td>\n",
       "      <td>0.648874</td>\n",
       "      <td>0.664238</td>\n",
       "      <td>0.653030</td>\n",
       "      <td>0.555223</td>\n",
       "      <td>0.551263</td>\n",
       "      <td>0.576946</td>\n",
       "      <td>0.621664</td>\n",
       "      <td>0.664557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482440</td>\n",
       "      <td>0.628199</td>\n",
       "      <td>0.475008</td>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.567281</td>\n",
       "      <td>0.501321</td>\n",
       "      <td>0.511395</td>\n",
       "      <td>0.555634</td>\n",
       "      <td>0.626558</td>\n",
       "      <td>0.551804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4   \n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000  \\\n",
       "mean       0.550271      0.754700      0.589733      0.609610      0.601288   \n",
       "std        0.088724      0.050777      0.087269      0.083502      0.075757   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.495039      0.723352      0.535086      0.556846      0.553519   \n",
       "50%        0.553409      0.755515      0.593349      0.611660      0.605174   \n",
       "75%        0.608866      0.787316      0.648874      0.664238      0.653030   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                  5             6             7             8             9   \n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000  \\\n",
       "mean       0.487618      0.501220      0.525312      0.571943      0.605235   \n",
       "std        0.102053      0.081256      0.082864      0.078021      0.084696   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.420877      0.448996      0.472891      0.523583      0.553599   \n",
       "50%        0.488805      0.499025      0.524849      0.573180      0.613151   \n",
       "75%        0.555223      0.551263      0.576946      0.621664      0.664557   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...            90            91            92            93   \n",
       "count  ...  50000.000000  50000.000000  50000.000000  50000.000000  \\\n",
       "mean   ...      0.427717      0.581620      0.421307      0.510394   \n",
       "std    ...      0.089415      0.072947      0.087979      0.070435   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.370170      0.534952      0.363118      0.465693   \n",
       "50%    ...      0.424129      0.581710      0.417338      0.510364   \n",
       "75%    ...      0.482440      0.628199      0.475008      0.554878   \n",
       "max    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 94            95            96            97            98   \n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000  \\\n",
       "mean       0.507440      0.438050      0.457499      0.505332      0.553980   \n",
       "std        0.091222      0.101604      0.083384      0.078811      0.106548   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.451185      0.368945      0.402454      0.457122      0.485590   \n",
       "50%        0.511194      0.431419      0.456414      0.506946      0.557970   \n",
       "75%        0.567281      0.501321      0.511395      0.555634      0.626558   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 99  \n",
       "count  50000.000000  \n",
       "mean       0.505223  \n",
       "std        0.071079  \n",
       "min        0.000000  \n",
       "25%        0.460958  \n",
       "50%        0.507741  \n",
       "75%        0.551804  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.5706 - accuracy: 0.7226 - val_loss: 0.4439 - val_accuracy: 0.8082\n",
      "Epoch 2/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.4083 - accuracy: 0.8169 - val_loss: 0.3763 - val_accuracy: 0.8378\n",
      "Epoch 3/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3790 - accuracy: 0.8338 - val_loss: 0.3601 - val_accuracy: 0.8474\n",
      "Epoch 4/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3680 - accuracy: 0.8390 - val_loss: 0.3597 - val_accuracy: 0.8453\n",
      "Epoch 5/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3601 - accuracy: 0.8426 - val_loss: 0.3495 - val_accuracy: 0.8503\n",
      "Epoch 6/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3606 - accuracy: 0.8418 - val_loss: 0.3485 - val_accuracy: 0.8505\n",
      "Epoch 7/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3568 - accuracy: 0.8447 - val_loss: 0.3463 - val_accuracy: 0.8524\n",
      "Epoch 8/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3513 - accuracy: 0.8473 - val_loss: 0.3436 - val_accuracy: 0.8518\n",
      "Epoch 9/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3505 - accuracy: 0.8475 - val_loss: 0.3478 - val_accuracy: 0.8486\n",
      "Epoch 10/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3489 - accuracy: 0.8480 - val_loss: 0.3406 - val_accuracy: 0.8542\n",
      "Epoch 11/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3488 - accuracy: 0.8488 - val_loss: 0.3404 - val_accuracy: 0.8533\n",
      "Epoch 12/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3513 - accuracy: 0.8476 - val_loss: 0.3414 - val_accuracy: 0.8521\n",
      "Epoch 13/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3457 - accuracy: 0.8506 - val_loss: 0.3417 - val_accuracy: 0.8519\n",
      "Epoch 14/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3478 - accuracy: 0.8491 - val_loss: 0.3538 - val_accuracy: 0.8449\n",
      "Epoch 15/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3456 - accuracy: 0.8501 - val_loss: 0.3399 - val_accuracy: 0.8522\n",
      "Epoch 16/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3421 - accuracy: 0.8522 - val_loss: 0.3510 - val_accuracy: 0.8475\n",
      "Epoch 17/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3472 - accuracy: 0.8493 - val_loss: 0.3406 - val_accuracy: 0.8528\n",
      "Epoch 18/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3427 - accuracy: 0.8522 - val_loss: 0.3377 - val_accuracy: 0.8533\n",
      "Epoch 19/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3475 - accuracy: 0.8478 - val_loss: 0.3354 - val_accuracy: 0.8535\n",
      "Epoch 20/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3417 - accuracy: 0.8532 - val_loss: 0.3496 - val_accuracy: 0.8484\n",
      "Epoch 21/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3433 - accuracy: 0.8509 - val_loss: 0.3353 - val_accuracy: 0.8527\n",
      "Epoch 22/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3423 - accuracy: 0.8511 - val_loss: 0.3364 - val_accuracy: 0.8525\n",
      "Epoch 23/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3421 - accuracy: 0.8529 - val_loss: 0.3358 - val_accuracy: 0.8544\n",
      "Epoch 24/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3433 - accuracy: 0.8521 - val_loss: 0.3788 - val_accuracy: 0.8290\n",
      "Epoch 25/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3422 - accuracy: 0.8521 - val_loss: 0.3508 - val_accuracy: 0.8453\n",
      "Epoch 26/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3384 - accuracy: 0.8535 - val_loss: 0.3337 - val_accuracy: 0.8546\n",
      "Epoch 27/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3389 - accuracy: 0.8548 - val_loss: 0.3354 - val_accuracy: 0.8557\n",
      "Epoch 28/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3405 - accuracy: 0.8538 - val_loss: 0.3477 - val_accuracy: 0.8464\n",
      "Epoch 29/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3391 - accuracy: 0.8542 - val_loss: 0.3417 - val_accuracy: 0.8483\n",
      "Epoch 30/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3376 - accuracy: 0.8545 - val_loss: 0.3371 - val_accuracy: 0.8527\n",
      "Epoch 31/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3399 - accuracy: 0.8530 - val_loss: 0.3403 - val_accuracy: 0.8521\n",
      "Epoch 32/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3386 - accuracy: 0.8540 - val_loss: 0.3337 - val_accuracy: 0.8555\n",
      "Epoch 33/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3393 - accuracy: 0.8531 - val_loss: 0.3335 - val_accuracy: 0.8547\n",
      "Epoch 34/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3404 - accuracy: 0.8514 - val_loss: 0.3335 - val_accuracy: 0.8552\n",
      "Epoch 35/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3415 - accuracy: 0.8523 - val_loss: 0.3349 - val_accuracy: 0.8549\n",
      "Epoch 36/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3370 - accuracy: 0.8541 - val_loss: 0.3321 - val_accuracy: 0.8560\n",
      "Epoch 37/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3361 - accuracy: 0.8551 - val_loss: 0.3331 - val_accuracy: 0.8558\n",
      "Epoch 38/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3343 - accuracy: 0.8557 - val_loss: 0.3326 - val_accuracy: 0.8537\n",
      "Epoch 39/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3361 - accuracy: 0.8544 - val_loss: 0.3326 - val_accuracy: 0.8568\n",
      "Epoch 40/100\n",
      "268/268 [==============================] - 1s 4ms/step - loss: 0.3366 - accuracy: 0.8548 - val_loss: 0.3453 - val_accuracy: 0.8496\n",
      "Epoch 41/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3374 - accuracy: 0.8534 - val_loss: 0.3394 - val_accuracy: 0.8532\n",
      "Epoch 42/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3345 - accuracy: 0.8548 - val_loss: 0.3368 - val_accuracy: 0.8505\n",
      "Epoch 43/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3358 - accuracy: 0.8549 - val_loss: 0.3543 - val_accuracy: 0.8428\n",
      "Epoch 44/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3368 - accuracy: 0.8538 - val_loss: 0.3302 - val_accuracy: 0.8569\n",
      "Epoch 45/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3358 - accuracy: 0.8543 - val_loss: 0.3447 - val_accuracy: 0.8490\n",
      "Epoch 46/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3343 - accuracy: 0.8534 - val_loss: 0.3302 - val_accuracy: 0.8571\n",
      "Epoch 47/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8562 - val_loss: 0.3326 - val_accuracy: 0.8566\n",
      "Epoch 48/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3353 - accuracy: 0.8545 - val_loss: 0.3297 - val_accuracy: 0.8572\n",
      "Epoch 49/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3324 - accuracy: 0.8562 - val_loss: 0.3324 - val_accuracy: 0.8566\n",
      "Epoch 50/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3334 - accuracy: 0.8548 - val_loss: 0.3498 - val_accuracy: 0.8485\n",
      "Epoch 51/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3332 - accuracy: 0.8563 - val_loss: 0.3353 - val_accuracy: 0.8519\n",
      "Epoch 52/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 0.8563 - val_loss: 0.3321 - val_accuracy: 0.8540\n",
      "Epoch 53/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3308 - accuracy: 0.8557 - val_loss: 0.3312 - val_accuracy: 0.8564\n",
      "Epoch 54/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3351 - accuracy: 0.8542 - val_loss: 0.3807 - val_accuracy: 0.8315\n",
      "Epoch 55/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3307 - accuracy: 0.8565 - val_loss: 0.3297 - val_accuracy: 0.8562\n",
      "Epoch 56/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3340 - accuracy: 0.8534 - val_loss: 0.3330 - val_accuracy: 0.8516\n",
      "Epoch 57/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3330 - accuracy: 0.8553 - val_loss: 0.3314 - val_accuracy: 0.8547\n",
      "Epoch 58/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3332 - accuracy: 0.8554 - val_loss: 0.3337 - val_accuracy: 0.8567\n",
      "Epoch 59/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3291 - accuracy: 0.8566 - val_loss: 0.3449 - val_accuracy: 0.8502\n",
      "Epoch 60/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3295 - accuracy: 0.8563 - val_loss: 0.3469 - val_accuracy: 0.8495\n",
      "Epoch 61/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3333 - accuracy: 0.8538 - val_loss: 0.3421 - val_accuracy: 0.8517\n",
      "Epoch 62/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3377 - accuracy: 0.8538 - val_loss: 0.3296 - val_accuracy: 0.8575\n",
      "Epoch 63/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3298 - accuracy: 0.8575 - val_loss: 0.3503 - val_accuracy: 0.8477\n",
      "Epoch 64/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3301 - accuracy: 0.8577 - val_loss: 0.3347 - val_accuracy: 0.8540\n",
      "Epoch 65/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3284 - accuracy: 0.8569 - val_loss: 0.3584 - val_accuracy: 0.8396\n",
      "Epoch 66/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3299 - accuracy: 0.8554 - val_loss: 0.3363 - val_accuracy: 0.8506\n",
      "Epoch 67/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3281 - accuracy: 0.8571 - val_loss: 0.3284 - val_accuracy: 0.8571\n",
      "Epoch 68/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3297 - accuracy: 0.8568 - val_loss: 0.3296 - val_accuracy: 0.8561\n",
      "Epoch 69/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3266 - accuracy: 0.8578 - val_loss: 0.3496 - val_accuracy: 0.8448\n",
      "Epoch 70/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3298 - accuracy: 0.8582 - val_loss: 0.3299 - val_accuracy: 0.8570\n",
      "Epoch 71/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3260 - accuracy: 0.8597 - val_loss: 0.3581 - val_accuracy: 0.8407\n",
      "Epoch 72/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3287 - accuracy: 0.8556 - val_loss: 0.3303 - val_accuracy: 0.8571\n",
      "Epoch 73/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3257 - accuracy: 0.8600 - val_loss: 0.3472 - val_accuracy: 0.8465\n",
      "Epoch 74/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3292 - accuracy: 0.8573 - val_loss: 0.3309 - val_accuracy: 0.8577\n",
      "Epoch 75/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3267 - accuracy: 0.8590 - val_loss: 0.3346 - val_accuracy: 0.8557\n",
      "Epoch 76/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3272 - accuracy: 0.8575 - val_loss: 0.3517 - val_accuracy: 0.8468\n",
      "Epoch 77/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3274 - accuracy: 0.8577 - val_loss: 0.3292 - val_accuracy: 0.8564\n",
      "Epoch 78/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3283 - accuracy: 0.8564 - val_loss: 0.3302 - val_accuracy: 0.8558\n",
      "Epoch 79/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3242 - accuracy: 0.8604 - val_loss: 0.3349 - val_accuracy: 0.8528\n",
      "Epoch 80/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3239 - accuracy: 0.8613 - val_loss: 0.3385 - val_accuracy: 0.8543\n",
      "Epoch 81/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3250 - accuracy: 0.8599 - val_loss: 0.3339 - val_accuracy: 0.8548\n",
      "Epoch 82/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3285 - accuracy: 0.8567 - val_loss: 0.3327 - val_accuracy: 0.8564\n",
      "Epoch 83/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3252 - accuracy: 0.8588 - val_loss: 0.3340 - val_accuracy: 0.8536\n",
      "Epoch 84/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3255 - accuracy: 0.8604 - val_loss: 0.3356 - val_accuracy: 0.8534\n",
      "Epoch 85/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3226 - accuracy: 0.8605 - val_loss: 0.3490 - val_accuracy: 0.8493\n",
      "Epoch 86/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3234 - accuracy: 0.8596 - val_loss: 0.3299 - val_accuracy: 0.8562\n",
      "Epoch 87/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3237 - accuracy: 0.8592 - val_loss: 0.3281 - val_accuracy: 0.8562\n",
      "Epoch 88/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3242 - accuracy: 0.8600 - val_loss: 0.3581 - val_accuracy: 0.8441\n",
      "Epoch 89/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3250 - accuracy: 0.8594 - val_loss: 0.3341 - val_accuracy: 0.8547\n",
      "Epoch 90/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3239 - accuracy: 0.8597 - val_loss: 0.3572 - val_accuracy: 0.8445\n",
      "Epoch 91/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3279 - accuracy: 0.8569 - val_loss: 0.3336 - val_accuracy: 0.8558\n",
      "Epoch 92/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3205 - accuracy: 0.8616 - val_loss: 0.3293 - val_accuracy: 0.8569\n",
      "Epoch 93/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3231 - accuracy: 0.8589 - val_loss: 0.3285 - val_accuracy: 0.8586\n",
      "Epoch 94/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3241 - accuracy: 0.8606 - val_loss: 0.3338 - val_accuracy: 0.8546\n",
      "Epoch 95/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3252 - accuracy: 0.8593 - val_loss: 0.3272 - val_accuracy: 0.8580\n",
      "Epoch 96/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3212 - accuracy: 0.8609 - val_loss: 0.3293 - val_accuracy: 0.8586\n",
      "Epoch 97/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3216 - accuracy: 0.8598 - val_loss: 0.3266 - val_accuracy: 0.8586\n",
      "Epoch 98/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3245 - accuracy: 0.8585 - val_loss: 0.3413 - val_accuracy: 0.8515\n",
      "Epoch 99/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3217 - accuracy: 0.8622 - val_loss: 0.3285 - val_accuracy: 0.8580\n",
      "Epoch 100/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3196 - accuracy: 0.8611 - val_loss: 0.3298 - val_accuracy: 0.8565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d98cf16f50>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test([60, 20], 100, x_scaled, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "86.11% training accuracy with 85.65% validation accuracy\n",
    "\n",
    "- accuracy dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_add_dropout(layers, dropouts, epochs, x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y,\n",
    "        test_size = 0.2\n",
    "    )\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    for units, dropout in zip(layers, dropouts):\n",
    "        model.add(\n",
    "            keras.layers.Dropout(dropout)\n",
    "        )\n",
    "        model.add(\n",
    "            keras.layers.Dense(\n",
    "                units = units,\n",
    "                kernel_initializer = 'he_uniform',\n",
    "                activation = 'relu'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    model.add(\n",
    "        keras.layers.Dense(\n",
    "            units = 1,\n",
    "            kernel_initializer = 'glorot_uniform',\n",
    "            activation = 'sigmoid',\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = 'adam',\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_split = 0.33,\n",
    "        batch_size = 100,\n",
    "        epochs = epochs\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN with 2 layers of 60 and 20 neurons (with 0.1 Dropout Layer) (without scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.4357 - accuracy: 0.7981 - val_loss: 0.3947 - val_accuracy: 0.8205\n",
      "Epoch 2/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3854 - accuracy: 0.8305 - val_loss: 0.3546 - val_accuracy: 0.8493\n",
      "Epoch 3/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3815 - accuracy: 0.8307 - val_loss: 0.3775 - val_accuracy: 0.8318\n",
      "Epoch 4/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3822 - accuracy: 0.8265 - val_loss: 0.3540 - val_accuracy: 0.8484\n",
      "Epoch 5/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3758 - accuracy: 0.8320 - val_loss: 0.3481 - val_accuracy: 0.8485\n",
      "Epoch 6/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3716 - accuracy: 0.8335 - val_loss: 0.3482 - val_accuracy: 0.8496\n",
      "Epoch 7/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3720 - accuracy: 0.8362 - val_loss: 0.3469 - val_accuracy: 0.8475\n",
      "Epoch 8/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3658 - accuracy: 0.8389 - val_loss: 0.3438 - val_accuracy: 0.8515\n",
      "Epoch 9/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3696 - accuracy: 0.8363 - val_loss: 0.3494 - val_accuracy: 0.8484\n",
      "Epoch 10/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3654 - accuracy: 0.8390 - val_loss: 0.3501 - val_accuracy: 0.8470\n",
      "Epoch 11/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3619 - accuracy: 0.8373 - val_loss: 0.3423 - val_accuracy: 0.8528\n",
      "Epoch 12/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3617 - accuracy: 0.8390 - val_loss: 0.3426 - val_accuracy: 0.8502\n",
      "Epoch 13/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3644 - accuracy: 0.8379 - val_loss: 0.3424 - val_accuracy: 0.8531\n",
      "Epoch 14/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3622 - accuracy: 0.8387 - val_loss: 0.3457 - val_accuracy: 0.8520\n",
      "Epoch 15/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3600 - accuracy: 0.8387 - val_loss: 0.3435 - val_accuracy: 0.8515\n",
      "Epoch 16/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3618 - accuracy: 0.8378 - val_loss: 0.3438 - val_accuracy: 0.8529\n",
      "Epoch 17/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3581 - accuracy: 0.8431 - val_loss: 0.3426 - val_accuracy: 0.8538\n",
      "Epoch 18/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3590 - accuracy: 0.8403 - val_loss: 0.3432 - val_accuracy: 0.8524\n",
      "Epoch 19/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3561 - accuracy: 0.8440 - val_loss: 0.3439 - val_accuracy: 0.8499\n",
      "Epoch 20/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3562 - accuracy: 0.8413 - val_loss: 0.3453 - val_accuracy: 0.8527\n",
      "Epoch 21/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3576 - accuracy: 0.8410 - val_loss: 0.3395 - val_accuracy: 0.8549\n",
      "Epoch 22/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3550 - accuracy: 0.8429 - val_loss: 0.3396 - val_accuracy: 0.8537\n",
      "Epoch 23/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3520 - accuracy: 0.8439 - val_loss: 0.3401 - val_accuracy: 0.8536\n",
      "Epoch 24/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3517 - accuracy: 0.8461 - val_loss: 0.3414 - val_accuracy: 0.8528\n",
      "Epoch 25/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3520 - accuracy: 0.8447 - val_loss: 0.3372 - val_accuracy: 0.8536\n",
      "Epoch 26/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3515 - accuracy: 0.8445 - val_loss: 0.3389 - val_accuracy: 0.8543\n",
      "Epoch 27/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3513 - accuracy: 0.8447 - val_loss: 0.3373 - val_accuracy: 0.8540\n",
      "Epoch 28/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3519 - accuracy: 0.8444 - val_loss: 0.3403 - val_accuracy: 0.8541\n",
      "Epoch 29/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3471 - accuracy: 0.8468 - val_loss: 0.3364 - val_accuracy: 0.8553\n",
      "Epoch 30/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3476 - accuracy: 0.8467 - val_loss: 0.3432 - val_accuracy: 0.8533\n",
      "Epoch 31/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3506 - accuracy: 0.8438 - val_loss: 0.3399 - val_accuracy: 0.8522\n",
      "Epoch 32/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3470 - accuracy: 0.8470 - val_loss: 0.3393 - val_accuracy: 0.8532\n",
      "Epoch 33/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3473 - accuracy: 0.8467 - val_loss: 0.3410 - val_accuracy: 0.8528\n",
      "Epoch 34/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3461 - accuracy: 0.8467 - val_loss: 0.3373 - val_accuracy: 0.8552\n",
      "Epoch 35/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3441 - accuracy: 0.8484 - val_loss: 0.3476 - val_accuracy: 0.8490\n",
      "Epoch 36/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3434 - accuracy: 0.8479 - val_loss: 0.3472 - val_accuracy: 0.8522\n",
      "Epoch 37/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3428 - accuracy: 0.8520 - val_loss: 0.3392 - val_accuracy: 0.8523\n",
      "Epoch 38/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3416 - accuracy: 0.8515 - val_loss: 0.3382 - val_accuracy: 0.8571\n",
      "Epoch 39/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3441 - accuracy: 0.8470 - val_loss: 0.3409 - val_accuracy: 0.8514\n",
      "Epoch 40/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3439 - accuracy: 0.8486 - val_loss: 0.3451 - val_accuracy: 0.8523\n",
      "Epoch 41/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3449 - accuracy: 0.8475 - val_loss: 0.3412 - val_accuracy: 0.8518\n",
      "Epoch 42/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3406 - accuracy: 0.8511 - val_loss: 0.3348 - val_accuracy: 0.8570\n",
      "Epoch 43/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3427 - accuracy: 0.8487 - val_loss: 0.3391 - val_accuracy: 0.8518\n",
      "Epoch 44/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3451 - accuracy: 0.8480 - val_loss: 0.3435 - val_accuracy: 0.8543\n",
      "Epoch 45/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3403 - accuracy: 0.8528 - val_loss: 0.3405 - val_accuracy: 0.8556\n",
      "Epoch 46/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3424 - accuracy: 0.8497 - val_loss: 0.3346 - val_accuracy: 0.8573\n",
      "Epoch 47/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3389 - accuracy: 0.8505 - val_loss: 0.3347 - val_accuracy: 0.8577\n",
      "Epoch 48/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3409 - accuracy: 0.8498 - val_loss: 0.3421 - val_accuracy: 0.8517\n",
      "Epoch 49/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3409 - accuracy: 0.8493 - val_loss: 0.3470 - val_accuracy: 0.8519\n",
      "Epoch 50/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3369 - accuracy: 0.8518 - val_loss: 0.3372 - val_accuracy: 0.8537\n",
      "Epoch 51/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3378 - accuracy: 0.8498 - val_loss: 0.3364 - val_accuracy: 0.8555\n",
      "Epoch 52/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3376 - accuracy: 0.8516 - val_loss: 0.3401 - val_accuracy: 0.8526\n",
      "Epoch 53/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3383 - accuracy: 0.8485 - val_loss: 0.3383 - val_accuracy: 0.8546\n",
      "Epoch 54/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3344 - accuracy: 0.8535 - val_loss: 0.3348 - val_accuracy: 0.8563\n",
      "Epoch 55/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3387 - accuracy: 0.8507 - val_loss: 0.3390 - val_accuracy: 0.8530\n",
      "Epoch 56/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3377 - accuracy: 0.8514 - val_loss: 0.3380 - val_accuracy: 0.8540\n",
      "Epoch 57/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3372 - accuracy: 0.8518 - val_loss: 0.3428 - val_accuracy: 0.8496\n",
      "Epoch 58/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3338 - accuracy: 0.8525 - val_loss: 0.3385 - val_accuracy: 0.8549\n",
      "Epoch 59/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3373 - accuracy: 0.8502 - val_loss: 0.3359 - val_accuracy: 0.8541\n",
      "Epoch 60/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3335 - accuracy: 0.8545 - val_loss: 0.3371 - val_accuracy: 0.8542\n",
      "Epoch 61/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3343 - accuracy: 0.8514 - val_loss: 0.3375 - val_accuracy: 0.8541\n",
      "Epoch 62/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3329 - accuracy: 0.8551 - val_loss: 0.3461 - val_accuracy: 0.8478\n",
      "Epoch 63/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3387 - accuracy: 0.8485 - val_loss: 0.3384 - val_accuracy: 0.8531\n",
      "Epoch 64/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3367 - accuracy: 0.8506 - val_loss: 0.3381 - val_accuracy: 0.8549\n",
      "Epoch 65/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3283 - accuracy: 0.8528 - val_loss: 0.3430 - val_accuracy: 0.8500\n",
      "Epoch 66/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3324 - accuracy: 0.8533 - val_loss: 0.3405 - val_accuracy: 0.8535\n",
      "Epoch 67/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3350 - accuracy: 0.8531 - val_loss: 0.3405 - val_accuracy: 0.8546\n",
      "Epoch 68/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8535 - val_loss: 0.3352 - val_accuracy: 0.8564\n",
      "Epoch 69/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8555 - val_loss: 0.3425 - val_accuracy: 0.8519\n",
      "Epoch 70/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3288 - accuracy: 0.8561 - val_loss: 0.3538 - val_accuracy: 0.8460\n",
      "Epoch 71/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3290 - accuracy: 0.8554 - val_loss: 0.3476 - val_accuracy: 0.8494\n",
      "Epoch 72/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3303 - accuracy: 0.8526 - val_loss: 0.3436 - val_accuracy: 0.8521\n",
      "Epoch 73/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3301 - accuracy: 0.8547 - val_loss: 0.3359 - val_accuracy: 0.8564\n",
      "Epoch 74/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3303 - accuracy: 0.8534 - val_loss: 0.3359 - val_accuracy: 0.8572\n",
      "Epoch 75/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3310 - accuracy: 0.8531 - val_loss: 0.3352 - val_accuracy: 0.8549\n",
      "Epoch 76/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3287 - accuracy: 0.8552 - val_loss: 0.3369 - val_accuracy: 0.8583\n",
      "Epoch 77/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3285 - accuracy: 0.8553 - val_loss: 0.3377 - val_accuracy: 0.8562\n",
      "Epoch 78/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3281 - accuracy: 0.8555 - val_loss: 0.3394 - val_accuracy: 0.8543\n",
      "Epoch 79/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3276 - accuracy: 0.8575 - val_loss: 0.3378 - val_accuracy: 0.8563\n",
      "Epoch 80/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3256 - accuracy: 0.8562 - val_loss: 0.3353 - val_accuracy: 0.8579\n",
      "Epoch 81/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3296 - accuracy: 0.8542 - val_loss: 0.3374 - val_accuracy: 0.8553\n",
      "Epoch 82/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3290 - accuracy: 0.8546 - val_loss: 0.3348 - val_accuracy: 0.8569\n",
      "Epoch 83/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3304 - accuracy: 0.8554 - val_loss: 0.3394 - val_accuracy: 0.8554\n",
      "Epoch 84/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3268 - accuracy: 0.8557 - val_loss: 0.3374 - val_accuracy: 0.8558\n",
      "Epoch 85/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3270 - accuracy: 0.8568 - val_loss: 0.3366 - val_accuracy: 0.8560\n",
      "Epoch 86/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3244 - accuracy: 0.8593 - val_loss: 0.3407 - val_accuracy: 0.8537\n",
      "Epoch 87/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3274 - accuracy: 0.8553 - val_loss: 0.3396 - val_accuracy: 0.8545\n",
      "Epoch 88/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3251 - accuracy: 0.8584 - val_loss: 0.3402 - val_accuracy: 0.8545\n",
      "Epoch 89/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3256 - accuracy: 0.8569 - val_loss: 0.3369 - val_accuracy: 0.8561\n",
      "Epoch 90/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3260 - accuracy: 0.8570 - val_loss: 0.3423 - val_accuracy: 0.8547\n",
      "Epoch 91/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3253 - accuracy: 0.8575 - val_loss: 0.3393 - val_accuracy: 0.8551\n",
      "Epoch 92/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3252 - accuracy: 0.8564 - val_loss: 0.3416 - val_accuracy: 0.8524\n",
      "Epoch 93/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3242 - accuracy: 0.8576 - val_loss: 0.3369 - val_accuracy: 0.8569\n",
      "Epoch 94/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3230 - accuracy: 0.8584 - val_loss: 0.3373 - val_accuracy: 0.8569\n",
      "Epoch 95/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3241 - accuracy: 0.8573 - val_loss: 0.3385 - val_accuracy: 0.8549\n",
      "Epoch 96/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3262 - accuracy: 0.8561 - val_loss: 0.3411 - val_accuracy: 0.8529\n",
      "Epoch 97/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3271 - accuracy: 0.8555 - val_loss: 0.3442 - val_accuracy: 0.8510\n",
      "Epoch 98/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3240 - accuracy: 0.8604 - val_loss: 0.3448 - val_accuracy: 0.8514\n",
      "Epoch 99/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3230 - accuracy: 0.8576 - val_loss: 0.3421 - val_accuracy: 0.8538\n",
      "Epoch 100/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3253 - accuracy: 0.8554 - val_loss: 0.3415 - val_accuracy: 0.8530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1da04497150>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_add_dropout(\n",
    "    [60, 20],\n",
    "    [0.1, 0],\n",
    "    100,\n",
    "    x, y\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85.54% training accuracy with 85.30% validation accuracy\n",
    "\n",
    "- accuracy dropped\n",
    "- equivalent accuracy (training and validating)\n",
    "- try adding another layer\n",
    "- try increasing epochs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN with 3 layers of 60, 30, 10 neurons (with 0.1 Dropout Layer) (without scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "268/268 [==============================] - 2s 3ms/step - loss: 0.4563 - accuracy: 0.7850 - val_loss: 0.3588 - val_accuracy: 0.8436\n",
      "Epoch 2/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3897 - accuracy: 0.8260 - val_loss: 0.3501 - val_accuracy: 0.8485\n",
      "Epoch 3/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3888 - accuracy: 0.8261 - val_loss: 0.3506 - val_accuracy: 0.8446\n",
      "Epoch 4/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3825 - accuracy: 0.8313 - val_loss: 0.3496 - val_accuracy: 0.8488\n",
      "Epoch 5/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3826 - accuracy: 0.8295 - val_loss: 0.3483 - val_accuracy: 0.8471\n",
      "Epoch 6/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3782 - accuracy: 0.8323 - val_loss: 0.3369 - val_accuracy: 0.8537\n",
      "Epoch 7/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3765 - accuracy: 0.8337 - val_loss: 0.3375 - val_accuracy: 0.8525\n",
      "Epoch 8/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3736 - accuracy: 0.8336 - val_loss: 0.3427 - val_accuracy: 0.8501\n",
      "Epoch 9/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3745 - accuracy: 0.8333 - val_loss: 0.3373 - val_accuracy: 0.8526\n",
      "Epoch 10/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3736 - accuracy: 0.8318 - val_loss: 0.3348 - val_accuracy: 0.8535\n",
      "Epoch 11/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3725 - accuracy: 0.8328 - val_loss: 0.3339 - val_accuracy: 0.8543\n",
      "Epoch 12/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3700 - accuracy: 0.8356 - val_loss: 0.3509 - val_accuracy: 0.8463\n",
      "Epoch 13/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3661 - accuracy: 0.8385 - val_loss: 0.3332 - val_accuracy: 0.8542\n",
      "Epoch 14/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3675 - accuracy: 0.8376 - val_loss: 0.3357 - val_accuracy: 0.8538\n",
      "Epoch 15/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3663 - accuracy: 0.8368 - val_loss: 0.3326 - val_accuracy: 0.8561\n",
      "Epoch 16/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3653 - accuracy: 0.8364 - val_loss: 0.3348 - val_accuracy: 0.8539\n",
      "Epoch 17/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3595 - accuracy: 0.8403 - val_loss: 0.3325 - val_accuracy: 0.8544\n",
      "Epoch 18/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3616 - accuracy: 0.8399 - val_loss: 0.3321 - val_accuracy: 0.8574\n",
      "Epoch 19/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3601 - accuracy: 0.8404 - val_loss: 0.3370 - val_accuracy: 0.8523\n",
      "Epoch 20/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3611 - accuracy: 0.8407 - val_loss: 0.3303 - val_accuracy: 0.8580\n",
      "Epoch 21/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3600 - accuracy: 0.8418 - val_loss: 0.3435 - val_accuracy: 0.8508\n",
      "Epoch 22/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3587 - accuracy: 0.8408 - val_loss: 0.3287 - val_accuracy: 0.8570\n",
      "Epoch 23/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3597 - accuracy: 0.8400 - val_loss: 0.3304 - val_accuracy: 0.8569\n",
      "Epoch 24/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3550 - accuracy: 0.8435 - val_loss: 0.3323 - val_accuracy: 0.8563\n",
      "Epoch 25/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3535 - accuracy: 0.8452 - val_loss: 0.3347 - val_accuracy: 0.8540\n",
      "Epoch 26/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3532 - accuracy: 0.8428 - val_loss: 0.3324 - val_accuracy: 0.8536\n",
      "Epoch 27/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3557 - accuracy: 0.8428 - val_loss: 0.3246 - val_accuracy: 0.8590\n",
      "Epoch 28/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3535 - accuracy: 0.8448 - val_loss: 0.3305 - val_accuracy: 0.8554\n",
      "Epoch 29/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3537 - accuracy: 0.8453 - val_loss: 0.3295 - val_accuracy: 0.8554\n",
      "Epoch 30/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3562 - accuracy: 0.8423 - val_loss: 0.3292 - val_accuracy: 0.8553\n",
      "Epoch 31/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3510 - accuracy: 0.8441 - val_loss: 0.3309 - val_accuracy: 0.8555\n",
      "Epoch 32/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3501 - accuracy: 0.8454 - val_loss: 0.3308 - val_accuracy: 0.8559\n",
      "Epoch 33/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3516 - accuracy: 0.8439 - val_loss: 0.3311 - val_accuracy: 0.8557\n",
      "Epoch 34/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3494 - accuracy: 0.8468 - val_loss: 0.3350 - val_accuracy: 0.8547\n",
      "Epoch 35/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3512 - accuracy: 0.8456 - val_loss: 0.3377 - val_accuracy: 0.8513\n",
      "Epoch 36/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3483 - accuracy: 0.8466 - val_loss: 0.3290 - val_accuracy: 0.8571\n",
      "Epoch 37/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3494 - accuracy: 0.8464 - val_loss: 0.3273 - val_accuracy: 0.8583\n",
      "Epoch 38/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3508 - accuracy: 0.8455 - val_loss: 0.3351 - val_accuracy: 0.8546\n",
      "Epoch 39/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3502 - accuracy: 0.8474 - val_loss: 0.3257 - val_accuracy: 0.8577\n",
      "Epoch 40/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3470 - accuracy: 0.8475 - val_loss: 0.3313 - val_accuracy: 0.8559\n",
      "Epoch 41/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3500 - accuracy: 0.8460 - val_loss: 0.3354 - val_accuracy: 0.8536\n",
      "Epoch 42/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3485 - accuracy: 0.8456 - val_loss: 0.3273 - val_accuracy: 0.8560\n",
      "Epoch 43/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3497 - accuracy: 0.8447 - val_loss: 0.3376 - val_accuracy: 0.8513\n",
      "Epoch 44/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3476 - accuracy: 0.8467 - val_loss: 0.3257 - val_accuracy: 0.8577\n",
      "Epoch 45/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3436 - accuracy: 0.8487 - val_loss: 0.3275 - val_accuracy: 0.8558\n",
      "Epoch 46/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3456 - accuracy: 0.8475 - val_loss: 0.3326 - val_accuracy: 0.8540\n",
      "Epoch 47/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3478 - accuracy: 0.8473 - val_loss: 0.3355 - val_accuracy: 0.8522\n",
      "Epoch 48/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3433 - accuracy: 0.8506 - val_loss: 0.3259 - val_accuracy: 0.8601\n",
      "Epoch 49/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3425 - accuracy: 0.8500 - val_loss: 0.3262 - val_accuracy: 0.8575\n",
      "Epoch 50/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3444 - accuracy: 0.8473 - val_loss: 0.3262 - val_accuracy: 0.8585\n",
      "Epoch 51/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3411 - accuracy: 0.8512 - val_loss: 0.3256 - val_accuracy: 0.8580\n",
      "Epoch 52/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3402 - accuracy: 0.8499 - val_loss: 0.3274 - val_accuracy: 0.8580\n",
      "Epoch 53/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3409 - accuracy: 0.8507 - val_loss: 0.3273 - val_accuracy: 0.8555\n",
      "Epoch 54/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3415 - accuracy: 0.8514 - val_loss: 0.3305 - val_accuracy: 0.8549\n",
      "Epoch 55/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3421 - accuracy: 0.8483 - val_loss: 0.3416 - val_accuracy: 0.8480\n",
      "Epoch 56/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3407 - accuracy: 0.8526 - val_loss: 0.3280 - val_accuracy: 0.8561\n",
      "Epoch 57/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3390 - accuracy: 0.8508 - val_loss: 0.3261 - val_accuracy: 0.8586\n",
      "Epoch 58/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3442 - accuracy: 0.8475 - val_loss: 0.3284 - val_accuracy: 0.8565\n",
      "Epoch 59/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3407 - accuracy: 0.8505 - val_loss: 0.3269 - val_accuracy: 0.8579\n",
      "Epoch 60/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3390 - accuracy: 0.8524 - val_loss: 0.3306 - val_accuracy: 0.8588\n",
      "Epoch 61/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3403 - accuracy: 0.8491 - val_loss: 0.3281 - val_accuracy: 0.8565\n",
      "Epoch 62/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3404 - accuracy: 0.8503 - val_loss: 0.3248 - val_accuracy: 0.8579\n",
      "Epoch 63/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3344 - accuracy: 0.8523 - val_loss: 0.3273 - val_accuracy: 0.8580\n",
      "Epoch 64/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3375 - accuracy: 0.8522 - val_loss: 0.3279 - val_accuracy: 0.8545\n",
      "Epoch 65/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3379 - accuracy: 0.8504 - val_loss: 0.3238 - val_accuracy: 0.8597\n",
      "Epoch 66/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3393 - accuracy: 0.8529 - val_loss: 0.3289 - val_accuracy: 0.8580\n",
      "Epoch 67/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3377 - accuracy: 0.8524 - val_loss: 0.3256 - val_accuracy: 0.8590\n",
      "Epoch 68/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3372 - accuracy: 0.8519 - val_loss: 0.3290 - val_accuracy: 0.8543\n",
      "Epoch 69/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3363 - accuracy: 0.8534 - val_loss: 0.3362 - val_accuracy: 0.8496\n",
      "Epoch 70/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3338 - accuracy: 0.8556 - val_loss: 0.3284 - val_accuracy: 0.8559\n",
      "Epoch 71/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3352 - accuracy: 0.8533 - val_loss: 0.3249 - val_accuracy: 0.8583\n",
      "Epoch 72/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3318 - accuracy: 0.8531 - val_loss: 0.3306 - val_accuracy: 0.8559\n",
      "Epoch 73/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8525 - val_loss: 0.3302 - val_accuracy: 0.8546\n",
      "Epoch 74/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3337 - accuracy: 0.8527 - val_loss: 0.3256 - val_accuracy: 0.8575\n",
      "Epoch 75/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 0.8530 - val_loss: 0.3414 - val_accuracy: 0.8495\n",
      "Epoch 76/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3336 - accuracy: 0.8540 - val_loss: 0.3253 - val_accuracy: 0.8561\n",
      "Epoch 77/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3334 - accuracy: 0.8524 - val_loss: 0.3310 - val_accuracy: 0.8547\n",
      "Epoch 78/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8542 - val_loss: 0.3257 - val_accuracy: 0.8587\n",
      "Epoch 79/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3347 - accuracy: 0.8530 - val_loss: 0.3269 - val_accuracy: 0.8563\n",
      "Epoch 80/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3315 - accuracy: 0.8556 - val_loss: 0.3257 - val_accuracy: 0.8578\n",
      "Epoch 81/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3342 - accuracy: 0.8532 - val_loss: 0.3257 - val_accuracy: 0.8569\n",
      "Epoch 82/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3303 - accuracy: 0.8526 - val_loss: 0.3265 - val_accuracy: 0.8572\n",
      "Epoch 83/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3315 - accuracy: 0.8538 - val_loss: 0.3259 - val_accuracy: 0.8568\n",
      "Epoch 84/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3292 - accuracy: 0.8573 - val_loss: 0.3251 - val_accuracy: 0.8574\n",
      "Epoch 85/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3299 - accuracy: 0.8565 - val_loss: 0.3277 - val_accuracy: 0.8571\n",
      "Epoch 86/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3293 - accuracy: 0.8540 - val_loss: 0.3264 - val_accuracy: 0.8577\n",
      "Epoch 87/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3282 - accuracy: 0.8584 - val_loss: 0.3338 - val_accuracy: 0.8530\n",
      "Epoch 88/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3296 - accuracy: 0.8563 - val_loss: 0.3288 - val_accuracy: 0.8565\n",
      "Epoch 89/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3295 - accuracy: 0.8552 - val_loss: 0.3286 - val_accuracy: 0.8543\n",
      "Epoch 90/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3282 - accuracy: 0.8568 - val_loss: 0.3280 - val_accuracy: 0.8572\n",
      "Epoch 91/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3271 - accuracy: 0.8555 - val_loss: 0.3242 - val_accuracy: 0.8597\n",
      "Epoch 92/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3271 - accuracy: 0.8578 - val_loss: 0.3242 - val_accuracy: 0.8584\n",
      "Epoch 93/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3288 - accuracy: 0.8587 - val_loss: 0.3279 - val_accuracy: 0.8557\n",
      "Epoch 94/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3287 - accuracy: 0.8564 - val_loss: 0.3267 - val_accuracy: 0.8567\n",
      "Epoch 95/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3268 - accuracy: 0.8586 - val_loss: 0.3261 - val_accuracy: 0.8571\n",
      "Epoch 96/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3260 - accuracy: 0.8575 - val_loss: 0.3373 - val_accuracy: 0.8539\n",
      "Epoch 97/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3239 - accuracy: 0.8588 - val_loss: 0.3269 - val_accuracy: 0.8561\n",
      "Epoch 98/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3249 - accuracy: 0.8582 - val_loss: 0.3278 - val_accuracy: 0.8544\n",
      "Epoch 99/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3242 - accuracy: 0.8579 - val_loss: 0.3257 - val_accuracy: 0.8568\n",
      "Epoch 100/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3193 - accuracy: 0.8614 - val_loss: 0.3271 - val_accuracy: 0.8556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1da045342d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_add_dropout(\n",
    "    [60, 30, 10],\n",
    "    [0.1, 0, 0],\n",
    "    100,\n",
    "    x, y\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "86.14% training accuracy with 85.56% validation accuracy\n",
    "\n",
    "- accuracy increased\n",
    "- equivalent accuracy (training and validating)\n",
    "- try scaling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN with 3 layers of 60, 30, 10 neurons (with 0.1 Dropout Layer) (with Standard scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = pd.DataFrame(StandardScaler().fit_transform(x), columns=x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "268/268 [==============================] - 2s 3ms/step - loss: 0.4472 - accuracy: 0.7926 - val_loss: 0.3580 - val_accuracy: 0.8452\n",
      "Epoch 2/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3658 - accuracy: 0.8403 - val_loss: 0.3440 - val_accuracy: 0.8518\n",
      "Epoch 3/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3522 - accuracy: 0.8481 - val_loss: 0.3422 - val_accuracy: 0.8519\n",
      "Epoch 4/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3430 - accuracy: 0.8529 - val_loss: 0.3371 - val_accuracy: 0.8551\n",
      "Epoch 5/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3380 - accuracy: 0.8535 - val_loss: 0.3344 - val_accuracy: 0.8566\n",
      "Epoch 6/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3329 - accuracy: 0.8567 - val_loss: 0.3334 - val_accuracy: 0.8586\n",
      "Epoch 7/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3294 - accuracy: 0.8578 - val_loss: 0.3308 - val_accuracy: 0.8590\n",
      "Epoch 8/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3224 - accuracy: 0.8623 - val_loss: 0.3316 - val_accuracy: 0.8586\n",
      "Epoch 9/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3244 - accuracy: 0.8592 - val_loss: 0.3315 - val_accuracy: 0.8576\n",
      "Epoch 10/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3173 - accuracy: 0.8645 - val_loss: 0.3304 - val_accuracy: 0.8582\n",
      "Epoch 11/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3171 - accuracy: 0.8631 - val_loss: 0.3327 - val_accuracy: 0.8580\n",
      "Epoch 12/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3126 - accuracy: 0.8662 - val_loss: 0.3307 - val_accuracy: 0.8578\n",
      "Epoch 13/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3077 - accuracy: 0.8695 - val_loss: 0.3339 - val_accuracy: 0.8584\n",
      "Epoch 14/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3073 - accuracy: 0.8667 - val_loss: 0.3350 - val_accuracy: 0.8565\n",
      "Epoch 15/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3058 - accuracy: 0.8684 - val_loss: 0.3349 - val_accuracy: 0.8570\n",
      "Epoch 16/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.3039 - accuracy: 0.8692 - val_loss: 0.3329 - val_accuracy: 0.8575\n",
      "Epoch 17/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3016 - accuracy: 0.8720 - val_loss: 0.3388 - val_accuracy: 0.8549\n",
      "Epoch 18/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3018 - accuracy: 0.8717 - val_loss: 0.3379 - val_accuracy: 0.8559\n",
      "Epoch 19/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2978 - accuracy: 0.8744 - val_loss: 0.3391 - val_accuracy: 0.8548\n",
      "Epoch 20/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2953 - accuracy: 0.8739 - val_loss: 0.3393 - val_accuracy: 0.8565\n",
      "Epoch 21/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2944 - accuracy: 0.8734 - val_loss: 0.3407 - val_accuracy: 0.8532\n",
      "Epoch 22/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2933 - accuracy: 0.8727 - val_loss: 0.3411 - val_accuracy: 0.8543\n",
      "Epoch 23/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2896 - accuracy: 0.8762 - val_loss: 0.3410 - val_accuracy: 0.8534\n",
      "Epoch 24/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2880 - accuracy: 0.8791 - val_loss: 0.3429 - val_accuracy: 0.8536\n",
      "Epoch 25/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2863 - accuracy: 0.8763 - val_loss: 0.3424 - val_accuracy: 0.8532\n",
      "Epoch 26/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2835 - accuracy: 0.8802 - val_loss: 0.3456 - val_accuracy: 0.8536\n",
      "Epoch 27/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2815 - accuracy: 0.8796 - val_loss: 0.3440 - val_accuracy: 0.8533\n",
      "Epoch 28/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2794 - accuracy: 0.8815 - val_loss: 0.3460 - val_accuracy: 0.8547\n",
      "Epoch 29/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2783 - accuracy: 0.8815 - val_loss: 0.3486 - val_accuracy: 0.8523\n",
      "Epoch 30/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2783 - accuracy: 0.8814 - val_loss: 0.3510 - val_accuracy: 0.8515\n",
      "Epoch 31/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2786 - accuracy: 0.8809 - val_loss: 0.3473 - val_accuracy: 0.8563\n",
      "Epoch 32/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2766 - accuracy: 0.8816 - val_loss: 0.3528 - val_accuracy: 0.8524\n",
      "Epoch 33/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2730 - accuracy: 0.8830 - val_loss: 0.3549 - val_accuracy: 0.8496\n",
      "Epoch 34/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2731 - accuracy: 0.8823 - val_loss: 0.3554 - val_accuracy: 0.8516\n",
      "Epoch 35/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2724 - accuracy: 0.8832 - val_loss: 0.3530 - val_accuracy: 0.8526\n",
      "Epoch 36/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2691 - accuracy: 0.8844 - val_loss: 0.3570 - val_accuracy: 0.8508\n",
      "Epoch 37/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2681 - accuracy: 0.8852 - val_loss: 0.3605 - val_accuracy: 0.8509\n",
      "Epoch 38/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2684 - accuracy: 0.8867 - val_loss: 0.3565 - val_accuracy: 0.8518\n",
      "Epoch 39/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2693 - accuracy: 0.8838 - val_loss: 0.3593 - val_accuracy: 0.8490\n",
      "Epoch 40/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2678 - accuracy: 0.8851 - val_loss: 0.3635 - val_accuracy: 0.8495\n",
      "Epoch 41/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2675 - accuracy: 0.8854 - val_loss: 0.3585 - val_accuracy: 0.8492\n",
      "Epoch 42/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2654 - accuracy: 0.8869 - val_loss: 0.3616 - val_accuracy: 0.8508\n",
      "Epoch 43/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2629 - accuracy: 0.8863 - val_loss: 0.3604 - val_accuracy: 0.8502\n",
      "Epoch 44/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2661 - accuracy: 0.8871 - val_loss: 0.3606 - val_accuracy: 0.8516\n",
      "Epoch 45/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2601 - accuracy: 0.8910 - val_loss: 0.3640 - val_accuracy: 0.8487\n",
      "Epoch 46/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2601 - accuracy: 0.8872 - val_loss: 0.3679 - val_accuracy: 0.8483\n",
      "Epoch 47/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2609 - accuracy: 0.8898 - val_loss: 0.3645 - val_accuracy: 0.8486\n",
      "Epoch 48/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2576 - accuracy: 0.8908 - val_loss: 0.3654 - val_accuracy: 0.8496\n",
      "Epoch 49/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2598 - accuracy: 0.8882 - val_loss: 0.3748 - val_accuracy: 0.8478\n",
      "Epoch 50/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2570 - accuracy: 0.8919 - val_loss: 0.3722 - val_accuracy: 0.8477\n",
      "Epoch 51/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2570 - accuracy: 0.8907 - val_loss: 0.3689 - val_accuracy: 0.8467\n",
      "Epoch 52/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2552 - accuracy: 0.8926 - val_loss: 0.3746 - val_accuracy: 0.8476\n",
      "Epoch 53/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2563 - accuracy: 0.8921 - val_loss: 0.3688 - val_accuracy: 0.8480\n",
      "Epoch 54/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2563 - accuracy: 0.8899 - val_loss: 0.3722 - val_accuracy: 0.8461\n",
      "Epoch 55/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2543 - accuracy: 0.8920 - val_loss: 0.3752 - val_accuracy: 0.8455\n",
      "Epoch 56/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2573 - accuracy: 0.8892 - val_loss: 0.3717 - val_accuracy: 0.8446\n",
      "Epoch 57/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2556 - accuracy: 0.8910 - val_loss: 0.3750 - val_accuracy: 0.8474\n",
      "Epoch 58/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2541 - accuracy: 0.8908 - val_loss: 0.3714 - val_accuracy: 0.8486\n",
      "Epoch 59/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2564 - accuracy: 0.8905 - val_loss: 0.3762 - val_accuracy: 0.8469\n",
      "Epoch 60/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2541 - accuracy: 0.8932 - val_loss: 0.3734 - val_accuracy: 0.8472\n",
      "Epoch 61/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2524 - accuracy: 0.8934 - val_loss: 0.3768 - val_accuracy: 0.8500\n",
      "Epoch 62/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2537 - accuracy: 0.8919 - val_loss: 0.3733 - val_accuracy: 0.8497\n",
      "Epoch 63/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2504 - accuracy: 0.8937 - val_loss: 0.3780 - val_accuracy: 0.8459\n",
      "Epoch 64/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2488 - accuracy: 0.8928 - val_loss: 0.3742 - val_accuracy: 0.8487\n",
      "Epoch 65/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2479 - accuracy: 0.8949 - val_loss: 0.3795 - val_accuracy: 0.8454\n",
      "Epoch 66/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2499 - accuracy: 0.8922 - val_loss: 0.3799 - val_accuracy: 0.8485\n",
      "Epoch 67/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2475 - accuracy: 0.8940 - val_loss: 0.3760 - val_accuracy: 0.8473\n",
      "Epoch 68/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2470 - accuracy: 0.8973 - val_loss: 0.3817 - val_accuracy: 0.8443\n",
      "Epoch 69/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2488 - accuracy: 0.8950 - val_loss: 0.3844 - val_accuracy: 0.8475\n",
      "Epoch 70/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2429 - accuracy: 0.8970 - val_loss: 0.3810 - val_accuracy: 0.8475\n",
      "Epoch 71/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2461 - accuracy: 0.8950 - val_loss: 0.3779 - val_accuracy: 0.8503\n",
      "Epoch 72/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2456 - accuracy: 0.8965 - val_loss: 0.3817 - val_accuracy: 0.8483\n",
      "Epoch 73/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2450 - accuracy: 0.8959 - val_loss: 0.3852 - val_accuracy: 0.8477\n",
      "Epoch 74/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2478 - accuracy: 0.8959 - val_loss: 0.3836 - val_accuracy: 0.8459\n",
      "Epoch 75/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2432 - accuracy: 0.8993 - val_loss: 0.3812 - val_accuracy: 0.8480\n",
      "Epoch 76/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2419 - accuracy: 0.8968 - val_loss: 0.3893 - val_accuracy: 0.8487\n",
      "Epoch 77/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2428 - accuracy: 0.8973 - val_loss: 0.3820 - val_accuracy: 0.8465\n",
      "Epoch 78/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2412 - accuracy: 0.8962 - val_loss: 0.3892 - val_accuracy: 0.8459\n",
      "Epoch 79/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2412 - accuracy: 0.8966 - val_loss: 0.3977 - val_accuracy: 0.8465\n",
      "Epoch 80/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2428 - accuracy: 0.8965 - val_loss: 0.3920 - val_accuracy: 0.8416\n",
      "Epoch 81/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2393 - accuracy: 0.8998 - val_loss: 0.3915 - val_accuracy: 0.8440\n",
      "Epoch 82/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2448 - accuracy: 0.8948 - val_loss: 0.3937 - val_accuracy: 0.8485\n",
      "Epoch 83/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2447 - accuracy: 0.8973 - val_loss: 0.3872 - val_accuracy: 0.8455\n",
      "Epoch 84/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2372 - accuracy: 0.9009 - val_loss: 0.3856 - val_accuracy: 0.8484\n",
      "Epoch 85/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2417 - accuracy: 0.8973 - val_loss: 0.3869 - val_accuracy: 0.8459\n",
      "Epoch 86/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2386 - accuracy: 0.8999 - val_loss: 0.3920 - val_accuracy: 0.8467\n",
      "Epoch 87/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2423 - accuracy: 0.8976 - val_loss: 0.3906 - val_accuracy: 0.8484\n",
      "Epoch 88/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2375 - accuracy: 0.8978 - val_loss: 0.3942 - val_accuracy: 0.8469\n",
      "Epoch 89/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2379 - accuracy: 0.9000 - val_loss: 0.3919 - val_accuracy: 0.8461\n",
      "Epoch 90/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2382 - accuracy: 0.8997 - val_loss: 0.3897 - val_accuracy: 0.8479\n",
      "Epoch 91/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2385 - accuracy: 0.8979 - val_loss: 0.3907 - val_accuracy: 0.8459\n",
      "Epoch 92/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2368 - accuracy: 0.8989 - val_loss: 0.3935 - val_accuracy: 0.8446\n",
      "Epoch 93/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2384 - accuracy: 0.8988 - val_loss: 0.3943 - val_accuracy: 0.8467\n",
      "Epoch 94/100\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2379 - accuracy: 0.9010 - val_loss: 0.3923 - val_accuracy: 0.8442\n",
      "Epoch 95/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2349 - accuracy: 0.9017 - val_loss: 0.3953 - val_accuracy: 0.8458\n",
      "Epoch 96/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2398 - accuracy: 0.8966 - val_loss: 0.3954 - val_accuracy: 0.8443\n",
      "Epoch 97/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2351 - accuracy: 0.9022 - val_loss: 0.4020 - val_accuracy: 0.8439\n",
      "Epoch 98/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2334 - accuracy: 0.9005 - val_loss: 0.3981 - val_accuracy: 0.8446\n",
      "Epoch 99/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2322 - accuracy: 0.9015 - val_loss: 0.4021 - val_accuracy: 0.8419\n",
      "Epoch 100/100\n",
      "268/268 [==============================] - 1s 2ms/step - loss: 0.2330 - accuracy: 0.9011 - val_loss: 0.3928 - val_accuracy: 0.8461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1da04318510>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_add_dropout(\n",
    "    [60, 30, 10],\n",
    "    [0.1, 0, 0],\n",
    "    100,\n",
    "    x_scaled,\n",
    "    y\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90.11% training accuracy with 84.61% validation accuracy\n",
    "\n",
    "- accuracy increased\n",
    "- slightly overfitting\n",
    "- try adding more dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN with 3 layers of 60, 30, 10 neurons (with 0.2 Dropout Layer) (with Standard scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "268/268 [==============================] - 2s 3ms/step - loss: 0.4390 - accuracy: 0.7958 - val_loss: 0.3596 - val_accuracy: 0.8443\n",
      "Epoch 2/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3702 - accuracy: 0.8379 - val_loss: 0.3467 - val_accuracy: 0.8519\n",
      "Epoch 3/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3553 - accuracy: 0.8453 - val_loss: 0.3387 - val_accuracy: 0.8562\n",
      "Epoch 4/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3489 - accuracy: 0.8475 - val_loss: 0.3368 - val_accuracy: 0.8561\n",
      "Epoch 5/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3452 - accuracy: 0.8497 - val_loss: 0.3402 - val_accuracy: 0.8552\n",
      "Epoch 6/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3432 - accuracy: 0.8520 - val_loss: 0.3352 - val_accuracy: 0.8590\n",
      "Epoch 7/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3401 - accuracy: 0.8538 - val_loss: 0.3337 - val_accuracy: 0.8580\n",
      "Epoch 8/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3337 - accuracy: 0.8564 - val_loss: 0.3327 - val_accuracy: 0.8586\n",
      "Epoch 9/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3343 - accuracy: 0.8539 - val_loss: 0.3317 - val_accuracy: 0.8595\n",
      "Epoch 10/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3337 - accuracy: 0.8564 - val_loss: 0.3323 - val_accuracy: 0.8588\n",
      "Epoch 11/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3304 - accuracy: 0.8572 - val_loss: 0.3306 - val_accuracy: 0.8583\n",
      "Epoch 12/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3253 - accuracy: 0.8589 - val_loss: 0.3298 - val_accuracy: 0.8605\n",
      "Epoch 13/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3263 - accuracy: 0.8609 - val_loss: 0.3322 - val_accuracy: 0.8581\n",
      "Epoch 14/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3264 - accuracy: 0.8594 - val_loss: 0.3284 - val_accuracy: 0.8602\n",
      "Epoch 15/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3199 - accuracy: 0.8619 - val_loss: 0.3313 - val_accuracy: 0.8592\n",
      "Epoch 16/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3202 - accuracy: 0.8616 - val_loss: 0.3280 - val_accuracy: 0.8610\n",
      "Epoch 17/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3196 - accuracy: 0.8601 - val_loss: 0.3307 - val_accuracy: 0.8603\n",
      "Epoch 18/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3143 - accuracy: 0.8629 - val_loss: 0.3295 - val_accuracy: 0.8617\n",
      "Epoch 19/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3154 - accuracy: 0.8654 - val_loss: 0.3288 - val_accuracy: 0.8614\n",
      "Epoch 20/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3158 - accuracy: 0.8643 - val_loss: 0.3302 - val_accuracy: 0.8616\n",
      "Epoch 21/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3114 - accuracy: 0.8669 - val_loss: 0.3325 - val_accuracy: 0.8594\n",
      "Epoch 22/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3114 - accuracy: 0.8663 - val_loss: 0.3297 - val_accuracy: 0.8601\n",
      "Epoch 23/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3127 - accuracy: 0.8646 - val_loss: 0.3295 - val_accuracy: 0.8599\n",
      "Epoch 24/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3130 - accuracy: 0.8663 - val_loss: 0.3314 - val_accuracy: 0.8599\n",
      "Epoch 25/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3124 - accuracy: 0.8658 - val_loss: 0.3287 - val_accuracy: 0.8594\n",
      "Epoch 26/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3091 - accuracy: 0.8675 - val_loss: 0.3297 - val_accuracy: 0.8593\n",
      "Epoch 27/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3091 - accuracy: 0.8660 - val_loss: 0.3299 - val_accuracy: 0.8602\n",
      "Epoch 28/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3050 - accuracy: 0.8673 - val_loss: 0.3301 - val_accuracy: 0.8594\n",
      "Epoch 29/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3057 - accuracy: 0.8693 - val_loss: 0.3296 - val_accuracy: 0.8593\n",
      "Epoch 30/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3040 - accuracy: 0.8685 - val_loss: 0.3297 - val_accuracy: 0.8586\n",
      "Epoch 31/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3036 - accuracy: 0.8694 - val_loss: 0.3329 - val_accuracy: 0.8578\n",
      "Epoch 32/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3008 - accuracy: 0.8711 - val_loss: 0.3346 - val_accuracy: 0.8585\n",
      "Epoch 33/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3004 - accuracy: 0.8723 - val_loss: 0.3312 - val_accuracy: 0.8584\n",
      "Epoch 34/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3017 - accuracy: 0.8715 - val_loss: 0.3317 - val_accuracy: 0.8577\n",
      "Epoch 35/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.3010 - accuracy: 0.8704 - val_loss: 0.3356 - val_accuracy: 0.8551\n",
      "Epoch 36/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2980 - accuracy: 0.8720 - val_loss: 0.3352 - val_accuracy: 0.8565\n",
      "Epoch 37/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2957 - accuracy: 0.8731 - val_loss: 0.3357 - val_accuracy: 0.8589\n",
      "Epoch 38/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2949 - accuracy: 0.8722 - val_loss: 0.3343 - val_accuracy: 0.8578\n",
      "Epoch 39/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2950 - accuracy: 0.8736 - val_loss: 0.3335 - val_accuracy: 0.8604\n",
      "Epoch 40/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2953 - accuracy: 0.8735 - val_loss: 0.3356 - val_accuracy: 0.8585\n",
      "Epoch 41/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2917 - accuracy: 0.8760 - val_loss: 0.3320 - val_accuracy: 0.8562\n",
      "Epoch 42/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2929 - accuracy: 0.8751 - val_loss: 0.3360 - val_accuracy: 0.8581\n",
      "Epoch 43/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2904 - accuracy: 0.8745 - val_loss: 0.3356 - val_accuracy: 0.8598\n",
      "Epoch 44/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2905 - accuracy: 0.8774 - val_loss: 0.3350 - val_accuracy: 0.8599\n",
      "Epoch 45/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2909 - accuracy: 0.8746 - val_loss: 0.3381 - val_accuracy: 0.8554\n",
      "Epoch 46/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2908 - accuracy: 0.8756 - val_loss: 0.3344 - val_accuracy: 0.8565\n",
      "Epoch 47/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2885 - accuracy: 0.8776 - val_loss: 0.3376 - val_accuracy: 0.8584\n",
      "Epoch 48/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2876 - accuracy: 0.8769 - val_loss: 0.3369 - val_accuracy: 0.8589\n",
      "Epoch 49/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2880 - accuracy: 0.8779 - val_loss: 0.3360 - val_accuracy: 0.8584\n",
      "Epoch 50/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2894 - accuracy: 0.8762 - val_loss: 0.3365 - val_accuracy: 0.8553\n",
      "Epoch 51/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2882 - accuracy: 0.8760 - val_loss: 0.3369 - val_accuracy: 0.8565\n",
      "Epoch 52/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2889 - accuracy: 0.8750 - val_loss: 0.3353 - val_accuracy: 0.8565\n",
      "Epoch 53/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2865 - accuracy: 0.8772 - val_loss: 0.3369 - val_accuracy: 0.8561\n",
      "Epoch 54/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2873 - accuracy: 0.8756 - val_loss: 0.3406 - val_accuracy: 0.8573\n",
      "Epoch 55/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2863 - accuracy: 0.8763 - val_loss: 0.3411 - val_accuracy: 0.8544\n",
      "Epoch 56/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2853 - accuracy: 0.8780 - val_loss: 0.3403 - val_accuracy: 0.8561\n",
      "Epoch 57/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2846 - accuracy: 0.8769 - val_loss: 0.3371 - val_accuracy: 0.8544\n",
      "Epoch 58/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2851 - accuracy: 0.8771 - val_loss: 0.3376 - val_accuracy: 0.8555\n",
      "Epoch 59/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2842 - accuracy: 0.8772 - val_loss: 0.3372 - val_accuracy: 0.8579\n",
      "Epoch 60/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2801 - accuracy: 0.8818 - val_loss: 0.3383 - val_accuracy: 0.8568\n",
      "Epoch 61/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2792 - accuracy: 0.8804 - val_loss: 0.3406 - val_accuracy: 0.8557\n",
      "Epoch 62/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2801 - accuracy: 0.8820 - val_loss: 0.3449 - val_accuracy: 0.8563\n",
      "Epoch 63/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2790 - accuracy: 0.8808 - val_loss: 0.3440 - val_accuracy: 0.8578\n",
      "Epoch 64/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2776 - accuracy: 0.8828 - val_loss: 0.3424 - val_accuracy: 0.8552\n",
      "Epoch 65/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2798 - accuracy: 0.8817 - val_loss: 0.3434 - val_accuracy: 0.8571\n",
      "Epoch 66/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2781 - accuracy: 0.8814 - val_loss: 0.3437 - val_accuracy: 0.8558\n",
      "Epoch 67/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2748 - accuracy: 0.8833 - val_loss: 0.3437 - val_accuracy: 0.8539\n",
      "Epoch 68/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2775 - accuracy: 0.8834 - val_loss: 0.3508 - val_accuracy: 0.8555\n",
      "Epoch 69/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2807 - accuracy: 0.8805 - val_loss: 0.3485 - val_accuracy: 0.8546\n",
      "Epoch 70/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2770 - accuracy: 0.8818 - val_loss: 0.3449 - val_accuracy: 0.8552\n",
      "Epoch 71/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2738 - accuracy: 0.8817 - val_loss: 0.3454 - val_accuracy: 0.8571\n",
      "Epoch 72/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2751 - accuracy: 0.8836 - val_loss: 0.3442 - val_accuracy: 0.8547\n",
      "Epoch 73/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2746 - accuracy: 0.8828 - val_loss: 0.3494 - val_accuracy: 0.8560\n",
      "Epoch 74/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2735 - accuracy: 0.8823 - val_loss: 0.3442 - val_accuracy: 0.8580\n",
      "Epoch 75/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2736 - accuracy: 0.8835 - val_loss: 0.3421 - val_accuracy: 0.8593\n",
      "Epoch 76/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2775 - accuracy: 0.8813 - val_loss: 0.3451 - val_accuracy: 0.8573\n",
      "Epoch 77/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2732 - accuracy: 0.8840 - val_loss: 0.3441 - val_accuracy: 0.8564\n",
      "Epoch 78/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2757 - accuracy: 0.8821 - val_loss: 0.3471 - val_accuracy: 0.8557\n",
      "Epoch 79/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2757 - accuracy: 0.8818 - val_loss: 0.3469 - val_accuracy: 0.8565\n",
      "Epoch 80/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2742 - accuracy: 0.8836 - val_loss: 0.3493 - val_accuracy: 0.8561\n",
      "Epoch 81/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2740 - accuracy: 0.8840 - val_loss: 0.3456 - val_accuracy: 0.8540\n",
      "Epoch 82/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2751 - accuracy: 0.8832 - val_loss: 0.3469 - val_accuracy: 0.8528\n",
      "Epoch 83/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2732 - accuracy: 0.8843 - val_loss: 0.3501 - val_accuracy: 0.8556\n",
      "Epoch 84/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2732 - accuracy: 0.8831 - val_loss: 0.3487 - val_accuracy: 0.8550\n",
      "Epoch 85/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2709 - accuracy: 0.8842 - val_loss: 0.3529 - val_accuracy: 0.8549\n",
      "Epoch 86/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2719 - accuracy: 0.8836 - val_loss: 0.3443 - val_accuracy: 0.8571\n",
      "Epoch 87/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2716 - accuracy: 0.8835 - val_loss: 0.3462 - val_accuracy: 0.8562\n",
      "Epoch 88/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2696 - accuracy: 0.8835 - val_loss: 0.3478 - val_accuracy: 0.8538\n",
      "Epoch 89/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2696 - accuracy: 0.8860 - val_loss: 0.3498 - val_accuracy: 0.8558\n",
      "Epoch 90/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2717 - accuracy: 0.8840 - val_loss: 0.3485 - val_accuracy: 0.8558\n",
      "Epoch 91/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2700 - accuracy: 0.8833 - val_loss: 0.3508 - val_accuracy: 0.8560\n",
      "Epoch 92/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2679 - accuracy: 0.8863 - val_loss: 0.3514 - val_accuracy: 0.8541\n",
      "Epoch 93/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2688 - accuracy: 0.8855 - val_loss: 0.3477 - val_accuracy: 0.8546\n",
      "Epoch 94/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2674 - accuracy: 0.8854 - val_loss: 0.3509 - val_accuracy: 0.8530\n",
      "Epoch 95/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2703 - accuracy: 0.8837 - val_loss: 0.3506 - val_accuracy: 0.8549\n",
      "Epoch 96/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2714 - accuracy: 0.8833 - val_loss: 0.3492 - val_accuracy: 0.8546\n",
      "Epoch 97/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2707 - accuracy: 0.8847 - val_loss: 0.3537 - val_accuracy: 0.8515\n",
      "Epoch 98/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2695 - accuracy: 0.8828 - val_loss: 0.3514 - val_accuracy: 0.8557\n",
      "Epoch 99/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2641 - accuracy: 0.8890 - val_loss: 0.3531 - val_accuracy: 0.8526\n",
      "Epoch 100/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2652 - accuracy: 0.8876 - val_loss: 0.3517 - val_accuracy: 0.8533\n",
      "Epoch 101/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2659 - accuracy: 0.8877 - val_loss: 0.3521 - val_accuracy: 0.8544\n",
      "Epoch 102/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2659 - accuracy: 0.8874 - val_loss: 0.3530 - val_accuracy: 0.8544\n",
      "Epoch 103/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2664 - accuracy: 0.8870 - val_loss: 0.3516 - val_accuracy: 0.8518\n",
      "Epoch 104/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2653 - accuracy: 0.8869 - val_loss: 0.3548 - val_accuracy: 0.8515\n",
      "Epoch 105/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2663 - accuracy: 0.8865 - val_loss: 0.3513 - val_accuracy: 0.8530\n",
      "Epoch 106/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2656 - accuracy: 0.8859 - val_loss: 0.3563 - val_accuracy: 0.8521\n",
      "Epoch 107/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2638 - accuracy: 0.8878 - val_loss: 0.3517 - val_accuracy: 0.8522\n",
      "Epoch 108/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2654 - accuracy: 0.8866 - val_loss: 0.3536 - val_accuracy: 0.8528\n",
      "Epoch 109/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2645 - accuracy: 0.8858 - val_loss: 0.3536 - val_accuracy: 0.8543\n",
      "Epoch 110/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2666 - accuracy: 0.8875 - val_loss: 0.3550 - val_accuracy: 0.8527\n",
      "Epoch 111/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2661 - accuracy: 0.8849 - val_loss: 0.3571 - val_accuracy: 0.8518\n",
      "Epoch 112/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2636 - accuracy: 0.8884 - val_loss: 0.3555 - val_accuracy: 0.8521\n",
      "Epoch 113/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2654 - accuracy: 0.8874 - val_loss: 0.3564 - val_accuracy: 0.8533\n",
      "Epoch 114/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2651 - accuracy: 0.8865 - val_loss: 0.3515 - val_accuracy: 0.8534\n",
      "Epoch 115/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2616 - accuracy: 0.8884 - val_loss: 0.3572 - val_accuracy: 0.8518\n",
      "Epoch 116/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2637 - accuracy: 0.8882 - val_loss: 0.3545 - val_accuracy: 0.8526\n",
      "Epoch 117/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2639 - accuracy: 0.8884 - val_loss: 0.3550 - val_accuracy: 0.8525\n",
      "Epoch 118/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2619 - accuracy: 0.8860 - val_loss: 0.3556 - val_accuracy: 0.8508\n",
      "Epoch 119/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2674 - accuracy: 0.8868 - val_loss: 0.3540 - val_accuracy: 0.8521\n",
      "Epoch 120/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2643 - accuracy: 0.8876 - val_loss: 0.3535 - val_accuracy: 0.8526\n",
      "Epoch 121/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2639 - accuracy: 0.8876 - val_loss: 0.3597 - val_accuracy: 0.8540\n",
      "Epoch 122/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2596 - accuracy: 0.8900 - val_loss: 0.3578 - val_accuracy: 0.8513\n",
      "Epoch 123/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2629 - accuracy: 0.8876 - val_loss: 0.3564 - val_accuracy: 0.8513\n",
      "Epoch 124/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2602 - accuracy: 0.8890 - val_loss: 0.3544 - val_accuracy: 0.8536\n",
      "Epoch 125/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2622 - accuracy: 0.8877 - val_loss: 0.3619 - val_accuracy: 0.8542\n",
      "Epoch 126/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2648 - accuracy: 0.8870 - val_loss: 0.3553 - val_accuracy: 0.8516\n",
      "Epoch 127/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2622 - accuracy: 0.8881 - val_loss: 0.3625 - val_accuracy: 0.8523\n",
      "Epoch 128/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2628 - accuracy: 0.8889 - val_loss: 0.3588 - val_accuracy: 0.8549\n",
      "Epoch 129/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2610 - accuracy: 0.8885 - val_loss: 0.3572 - val_accuracy: 0.8527\n",
      "Epoch 130/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2610 - accuracy: 0.8887 - val_loss: 0.3536 - val_accuracy: 0.8533\n",
      "Epoch 131/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2608 - accuracy: 0.8883 - val_loss: 0.3567 - val_accuracy: 0.8529\n",
      "Epoch 132/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2619 - accuracy: 0.8897 - val_loss: 0.3600 - val_accuracy: 0.8531\n",
      "Epoch 133/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2611 - accuracy: 0.8885 - val_loss: 0.3628 - val_accuracy: 0.8533\n",
      "Epoch 134/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2625 - accuracy: 0.8867 - val_loss: 0.3594 - val_accuracy: 0.8520\n",
      "Epoch 135/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2612 - accuracy: 0.8875 - val_loss: 0.3572 - val_accuracy: 0.8540\n",
      "Epoch 136/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2609 - accuracy: 0.8884 - val_loss: 0.3647 - val_accuracy: 0.8500\n",
      "Epoch 137/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2627 - accuracy: 0.8885 - val_loss: 0.3576 - val_accuracy: 0.8514\n",
      "Epoch 138/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2616 - accuracy: 0.8876 - val_loss: 0.3592 - val_accuracy: 0.8502\n",
      "Epoch 139/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2566 - accuracy: 0.8896 - val_loss: 0.3613 - val_accuracy: 0.8501\n",
      "Epoch 140/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2596 - accuracy: 0.8897 - val_loss: 0.3598 - val_accuracy: 0.8485\n",
      "Epoch 141/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2578 - accuracy: 0.8884 - val_loss: 0.3627 - val_accuracy: 0.8529\n",
      "Epoch 142/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2614 - accuracy: 0.8890 - val_loss: 0.3612 - val_accuracy: 0.8508\n",
      "Epoch 143/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2599 - accuracy: 0.8881 - val_loss: 0.3603 - val_accuracy: 0.8509\n",
      "Epoch 144/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2610 - accuracy: 0.8912 - val_loss: 0.3658 - val_accuracy: 0.8504\n",
      "Epoch 145/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2579 - accuracy: 0.8906 - val_loss: 0.3620 - val_accuracy: 0.8510\n",
      "Epoch 146/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2565 - accuracy: 0.8888 - val_loss: 0.3612 - val_accuracy: 0.8529\n",
      "Epoch 147/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2586 - accuracy: 0.8896 - val_loss: 0.3620 - val_accuracy: 0.8524\n",
      "Epoch 148/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2610 - accuracy: 0.8911 - val_loss: 0.3582 - val_accuracy: 0.8517\n",
      "Epoch 149/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2579 - accuracy: 0.8917 - val_loss: 0.3606 - val_accuracy: 0.8512\n",
      "Epoch 150/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2585 - accuracy: 0.8880 - val_loss: 0.3635 - val_accuracy: 0.8534\n",
      "Epoch 151/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2609 - accuracy: 0.8886 - val_loss: 0.3624 - val_accuracy: 0.8511\n",
      "Epoch 152/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2593 - accuracy: 0.8885 - val_loss: 0.3620 - val_accuracy: 0.8518\n",
      "Epoch 153/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2538 - accuracy: 0.8912 - val_loss: 0.3638 - val_accuracy: 0.8495\n",
      "Epoch 154/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2567 - accuracy: 0.8917 - val_loss: 0.3621 - val_accuracy: 0.8509\n",
      "Epoch 155/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2580 - accuracy: 0.8885 - val_loss: 0.3637 - val_accuracy: 0.8537\n",
      "Epoch 156/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2612 - accuracy: 0.8884 - val_loss: 0.3584 - val_accuracy: 0.8502\n",
      "Epoch 157/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2562 - accuracy: 0.8916 - val_loss: 0.3578 - val_accuracy: 0.8508\n",
      "Epoch 158/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2574 - accuracy: 0.8908 - val_loss: 0.3619 - val_accuracy: 0.8497\n",
      "Epoch 159/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2602 - accuracy: 0.8887 - val_loss: 0.3613 - val_accuracy: 0.8508\n",
      "Epoch 160/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2584 - accuracy: 0.8891 - val_loss: 0.3657 - val_accuracy: 0.8504\n",
      "Epoch 161/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2543 - accuracy: 0.8917 - val_loss: 0.3584 - val_accuracy: 0.8515\n",
      "Epoch 162/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2597 - accuracy: 0.8895 - val_loss: 0.3649 - val_accuracy: 0.8524\n",
      "Epoch 163/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2602 - accuracy: 0.8904 - val_loss: 0.3556 - val_accuracy: 0.8528\n",
      "Epoch 164/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2596 - accuracy: 0.8890 - val_loss: 0.3624 - val_accuracy: 0.8498\n",
      "Epoch 165/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2545 - accuracy: 0.8930 - val_loss: 0.3665 - val_accuracy: 0.8495\n",
      "Epoch 166/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2523 - accuracy: 0.8936 - val_loss: 0.3638 - val_accuracy: 0.8469\n",
      "Epoch 167/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2545 - accuracy: 0.8916 - val_loss: 0.3627 - val_accuracy: 0.8471\n",
      "Epoch 168/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2542 - accuracy: 0.8897 - val_loss: 0.3672 - val_accuracy: 0.8505\n",
      "Epoch 169/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2554 - accuracy: 0.8903 - val_loss: 0.3639 - val_accuracy: 0.8493\n",
      "Epoch 170/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2589 - accuracy: 0.8912 - val_loss: 0.3681 - val_accuracy: 0.8485\n",
      "Epoch 171/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2569 - accuracy: 0.8894 - val_loss: 0.3646 - val_accuracy: 0.8485\n",
      "Epoch 172/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2530 - accuracy: 0.8930 - val_loss: 0.3615 - val_accuracy: 0.8484\n",
      "Epoch 173/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2567 - accuracy: 0.8917 - val_loss: 0.3645 - val_accuracy: 0.8515\n",
      "Epoch 174/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2519 - accuracy: 0.8938 - val_loss: 0.3665 - val_accuracy: 0.8483\n",
      "Epoch 175/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2548 - accuracy: 0.8911 - val_loss: 0.3664 - val_accuracy: 0.8479\n",
      "Epoch 176/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2535 - accuracy: 0.8925 - val_loss: 0.3629 - val_accuracy: 0.8501\n",
      "Epoch 177/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2557 - accuracy: 0.8905 - val_loss: 0.3700 - val_accuracy: 0.8469\n",
      "Epoch 178/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2529 - accuracy: 0.8925 - val_loss: 0.3645 - val_accuracy: 0.8507\n",
      "Epoch 179/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2532 - accuracy: 0.8915 - val_loss: 0.3620 - val_accuracy: 0.8509\n",
      "Epoch 180/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2617 - accuracy: 0.8889 - val_loss: 0.3635 - val_accuracy: 0.8499\n",
      "Epoch 181/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2544 - accuracy: 0.8912 - val_loss: 0.3665 - val_accuracy: 0.8480\n",
      "Epoch 182/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2528 - accuracy: 0.8929 - val_loss: 0.3743 - val_accuracy: 0.8511\n",
      "Epoch 183/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2538 - accuracy: 0.8924 - val_loss: 0.3646 - val_accuracy: 0.8486\n",
      "Epoch 184/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2529 - accuracy: 0.8920 - val_loss: 0.3671 - val_accuracy: 0.8477\n",
      "Epoch 185/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2558 - accuracy: 0.8890 - val_loss: 0.3655 - val_accuracy: 0.8505\n",
      "Epoch 186/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2528 - accuracy: 0.8919 - val_loss: 0.3675 - val_accuracy: 0.8503\n",
      "Epoch 187/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2545 - accuracy: 0.8898 - val_loss: 0.3675 - val_accuracy: 0.8515\n",
      "Epoch 188/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2516 - accuracy: 0.8917 - val_loss: 0.3699 - val_accuracy: 0.8511\n",
      "Epoch 189/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2530 - accuracy: 0.8935 - val_loss: 0.3657 - val_accuracy: 0.8515\n",
      "Epoch 190/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2512 - accuracy: 0.8934 - val_loss: 0.3673 - val_accuracy: 0.8488\n",
      "Epoch 191/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2550 - accuracy: 0.8896 - val_loss: 0.3697 - val_accuracy: 0.8460\n",
      "Epoch 192/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2504 - accuracy: 0.8929 - val_loss: 0.3716 - val_accuracy: 0.8477\n",
      "Epoch 193/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2542 - accuracy: 0.8914 - val_loss: 0.3744 - val_accuracy: 0.8512\n",
      "Epoch 194/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2557 - accuracy: 0.8906 - val_loss: 0.3707 - val_accuracy: 0.8495\n",
      "Epoch 195/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2558 - accuracy: 0.8904 - val_loss: 0.3687 - val_accuracy: 0.8513\n",
      "Epoch 196/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2528 - accuracy: 0.8908 - val_loss: 0.3721 - val_accuracy: 0.8492\n",
      "Epoch 197/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2513 - accuracy: 0.8923 - val_loss: 0.3739 - val_accuracy: 0.8483\n",
      "Epoch 198/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2532 - accuracy: 0.8930 - val_loss: 0.3675 - val_accuracy: 0.8501\n",
      "Epoch 199/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2514 - accuracy: 0.8940 - val_loss: 0.3704 - val_accuracy: 0.8508\n",
      "Epoch 200/200\n",
      "268/268 [==============================] - 1s 3ms/step - loss: 0.2532 - accuracy: 0.8922 - val_loss: 0.3713 - val_accuracy: 0.8506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1da1ae686d0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_add_dropout(\n",
    "    [60, 40, 20],\n",
    "    [0.1, 0.1, 0],\n",
    "    200,\n",
    "    x_scaled,\n",
    "    y\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
