{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='sentiment'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.sentiment.value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Phil the Alien is one of those quirky films where the humour is based around the oddness of everything rather than actual punchlines.<br /><br />At first it was very odd and pretty funny but as the movie progressed I didn\\'t find the jokes or oddness funny anymore.<br /><br />Its a low budget film (thats never a problem in itself), there were some pretty interesting characters, but eventually I just lost interest.<br /><br />I imagine this film would appeal to a stoner who is currently partaking.<br /><br />For something similar but better try \"Brother from another planet\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review[10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presence of `<br>`, `comma(,)`, `capital alphabets`, `Single(') and Double(\") Apostrophes` observed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df.review.replace(\"<br />\", \"\", regex=True).replace(\"[^a-zA-Z0-9 ]\", \"\", regex=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling sentiment in binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df.sentiment.map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production The filming tech...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically theres a family where a little boy J...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Matteis Love in the Time of Money is a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production The filming tech...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically theres a family where a little boy J...          0\n",
       "4  Petter Matteis Love in the Time of Money is a ...          1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem(review):\n",
    "    try:\n",
    "        return [\n",
    "            stemmer.stem(word)\n",
    "            for word in word_tokenize(review)\n",
    "            if word not in stopwords_english\n",
    "        ]\n",
    "    except:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stem'] = df['review'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[one, review, mention, watch, 1, oz, episod, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production The filming tech...</td>\n",
       "      <td>1</td>\n",
       "      <td>[a, wonder, littl, product, the, film, techniq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, thought, wonder, way, spend, time, hot, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically theres a family where a little boy J...</td>\n",
       "      <td>0</td>\n",
       "      <td>[basic, there, famili, littl, boy, jake, think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Matteis Love in the Time of Money is a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[petter, mattei, love, time, money, visual, st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment   \n",
       "0  One of the other reviewers has mentioned that ...          1  \\\n",
       "1  A wonderful little production The filming tech...          1   \n",
       "2  I thought this was a wonderful way to spend ti...          1   \n",
       "3  Basically theres a family where a little boy J...          0   \n",
       "4  Petter Matteis Love in the Time of Money is a ...          1   \n",
       "\n",
       "                                                stem  \n",
       "0  [one, review, mention, watch, 1, oz, episod, y...  \n",
       "1  [a, wonder, littl, product, the, film, techniq...  \n",
       "2  [i, thought, wonder, way, spend, time, hot, su...  \n",
       "3  [basic, there, famili, littl, boy, jake, think...  \n",
       "4  [petter, mattei, love, time, money, visual, st...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Word2Vec(\n",
    "    df['stem'],\n",
    "    vector_size = 100,\n",
    "    min_count = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181925"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('way', 0.5391795039176941),\n",
       " ('twice', 0.5027369856834412),\n",
       " ('occas', 0.4968571066856384),\n",
       " ('movi', 0.49312925338745117),\n",
       " ('day', 0.4903274178504944),\n",
       " ('hour', 0.4782518446445465),\n",
       " ('one', 0.4760948121547699),\n",
       " ('opportun', 0.47326377034187317),\n",
       " ('glanc', 0.4718545079231262),\n",
       " ('katanafrom', 0.47183889150619507)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.wv.most_similar('time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_average(keys):\n",
    "    vector_sum = np.array([0.0] * 100)\n",
    "    \n",
    "    for key in keys:\n",
    "        vector_sum += np.array(vectorizer.wv.get_vector(key).tolist())\n",
    "    \n",
    "    return vector_sum / len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame()\n",
    "x[[str(i) for i in range(100)]] = df.apply(lambda row: sentence_average(row[2]), axis = 1, result_type = \"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.224086</td>\n",
       "      <td>0.124548</td>\n",
       "      <td>-0.560523</td>\n",
       "      <td>-0.280118</td>\n",
       "      <td>-0.347190</td>\n",
       "      <td>-0.669528</td>\n",
       "      <td>0.387310</td>\n",
       "      <td>0.308927</td>\n",
       "      <td>0.298159</td>\n",
       "      <td>-0.228007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595217</td>\n",
       "      <td>0.635776</td>\n",
       "      <td>0.343884</td>\n",
       "      <td>-0.018723</td>\n",
       "      <td>0.878769</td>\n",
       "      <td>-0.188055</td>\n",
       "      <td>0.028916</td>\n",
       "      <td>-0.691826</td>\n",
       "      <td>0.481183</td>\n",
       "      <td>-0.296126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.564464</td>\n",
       "      <td>0.356956</td>\n",
       "      <td>-0.855516</td>\n",
       "      <td>-0.186919</td>\n",
       "      <td>0.333399</td>\n",
       "      <td>-0.449246</td>\n",
       "      <td>0.284913</td>\n",
       "      <td>0.592180</td>\n",
       "      <td>0.314848</td>\n",
       "      <td>-0.311810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264220</td>\n",
       "      <td>0.541035</td>\n",
       "      <td>0.047024</td>\n",
       "      <td>-0.307434</td>\n",
       "      <td>1.486784</td>\n",
       "      <td>-0.291462</td>\n",
       "      <td>0.323980</td>\n",
       "      <td>-0.127526</td>\n",
       "      <td>0.177815</td>\n",
       "      <td>0.088189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.404502</td>\n",
       "      <td>-0.202063</td>\n",
       "      <td>-0.432939</td>\n",
       "      <td>-0.261218</td>\n",
       "      <td>-0.053913</td>\n",
       "      <td>-0.664073</td>\n",
       "      <td>0.360001</td>\n",
       "      <td>0.429876</td>\n",
       "      <td>-0.131980</td>\n",
       "      <td>-0.061973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522549</td>\n",
       "      <td>0.676358</td>\n",
       "      <td>0.046866</td>\n",
       "      <td>0.025495</td>\n",
       "      <td>1.049829</td>\n",
       "      <td>-0.170199</td>\n",
       "      <td>0.142993</td>\n",
       "      <td>-0.494416</td>\n",
       "      <td>0.505590</td>\n",
       "      <td>-0.258266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.386408</td>\n",
       "      <td>-0.021483</td>\n",
       "      <td>-0.766305</td>\n",
       "      <td>-0.319618</td>\n",
       "      <td>-0.247374</td>\n",
       "      <td>-0.613956</td>\n",
       "      <td>0.230997</td>\n",
       "      <td>0.207710</td>\n",
       "      <td>0.288456</td>\n",
       "      <td>-0.490740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584336</td>\n",
       "      <td>0.823562</td>\n",
       "      <td>0.285563</td>\n",
       "      <td>-0.219183</td>\n",
       "      <td>1.231067</td>\n",
       "      <td>-0.329971</td>\n",
       "      <td>-0.195295</td>\n",
       "      <td>-0.659005</td>\n",
       "      <td>0.545288</td>\n",
       "      <td>-0.616307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.479537</td>\n",
       "      <td>-0.228891</td>\n",
       "      <td>-0.955542</td>\n",
       "      <td>-0.275452</td>\n",
       "      <td>0.121603</td>\n",
       "      <td>-0.343834</td>\n",
       "      <td>0.303828</td>\n",
       "      <td>0.414734</td>\n",
       "      <td>0.507540</td>\n",
       "      <td>-0.086304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534609</td>\n",
       "      <td>0.502321</td>\n",
       "      <td>0.239422</td>\n",
       "      <td>0.139834</td>\n",
       "      <td>1.535233</td>\n",
       "      <td>-0.328742</td>\n",
       "      <td>0.115738</td>\n",
       "      <td>-0.273909</td>\n",
       "      <td>0.640233</td>\n",
       "      <td>-0.522307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \n",
       "0 -0.224086  0.124548 -0.560523 -0.280118 -0.347190 -0.669528  0.387310  \\\n",
       "1 -0.564464  0.356956 -0.855516 -0.186919  0.333399 -0.449246  0.284913   \n",
       "2 -0.404502 -0.202063 -0.432939 -0.261218 -0.053913 -0.664073  0.360001   \n",
       "3 -0.386408 -0.021483 -0.766305 -0.319618 -0.247374 -0.613956  0.230997   \n",
       "4 -0.479537 -0.228891 -0.955542 -0.275452  0.121603 -0.343834  0.303828   \n",
       "\n",
       "          7         8         9  ...        90        91        92        93   \n",
       "0  0.308927  0.298159 -0.228007  ...  0.595217  0.635776  0.343884 -0.018723  \\\n",
       "1  0.592180  0.314848 -0.311810  ...  0.264220  0.541035  0.047024 -0.307434   \n",
       "2  0.429876 -0.131980 -0.061973  ...  0.522549  0.676358  0.046866  0.025495   \n",
       "3  0.207710  0.288456 -0.490740  ...  0.584336  0.823562  0.285563 -0.219183   \n",
       "4  0.414734  0.507540 -0.086304  ...  0.534609  0.502321  0.239422  0.139834   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0  0.878769 -0.188055  0.028916 -0.691826  0.481183 -0.296126  \n",
       "1  1.486784 -0.291462  0.323980 -0.127526  0.177815  0.088189  \n",
       "2  1.049829 -0.170199  0.142993 -0.494416  0.505590 -0.258266  \n",
       "3  1.231067 -0.329971 -0.195295 -0.659005  0.545288 -0.616307  \n",
       "4  1.535233 -0.328742  0.115738 -0.273909  0.640233 -0.522307  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.357782</td>\n",
       "      <td>-0.055713</td>\n",
       "      <td>-0.585418</td>\n",
       "      <td>-0.290256</td>\n",
       "      <td>0.017274</td>\n",
       "      <td>-0.666192</td>\n",
       "      <td>0.424744</td>\n",
       "      <td>0.202211</td>\n",
       "      <td>0.176004</td>\n",
       "      <td>-0.247238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557878</td>\n",
       "      <td>0.674276</td>\n",
       "      <td>0.206508</td>\n",
       "      <td>-0.111757</td>\n",
       "      <td>1.262616</td>\n",
       "      <td>-0.180199</td>\n",
       "      <td>0.049589</td>\n",
       "      <td>-0.467771</td>\n",
       "      <td>0.460267</td>\n",
       "      <td>-0.386604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.201517</td>\n",
       "      <td>0.222543</td>\n",
       "      <td>0.225167</td>\n",
       "      <td>0.197058</td>\n",
       "      <td>0.219094</td>\n",
       "      <td>0.265696</td>\n",
       "      <td>0.211725</td>\n",
       "      <td>0.285604</td>\n",
       "      <td>0.187581</td>\n",
       "      <td>0.143813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184465</td>\n",
       "      <td>0.229161</td>\n",
       "      <td>0.129169</td>\n",
       "      <td>0.167785</td>\n",
       "      <td>0.203204</td>\n",
       "      <td>0.167661</td>\n",
       "      <td>0.225187</td>\n",
       "      <td>0.208375</td>\n",
       "      <td>0.187050</td>\n",
       "      <td>0.204745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.664191</td>\n",
       "      <td>-1.240832</td>\n",
       "      <td>-2.160181</td>\n",
       "      <td>-1.619422</td>\n",
       "      <td>-0.844185</td>\n",
       "      <td>-1.973544</td>\n",
       "      <td>-0.790706</td>\n",
       "      <td>-1.397818</td>\n",
       "      <td>-1.128849</td>\n",
       "      <td>-1.228149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.427693</td>\n",
       "      <td>-0.223124</td>\n",
       "      <td>-0.661434</td>\n",
       "      <td>-1.306401</td>\n",
       "      <td>0.042776</td>\n",
       "      <td>-1.250860</td>\n",
       "      <td>-1.226282</td>\n",
       "      <td>-1.544755</td>\n",
       "      <td>-1.002429</td>\n",
       "      <td>-1.873882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.488625</td>\n",
       "      <td>-0.198587</td>\n",
       "      <td>-0.728228</td>\n",
       "      <td>-0.419332</td>\n",
       "      <td>-0.130442</td>\n",
       "      <td>-0.840592</td>\n",
       "      <td>0.278219</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.059032</td>\n",
       "      <td>-0.332190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437524</td>\n",
       "      <td>0.514657</td>\n",
       "      <td>0.125371</td>\n",
       "      <td>-0.215231</td>\n",
       "      <td>1.131094</td>\n",
       "      <td>-0.278000</td>\n",
       "      <td>-0.098934</td>\n",
       "      <td>-0.595193</td>\n",
       "      <td>0.342267</td>\n",
       "      <td>-0.522306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.353631</td>\n",
       "      <td>-0.051612</td>\n",
       "      <td>-0.580155</td>\n",
       "      <td>-0.290110</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>-0.654389</td>\n",
       "      <td>0.415947</td>\n",
       "      <td>0.219655</td>\n",
       "      <td>0.178676</td>\n",
       "      <td>-0.239958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557769</td>\n",
       "      <td>0.660442</td>\n",
       "      <td>0.205381</td>\n",
       "      <td>-0.106339</td>\n",
       "      <td>1.262120</td>\n",
       "      <td>-0.167962</td>\n",
       "      <td>0.054544</td>\n",
       "      <td>-0.452565</td>\n",
       "      <td>0.463311</td>\n",
       "      <td>-0.384131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.223064</td>\n",
       "      <td>0.088947</td>\n",
       "      <td>-0.436885</td>\n",
       "      <td>-0.161091</td>\n",
       "      <td>0.148526</td>\n",
       "      <td>-0.478096</td>\n",
       "      <td>0.561817</td>\n",
       "      <td>0.404238</td>\n",
       "      <td>0.295431</td>\n",
       "      <td>-0.154098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678449</td>\n",
       "      <td>0.818430</td>\n",
       "      <td>0.285707</td>\n",
       "      <td>-0.002395</td>\n",
       "      <td>1.391761</td>\n",
       "      <td>-0.070830</td>\n",
       "      <td>0.201847</td>\n",
       "      <td>-0.327090</td>\n",
       "      <td>0.581628</td>\n",
       "      <td>-0.250172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.652195</td>\n",
       "      <td>1.172972</td>\n",
       "      <td>0.811553</td>\n",
       "      <td>0.865171</td>\n",
       "      <td>1.517697</td>\n",
       "      <td>0.476949</td>\n",
       "      <td>1.574111</td>\n",
       "      <td>1.363062</td>\n",
       "      <td>1.373078</td>\n",
       "      <td>0.562030</td>\n",
       "      <td>...</td>\n",
       "      <td>1.579512</td>\n",
       "      <td>1.913835</td>\n",
       "      <td>0.986538</td>\n",
       "      <td>0.802429</td>\n",
       "      <td>2.334129</td>\n",
       "      <td>1.043303</td>\n",
       "      <td>1.115096</td>\n",
       "      <td>0.399779</td>\n",
       "      <td>1.507849</td>\n",
       "      <td>0.884437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4   \n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000  \\\n",
       "mean      -0.357782     -0.055713     -0.585418     -0.290256      0.017274   \n",
       "std        0.201517      0.222543      0.225167      0.197058      0.219094   \n",
       "min       -1.664191     -1.240832     -2.160181     -1.619422     -0.844185   \n",
       "25%       -0.488625     -0.198587     -0.728228     -0.419332     -0.130442   \n",
       "50%       -0.353631     -0.051612     -0.580155     -0.290110      0.003435   \n",
       "75%       -0.223064      0.088947     -0.436885     -0.161091      0.148526   \n",
       "max        0.652195      1.172972      0.811553      0.865171      1.517697   \n",
       "\n",
       "                  5             6             7             8             9   \n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000  \\\n",
       "mean      -0.666192      0.424744      0.202211      0.176004     -0.247238   \n",
       "std        0.265696      0.211725      0.285604      0.187581      0.143813   \n",
       "min       -1.973544     -0.790706     -1.397818     -1.128849     -1.228149   \n",
       "25%       -0.840592      0.278219      0.018433      0.059032     -0.332190   \n",
       "50%       -0.654389      0.415947      0.219655      0.178676     -0.239958   \n",
       "75%       -0.478096      0.561817      0.404238      0.295431     -0.154098   \n",
       "max        0.476949      1.574111      1.363062      1.373078      0.562030   \n",
       "\n",
       "       ...            90            91            92            93   \n",
       "count  ...  50000.000000  50000.000000  50000.000000  50000.000000  \\\n",
       "mean   ...      0.557878      0.674276      0.206508     -0.111757   \n",
       "std    ...      0.184465      0.229161      0.129169      0.167785   \n",
       "min    ...     -0.427693     -0.223124     -0.661434     -1.306401   \n",
       "25%    ...      0.437524      0.514657      0.125371     -0.215231   \n",
       "50%    ...      0.557769      0.660442      0.205381     -0.106339   \n",
       "75%    ...      0.678449      0.818430      0.285707     -0.002395   \n",
       "max    ...      1.579512      1.913835      0.986538      0.802429   \n",
       "\n",
       "                 94            95            96            97            98   \n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000  \\\n",
       "mean       1.262616     -0.180199      0.049589     -0.467771      0.460267   \n",
       "std        0.203204      0.167661      0.225187      0.208375      0.187050   \n",
       "min        0.042776     -1.250860     -1.226282     -1.544755     -1.002429   \n",
       "25%        1.131094     -0.278000     -0.098934     -0.595193      0.342267   \n",
       "50%        1.262120     -0.167962      0.054544     -0.452565      0.463311   \n",
       "75%        1.391761     -0.070830      0.201847     -0.327090      0.581628   \n",
       "max        2.334129      1.043303      1.115096      0.399779      1.507849   \n",
       "\n",
       "                 99  \n",
       "count  50000.000000  \n",
       "mean      -0.386604  \n",
       "std        0.204745  \n",
       "min       -1.873882  \n",
       "25%       -0.522306  \n",
       "50%       -0.384131  \n",
       "75%       -0.250172  \n",
       "max        0.884437  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_scaled = pd.DataFrame(scaler.fit_transform(x), columns=x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.663457</td>\n",
       "      <td>0.810013</td>\n",
       "      <td>0.110563</td>\n",
       "      <td>0.051447</td>\n",
       "      <td>-1.663520</td>\n",
       "      <td>-0.012559</td>\n",
       "      <td>-0.176807</td>\n",
       "      <td>0.373655</td>\n",
       "      <td>0.651219</td>\n",
       "      <td>0.133722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202425</td>\n",
       "      <td>-0.168003</td>\n",
       "      <td>1.063549</td>\n",
       "      <td>0.554486</td>\n",
       "      <td>-1.888986</td>\n",
       "      <td>-0.046855</td>\n",
       "      <td>-0.091802</td>\n",
       "      <td>-1.075258</td>\n",
       "      <td>0.111824</td>\n",
       "      <td>0.441910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.025638</td>\n",
       "      <td>1.854351</td>\n",
       "      <td>-1.199555</td>\n",
       "      <td>0.524402</td>\n",
       "      <td>1.442885</td>\n",
       "      <td>0.816526</td>\n",
       "      <td>-0.660440</td>\n",
       "      <td>1.365430</td>\n",
       "      <td>0.740187</td>\n",
       "      <td>-0.449009</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.591956</td>\n",
       "      <td>-0.581432</td>\n",
       "      <td>-1.234701</td>\n",
       "      <td>-1.166249</td>\n",
       "      <td>1.103176</td>\n",
       "      <td>-0.663623</td>\n",
       "      <td>1.218516</td>\n",
       "      <td>1.632867</td>\n",
       "      <td>-1.510045</td>\n",
       "      <td>2.318975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.231840</td>\n",
       "      <td>-0.657635</td>\n",
       "      <td>0.677191</td>\n",
       "      <td>0.147359</td>\n",
       "      <td>-0.324919</td>\n",
       "      <td>0.007974</td>\n",
       "      <td>-0.305787</td>\n",
       "      <td>0.797142</td>\n",
       "      <td>-1.641896</td>\n",
       "      <td>1.288244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191521</td>\n",
       "      <td>0.009088</td>\n",
       "      <td>-1.235925</td>\n",
       "      <td>0.818033</td>\n",
       "      <td>-1.047169</td>\n",
       "      <td>0.059647</td>\n",
       "      <td>0.414788</td>\n",
       "      <td>-0.127868</td>\n",
       "      <td>0.242306</td>\n",
       "      <td>0.626823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.142050</td>\n",
       "      <td>0.153812</td>\n",
       "      <td>-0.803352</td>\n",
       "      <td>-0.149005</td>\n",
       "      <td>-1.207929</td>\n",
       "      <td>0.196602</td>\n",
       "      <td>-0.915093</td>\n",
       "      <td>0.019256</td>\n",
       "      <td>0.599493</td>\n",
       "      <td>-1.693200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143433</td>\n",
       "      <td>0.651451</td>\n",
       "      <td>0.612037</td>\n",
       "      <td>-0.640272</td>\n",
       "      <td>-0.155257</td>\n",
       "      <td>-0.893308</td>\n",
       "      <td>-1.087481</td>\n",
       "      <td>-0.917745</td>\n",
       "      <td>0.454541</td>\n",
       "      <td>-1.121914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.604194</td>\n",
       "      <td>-0.778188</td>\n",
       "      <td>-1.643789</td>\n",
       "      <td>0.075125</td>\n",
       "      <td>0.476188</td>\n",
       "      <td>1.213267</td>\n",
       "      <td>-0.571102</td>\n",
       "      <td>0.744126</td>\n",
       "      <td>1.767450</td>\n",
       "      <td>1.119053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126143</td>\n",
       "      <td>-0.750372</td>\n",
       "      <td>0.254819</td>\n",
       "      <td>1.499499</td>\n",
       "      <td>1.341605</td>\n",
       "      <td>-0.885981</td>\n",
       "      <td>0.293755</td>\n",
       "      <td>0.930363</td>\n",
       "      <td>0.962137</td>\n",
       "      <td>-0.662798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \n",
       "0  0.663457  0.810013  0.110563  0.051447 -1.663520 -0.012559 -0.176807  \\\n",
       "1 -1.025638  1.854351 -1.199555  0.524402  1.442885  0.816526 -0.660440   \n",
       "2 -0.231840 -0.657635  0.677191  0.147359 -0.324919  0.007974 -0.305787   \n",
       "3 -0.142050  0.153812 -0.803352 -0.149005 -1.207929  0.196602 -0.915093   \n",
       "4 -0.604194 -0.778188 -1.643789  0.075125  0.476188  1.213267 -0.571102   \n",
       "\n",
       "          7         8         9  ...        90        91        92        93   \n",
       "0  0.373655  0.651219  0.133722  ...  0.202425 -0.168003  1.063549  0.554486  \\\n",
       "1  1.365430  0.740187 -0.449009  ... -1.591956 -0.581432 -1.234701 -1.166249   \n",
       "2  0.797142 -1.641896  1.288244  ... -0.191521  0.009088 -1.235925  0.818033   \n",
       "3  0.019256  0.599493 -1.693200  ...  0.143433  0.651451  0.612037 -0.640272   \n",
       "4  0.744126  1.767450  1.119053  ... -0.126143 -0.750372  0.254819  1.499499   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0 -1.888986 -0.046855 -0.091802 -1.075258  0.111824  0.441910  \n",
       "1  1.103176 -0.663623  1.218516  1.632867 -1.510045  2.318975  \n",
       "2 -1.047169  0.059647  0.414788 -0.127868  0.242306  0.626823  \n",
       "3 -0.155257 -0.893308 -1.087481 -0.917745  0.454541 -1.121914  \n",
       "4  1.341605 -0.885981  0.293755  0.930363  0.962137 -0.662798  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    1\n",
       " 1    1\n",
       " 2    1\n",
       " 3    0\n",
       " 4    1\n",
       " Name: sentiment, dtype: int64,\n",
       " sentiment\n",
       " 1    25000\n",
       " 0    25000\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(), y.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Simple Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.4211 - accuracy: 0.8111 - val_loss: 0.3507 - val_accuracy: 0.8528\n",
      "Epoch 2/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.3617 - accuracy: 0.8436 - val_loss: 0.3425 - val_accuracy: 0.8535\n",
      "Epoch 3/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.3533 - accuracy: 0.8468 - val_loss: 0.3365 - val_accuracy: 0.8572\n",
      "Epoch 4/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.3483 - accuracy: 0.8487 - val_loss: 0.3332 - val_accuracy: 0.8583\n",
      "Epoch 5/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.3443 - accuracy: 0.8518 - val_loss: 0.3340 - val_accuracy: 0.8596\n",
      "Epoch 6/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.3408 - accuracy: 0.8525 - val_loss: 0.3310 - val_accuracy: 0.8604\n",
      "Epoch 7/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.3360 - accuracy: 0.8537 - val_loss: 0.3377 - val_accuracy: 0.8591\n",
      "Epoch 8/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.3353 - accuracy: 0.8550 - val_loss: 0.3305 - val_accuracy: 0.8585\n",
      "Epoch 9/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.3322 - accuracy: 0.8550 - val_loss: 0.3310 - val_accuracy: 0.8604\n",
      "Epoch 10/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.3297 - accuracy: 0.8583 - val_loss: 0.3293 - val_accuracy: 0.8595\n",
      "Epoch 11/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.3266 - accuracy: 0.8580 - val_loss: 0.3288 - val_accuracy: 0.8600\n",
      "Epoch 12/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.3244 - accuracy: 0.8606 - val_loss: 0.3271 - val_accuracy: 0.8616\n",
      "Epoch 13/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.3213 - accuracy: 0.8596 - val_loss: 0.3285 - val_accuracy: 0.8606\n",
      "Epoch 14/200\n",
      "524/524 [==============================] - 2s 4ms/step - loss: 0.3187 - accuracy: 0.8630 - val_loss: 0.3256 - val_accuracy: 0.8612\n",
      "Epoch 15/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.3146 - accuracy: 0.8651 - val_loss: 0.3281 - val_accuracy: 0.8598\n",
      "Epoch 16/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.3137 - accuracy: 0.8639 - val_loss: 0.3322 - val_accuracy: 0.8562\n",
      "Epoch 17/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.3130 - accuracy: 0.8653 - val_loss: 0.3327 - val_accuracy: 0.8575\n",
      "Epoch 18/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.3155 - accuracy: 0.8635 - val_loss: 0.3292 - val_accuracy: 0.8597\n",
      "Epoch 19/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.3095 - accuracy: 0.8664 - val_loss: 0.3301 - val_accuracy: 0.8581\n",
      "Epoch 20/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.3076 - accuracy: 0.8670 - val_loss: 0.3306 - val_accuracy: 0.8592\n",
      "Epoch 21/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.3071 - accuracy: 0.8657 - val_loss: 0.3336 - val_accuracy: 0.8564\n",
      "Epoch 22/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.3094 - accuracy: 0.8666 - val_loss: 0.3310 - val_accuracy: 0.8593\n",
      "Epoch 23/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.3027 - accuracy: 0.8696 - val_loss: 0.3306 - val_accuracy: 0.8582\n",
      "Epoch 24/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.3021 - accuracy: 0.8713 - val_loss: 0.3347 - val_accuracy: 0.8598\n",
      "Epoch 25/200\n",
      "524/524 [==============================] - 2s 4ms/step - loss: 0.3031 - accuracy: 0.8686 - val_loss: 0.3335 - val_accuracy: 0.8573\n",
      "Epoch 26/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2987 - accuracy: 0.8708 - val_loss: 0.3348 - val_accuracy: 0.8586\n",
      "Epoch 27/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2966 - accuracy: 0.8726 - val_loss: 0.3334 - val_accuracy: 0.8579\n",
      "Epoch 28/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2998 - accuracy: 0.8689 - val_loss: 0.3373 - val_accuracy: 0.8564\n",
      "Epoch 29/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2962 - accuracy: 0.8718 - val_loss: 0.3330 - val_accuracy: 0.8572\n",
      "Epoch 30/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2939 - accuracy: 0.8732 - val_loss: 0.3342 - val_accuracy: 0.8587\n",
      "Epoch 31/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2960 - accuracy: 0.8730 - val_loss: 0.3354 - val_accuracy: 0.8576\n",
      "Epoch 32/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2955 - accuracy: 0.8722 - val_loss: 0.3373 - val_accuracy: 0.8599\n",
      "Epoch 33/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2913 - accuracy: 0.8746 - val_loss: 0.3352 - val_accuracy: 0.8568\n",
      "Epoch 34/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2911 - accuracy: 0.8738 - val_loss: 0.3378 - val_accuracy: 0.8578\n",
      "Epoch 35/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2925 - accuracy: 0.8729 - val_loss: 0.3411 - val_accuracy: 0.8592\n",
      "Epoch 36/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2929 - accuracy: 0.8742 - val_loss: 0.3373 - val_accuracy: 0.8565\n",
      "Epoch 37/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2887 - accuracy: 0.8753 - val_loss: 0.3390 - val_accuracy: 0.8550\n",
      "Epoch 38/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2874 - accuracy: 0.8755 - val_loss: 0.3366 - val_accuracy: 0.8548\n",
      "Epoch 39/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2873 - accuracy: 0.8746 - val_loss: 0.3397 - val_accuracy: 0.8565\n",
      "Epoch 40/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2858 - accuracy: 0.8769 - val_loss: 0.3435 - val_accuracy: 0.8553\n",
      "Epoch 41/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2854 - accuracy: 0.8764 - val_loss: 0.3475 - val_accuracy: 0.8550\n",
      "Epoch 42/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2841 - accuracy: 0.8772 - val_loss: 0.3446 - val_accuracy: 0.8575\n",
      "Epoch 43/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2873 - accuracy: 0.8758 - val_loss: 0.3426 - val_accuracy: 0.8570\n",
      "Epoch 44/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2857 - accuracy: 0.8766 - val_loss: 0.3400 - val_accuracy: 0.8555\n",
      "Epoch 45/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2844 - accuracy: 0.8766 - val_loss: 0.3454 - val_accuracy: 0.8546\n",
      "Epoch 46/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2808 - accuracy: 0.8781 - val_loss: 0.3399 - val_accuracy: 0.8559\n",
      "Epoch 47/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2846 - accuracy: 0.8755 - val_loss: 0.3402 - val_accuracy: 0.8562\n",
      "Epoch 48/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2829 - accuracy: 0.8784 - val_loss: 0.3418 - val_accuracy: 0.8548\n",
      "Epoch 49/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2806 - accuracy: 0.8779 - val_loss: 0.3458 - val_accuracy: 0.8549\n",
      "Epoch 50/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2846 - accuracy: 0.8778 - val_loss: 0.3459 - val_accuracy: 0.8538\n",
      "Epoch 51/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2773 - accuracy: 0.8811 - val_loss: 0.3490 - val_accuracy: 0.8530\n",
      "Epoch 52/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2797 - accuracy: 0.8803 - val_loss: 0.3484 - val_accuracy: 0.8538\n",
      "Epoch 53/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2768 - accuracy: 0.8810 - val_loss: 0.3452 - val_accuracy: 0.8544\n",
      "Epoch 54/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2773 - accuracy: 0.8807 - val_loss: 0.3477 - val_accuracy: 0.8538\n",
      "Epoch 55/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2744 - accuracy: 0.8825 - val_loss: 0.3478 - val_accuracy: 0.8505\n",
      "Epoch 56/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2771 - accuracy: 0.8823 - val_loss: 0.3435 - val_accuracy: 0.8548\n",
      "Epoch 57/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2760 - accuracy: 0.8792 - val_loss: 0.3471 - val_accuracy: 0.8542\n",
      "Epoch 58/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2758 - accuracy: 0.8824 - val_loss: 0.3461 - val_accuracy: 0.8556\n",
      "Epoch 59/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2759 - accuracy: 0.8812 - val_loss: 0.3443 - val_accuracy: 0.8542\n",
      "Epoch 60/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2775 - accuracy: 0.8828 - val_loss: 0.3477 - val_accuracy: 0.8553\n",
      "Epoch 61/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2729 - accuracy: 0.8838 - val_loss: 0.3521 - val_accuracy: 0.8546\n",
      "Epoch 62/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2753 - accuracy: 0.8830 - val_loss: 0.3521 - val_accuracy: 0.8518\n",
      "Epoch 63/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2746 - accuracy: 0.8822 - val_loss: 0.3489 - val_accuracy: 0.8556\n",
      "Epoch 64/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2732 - accuracy: 0.8815 - val_loss: 0.3466 - val_accuracy: 0.8536\n",
      "Epoch 65/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2733 - accuracy: 0.8836 - val_loss: 0.3485 - val_accuracy: 0.8541\n",
      "Epoch 66/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2696 - accuracy: 0.8836 - val_loss: 0.3490 - val_accuracy: 0.8542\n",
      "Epoch 67/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2750 - accuracy: 0.8821 - val_loss: 0.3511 - val_accuracy: 0.8527\n",
      "Epoch 68/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2754 - accuracy: 0.8797 - val_loss: 0.3527 - val_accuracy: 0.8505\n",
      "Epoch 69/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2701 - accuracy: 0.8845 - val_loss: 0.3526 - val_accuracy: 0.8536\n",
      "Epoch 70/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2710 - accuracy: 0.8813 - val_loss: 0.3508 - val_accuracy: 0.8549\n",
      "Epoch 71/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2692 - accuracy: 0.8867 - val_loss: 0.3513 - val_accuracy: 0.8541\n",
      "Epoch 72/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2707 - accuracy: 0.8842 - val_loss: 0.3548 - val_accuracy: 0.8545\n",
      "Epoch 73/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2666 - accuracy: 0.8864 - val_loss: 0.3551 - val_accuracy: 0.8528\n",
      "Epoch 74/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2705 - accuracy: 0.8825 - val_loss: 0.3490 - val_accuracy: 0.8527\n",
      "Epoch 75/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2660 - accuracy: 0.8853 - val_loss: 0.3490 - val_accuracy: 0.8512\n",
      "Epoch 76/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2720 - accuracy: 0.8839 - val_loss: 0.3546 - val_accuracy: 0.8512\n",
      "Epoch 77/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2684 - accuracy: 0.8854 - val_loss: 0.3553 - val_accuracy: 0.8513\n",
      "Epoch 78/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2661 - accuracy: 0.8870 - val_loss: 0.3557 - val_accuracy: 0.8520\n",
      "Epoch 79/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2684 - accuracy: 0.8848 - val_loss: 0.3576 - val_accuracy: 0.8547\n",
      "Epoch 80/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2624 - accuracy: 0.8879 - val_loss: 0.3597 - val_accuracy: 0.8515\n",
      "Epoch 81/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2648 - accuracy: 0.8861 - val_loss: 0.3573 - val_accuracy: 0.8496\n",
      "Epoch 82/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2668 - accuracy: 0.8851 - val_loss: 0.3573 - val_accuracy: 0.8528\n",
      "Epoch 83/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2670 - accuracy: 0.8858 - val_loss: 0.3583 - val_accuracy: 0.8507\n",
      "Epoch 84/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2621 - accuracy: 0.8887 - val_loss: 0.3564 - val_accuracy: 0.8535\n",
      "Epoch 85/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2641 - accuracy: 0.8865 - val_loss: 0.3556 - val_accuracy: 0.8527\n",
      "Epoch 86/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2628 - accuracy: 0.8875 - val_loss: 0.3612 - val_accuracy: 0.8485\n",
      "Epoch 87/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2663 - accuracy: 0.8867 - val_loss: 0.3600 - val_accuracy: 0.8499\n",
      "Epoch 88/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2660 - accuracy: 0.8851 - val_loss: 0.3562 - val_accuracy: 0.8520\n",
      "Epoch 89/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2633 - accuracy: 0.8867 - val_loss: 0.3553 - val_accuracy: 0.8501\n",
      "Epoch 90/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2654 - accuracy: 0.8858 - val_loss: 0.3590 - val_accuracy: 0.8511\n",
      "Epoch 91/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2645 - accuracy: 0.8869 - val_loss: 0.3528 - val_accuracy: 0.8518\n",
      "Epoch 92/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2639 - accuracy: 0.8877 - val_loss: 0.3579 - val_accuracy: 0.8521\n",
      "Epoch 93/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2617 - accuracy: 0.8884 - val_loss: 0.3576 - val_accuracy: 0.8488\n",
      "Epoch 94/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2622 - accuracy: 0.8886 - val_loss: 0.3607 - val_accuracy: 0.8501\n",
      "Epoch 95/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2631 - accuracy: 0.8855 - val_loss: 0.3623 - val_accuracy: 0.8502\n",
      "Epoch 96/200\n",
      "524/524 [==============================] - 2s 4ms/step - loss: 0.2633 - accuracy: 0.8884 - val_loss: 0.3552 - val_accuracy: 0.8500\n",
      "Epoch 97/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2648 - accuracy: 0.8860 - val_loss: 0.3554 - val_accuracy: 0.8494\n",
      "Epoch 98/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2594 - accuracy: 0.8895 - val_loss: 0.3614 - val_accuracy: 0.8501\n",
      "Epoch 99/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2598 - accuracy: 0.8893 - val_loss: 0.3562 - val_accuracy: 0.8511\n",
      "Epoch 100/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2619 - accuracy: 0.8875 - val_loss: 0.3619 - val_accuracy: 0.8494\n",
      "Epoch 101/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2635 - accuracy: 0.8886 - val_loss: 0.3574 - val_accuracy: 0.8505\n",
      "Epoch 102/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2574 - accuracy: 0.8927 - val_loss: 0.3627 - val_accuracy: 0.8496\n",
      "Epoch 103/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2609 - accuracy: 0.8879 - val_loss: 0.3592 - val_accuracy: 0.8478\n",
      "Epoch 104/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2586 - accuracy: 0.8903 - val_loss: 0.3621 - val_accuracy: 0.8489\n",
      "Epoch 105/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2578 - accuracy: 0.8913 - val_loss: 0.3600 - val_accuracy: 0.8499\n",
      "Epoch 106/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2614 - accuracy: 0.8879 - val_loss: 0.3578 - val_accuracy: 0.8524\n",
      "Epoch 107/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2615 - accuracy: 0.8870 - val_loss: 0.3572 - val_accuracy: 0.8508\n",
      "Epoch 108/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2594 - accuracy: 0.8890 - val_loss: 0.3651 - val_accuracy: 0.8515\n",
      "Epoch 109/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2577 - accuracy: 0.8902 - val_loss: 0.3618 - val_accuracy: 0.8501\n",
      "Epoch 110/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2591 - accuracy: 0.8880 - val_loss: 0.3601 - val_accuracy: 0.8472\n",
      "Epoch 111/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2581 - accuracy: 0.8907 - val_loss: 0.3645 - val_accuracy: 0.8473\n",
      "Epoch 112/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2596 - accuracy: 0.8901 - val_loss: 0.3596 - val_accuracy: 0.8465\n",
      "Epoch 113/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2565 - accuracy: 0.8914 - val_loss: 0.3627 - val_accuracy: 0.8459\n",
      "Epoch 114/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2581 - accuracy: 0.8902 - val_loss: 0.3607 - val_accuracy: 0.8483\n",
      "Epoch 115/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2567 - accuracy: 0.8895 - val_loss: 0.3676 - val_accuracy: 0.8497\n",
      "Epoch 116/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2580 - accuracy: 0.8906 - val_loss: 0.3663 - val_accuracy: 0.8487\n",
      "Epoch 117/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2600 - accuracy: 0.8885 - val_loss: 0.3636 - val_accuracy: 0.8488\n",
      "Epoch 118/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2578 - accuracy: 0.8910 - val_loss: 0.3634 - val_accuracy: 0.8488\n",
      "Epoch 119/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2561 - accuracy: 0.8896 - val_loss: 0.3625 - val_accuracy: 0.8482\n",
      "Epoch 120/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2534 - accuracy: 0.8920 - val_loss: 0.3667 - val_accuracy: 0.8481\n",
      "Epoch 121/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2570 - accuracy: 0.8895 - val_loss: 0.3613 - val_accuracy: 0.8493\n",
      "Epoch 122/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2558 - accuracy: 0.8934 - val_loss: 0.3643 - val_accuracy: 0.8502\n",
      "Epoch 123/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2590 - accuracy: 0.8887 - val_loss: 0.3625 - val_accuracy: 0.8492\n",
      "Epoch 124/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2525 - accuracy: 0.8918 - val_loss: 0.3688 - val_accuracy: 0.8502\n",
      "Epoch 125/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2547 - accuracy: 0.8919 - val_loss: 0.3623 - val_accuracy: 0.8493\n",
      "Epoch 126/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2547 - accuracy: 0.8915 - val_loss: 0.3627 - val_accuracy: 0.8490\n",
      "Epoch 127/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2541 - accuracy: 0.8914 - val_loss: 0.3642 - val_accuracy: 0.8506\n",
      "Epoch 128/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2544 - accuracy: 0.8913 - val_loss: 0.3736 - val_accuracy: 0.8495\n",
      "Epoch 129/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2540 - accuracy: 0.8914 - val_loss: 0.3663 - val_accuracy: 0.8474\n",
      "Epoch 130/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2531 - accuracy: 0.8924 - val_loss: 0.3684 - val_accuracy: 0.8501\n",
      "Epoch 131/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2536 - accuracy: 0.8916 - val_loss: 0.3703 - val_accuracy: 0.8462\n",
      "Epoch 132/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2536 - accuracy: 0.8909 - val_loss: 0.3636 - val_accuracy: 0.8485\n",
      "Epoch 133/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2545 - accuracy: 0.8938 - val_loss: 0.3683 - val_accuracy: 0.8470\n",
      "Epoch 134/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2495 - accuracy: 0.8959 - val_loss: 0.3734 - val_accuracy: 0.8472\n",
      "Epoch 135/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2512 - accuracy: 0.8907 - val_loss: 0.3679 - val_accuracy: 0.8473\n",
      "Epoch 136/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2519 - accuracy: 0.8926 - val_loss: 0.3656 - val_accuracy: 0.8472\n",
      "Epoch 137/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2519 - accuracy: 0.8933 - val_loss: 0.3783 - val_accuracy: 0.8492\n",
      "Epoch 138/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2517 - accuracy: 0.8959 - val_loss: 0.3717 - val_accuracy: 0.8476\n",
      "Epoch 139/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2551 - accuracy: 0.8914 - val_loss: 0.3658 - val_accuracy: 0.8481\n",
      "Epoch 140/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2513 - accuracy: 0.8924 - val_loss: 0.3678 - val_accuracy: 0.8432\n",
      "Epoch 141/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2515 - accuracy: 0.8928 - val_loss: 0.3688 - val_accuracy: 0.8485\n",
      "Epoch 142/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2516 - accuracy: 0.8932 - val_loss: 0.3675 - val_accuracy: 0.8490\n",
      "Epoch 143/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2515 - accuracy: 0.8912 - val_loss: 0.3720 - val_accuracy: 0.8473\n",
      "Epoch 144/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2543 - accuracy: 0.8919 - val_loss: 0.3699 - val_accuracy: 0.8508\n",
      "Epoch 145/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2496 - accuracy: 0.8934 - val_loss: 0.3676 - val_accuracy: 0.8468\n",
      "Epoch 146/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2511 - accuracy: 0.8938 - val_loss: 0.3674 - val_accuracy: 0.8466\n",
      "Epoch 147/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2485 - accuracy: 0.8933 - val_loss: 0.3686 - val_accuracy: 0.8471\n",
      "Epoch 148/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2517 - accuracy: 0.8936 - val_loss: 0.3661 - val_accuracy: 0.8464\n",
      "Epoch 149/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2514 - accuracy: 0.8924 - val_loss: 0.3713 - val_accuracy: 0.8464\n",
      "Epoch 150/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2497 - accuracy: 0.8934 - val_loss: 0.3686 - val_accuracy: 0.8495\n",
      "Epoch 151/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2489 - accuracy: 0.8958 - val_loss: 0.3686 - val_accuracy: 0.8487\n",
      "Epoch 152/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2490 - accuracy: 0.8951 - val_loss: 0.3712 - val_accuracy: 0.8470\n",
      "Epoch 153/200\n",
      "524/524 [==============================] - 1s 2ms/step - loss: 0.2463 - accuracy: 0.8943 - val_loss: 0.3755 - val_accuracy: 0.8491\n",
      "Epoch 154/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2512 - accuracy: 0.8937 - val_loss: 0.3699 - val_accuracy: 0.8483\n",
      "Epoch 155/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2510 - accuracy: 0.8912 - val_loss: 0.3730 - val_accuracy: 0.8465\n",
      "Epoch 156/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2489 - accuracy: 0.8944 - val_loss: 0.3697 - val_accuracy: 0.8478\n",
      "Epoch 157/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2497 - accuracy: 0.8936 - val_loss: 0.3689 - val_accuracy: 0.8485\n",
      "Epoch 158/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.8951 - val_loss: 0.3674 - val_accuracy: 0.8469\n",
      "Epoch 159/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2482 - accuracy: 0.8948 - val_loss: 0.3743 - val_accuracy: 0.8479\n",
      "Epoch 160/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2480 - accuracy: 0.8944 - val_loss: 0.3755 - val_accuracy: 0.8501\n",
      "Epoch 161/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2517 - accuracy: 0.8929 - val_loss: 0.3733 - val_accuracy: 0.8495\n",
      "Epoch 162/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2471 - accuracy: 0.8935 - val_loss: 0.3704 - val_accuracy: 0.8494\n",
      "Epoch 163/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2470 - accuracy: 0.8946 - val_loss: 0.3710 - val_accuracy: 0.8481\n",
      "Epoch 164/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2458 - accuracy: 0.8951 - val_loss: 0.3748 - val_accuracy: 0.8497\n",
      "Epoch 165/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2492 - accuracy: 0.8944 - val_loss: 0.3732 - val_accuracy: 0.8452\n",
      "Epoch 166/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2469 - accuracy: 0.8953 - val_loss: 0.3767 - val_accuracy: 0.8455\n",
      "Epoch 167/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2458 - accuracy: 0.8976 - val_loss: 0.3724 - val_accuracy: 0.8475\n",
      "Epoch 168/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2468 - accuracy: 0.8952 - val_loss: 0.3739 - val_accuracy: 0.8448\n",
      "Epoch 169/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2461 - accuracy: 0.8972 - val_loss: 0.3717 - val_accuracy: 0.8452\n",
      "Epoch 170/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2495 - accuracy: 0.8934 - val_loss: 0.3713 - val_accuracy: 0.8464\n",
      "Epoch 171/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2481 - accuracy: 0.8944 - val_loss: 0.3708 - val_accuracy: 0.8481\n",
      "Epoch 172/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2492 - accuracy: 0.8957 - val_loss: 0.3724 - val_accuracy: 0.8455\n",
      "Epoch 173/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2448 - accuracy: 0.8956 - val_loss: 0.3748 - val_accuracy: 0.8444\n",
      "Epoch 174/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2461 - accuracy: 0.8956 - val_loss: 0.3769 - val_accuracy: 0.8468\n",
      "Epoch 175/200\n",
      "524/524 [==============================] - 2s 3ms/step - loss: 0.2449 - accuracy: 0.8959 - val_loss: 0.3731 - val_accuracy: 0.8460\n",
      "Epoch 176/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2434 - accuracy: 0.8959 - val_loss: 0.3759 - val_accuracy: 0.8441\n",
      "Epoch 177/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2498 - accuracy: 0.8924 - val_loss: 0.3742 - val_accuracy: 0.8463\n",
      "Epoch 178/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2448 - accuracy: 0.8954 - val_loss: 0.3763 - val_accuracy: 0.8444\n",
      "Epoch 179/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2470 - accuracy: 0.8962 - val_loss: 0.3719 - val_accuracy: 0.8464\n",
      "Epoch 180/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2483 - accuracy: 0.8938 - val_loss: 0.3747 - val_accuracy: 0.8472\n",
      "Epoch 181/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2466 - accuracy: 0.8951 - val_loss: 0.3735 - val_accuracy: 0.8475\n",
      "Epoch 182/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2439 - accuracy: 0.8973 - val_loss: 0.3725 - val_accuracy: 0.8472\n",
      "Epoch 183/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2455 - accuracy: 0.8961 - val_loss: 0.3711 - val_accuracy: 0.8473\n",
      "Epoch 184/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2487 - accuracy: 0.8967 - val_loss: 0.3669 - val_accuracy: 0.8499\n",
      "Epoch 185/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2441 - accuracy: 0.8964 - val_loss: 0.3726 - val_accuracy: 0.8488\n",
      "Epoch 186/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2440 - accuracy: 0.8959 - val_loss: 0.3717 - val_accuracy: 0.8468\n",
      "Epoch 187/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2450 - accuracy: 0.8935 - val_loss: 0.3725 - val_accuracy: 0.8479\n",
      "Epoch 188/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2493 - accuracy: 0.8942 - val_loss: 0.3725 - val_accuracy: 0.8500\n",
      "Epoch 189/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2429 - accuracy: 0.8956 - val_loss: 0.3705 - val_accuracy: 0.8490\n",
      "Epoch 190/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2428 - accuracy: 0.8953 - val_loss: 0.3738 - val_accuracy: 0.8492\n",
      "Epoch 191/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2469 - accuracy: 0.8951 - val_loss: 0.3732 - val_accuracy: 0.8488\n",
      "Epoch 192/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2439 - accuracy: 0.8970 - val_loss: 0.3744 - val_accuracy: 0.8473\n",
      "Epoch 193/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2457 - accuracy: 0.8958 - val_loss: 0.3696 - val_accuracy: 0.8479\n",
      "Epoch 194/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2399 - accuracy: 0.8985 - val_loss: 0.3731 - val_accuracy: 0.8482\n",
      "Epoch 195/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2405 - accuracy: 0.8966 - val_loss: 0.3729 - val_accuracy: 0.8461\n",
      "Epoch 196/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2408 - accuracy: 0.8969 - val_loss: 0.3781 - val_accuracy: 0.8453\n",
      "Epoch 197/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2419 - accuracy: 0.8980 - val_loss: 0.3761 - val_accuracy: 0.8460\n",
      "Epoch 198/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2437 - accuracy: 0.8967 - val_loss: 0.3753 - val_accuracy: 0.8452\n",
      "Epoch 199/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2451 - accuracy: 0.8964 - val_loss: 0.3782 - val_accuracy: 0.8458\n",
      "Epoch 200/200\n",
      "524/524 [==============================] - 1s 3ms/step - loss: 0.2436 - accuracy: 0.8973 - val_loss: 0.3775 - val_accuracy: 0.8475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2162a8cce50>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(units=80, kernel_initializer='he_uniform', activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(units=50, kernel_initializer='he_uniform', activation='relu'))\n",
    "model.add(Dense(units=20, kernel_initializer='he_uniform', activation='relu'))\n",
    "model.add(Dense(units=1, kernel_initializer='glorot_uniform', activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_scaled, y, validation_split=0.33, batch_size=64, epochs=200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 2s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91384"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(x_scaled)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(vectorizer, open('assets/vectorizer.pkl', 'wb'))\n",
    "pkl.dump(scaler, open('assets/scaler.pkl', 'wb'))\n",
    "pkl.dump(model, open('assets/model.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
